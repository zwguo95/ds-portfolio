---
title: "Weights in Statistics: What Do People Often Get Wrong?"
date: 04/30/2024
author:
  - name: Zhaowen Guo
    url: "https://www.linkedin.com/in/zhaowen-guo-20535312a/"
title-block-banner: true
format:
  html:
    theme: flatly
    code-fold: false
    code-tools: false
    toc: false
    number-sections: false
link-citations: true
categories: [code, survey methodology]
image: "sample_population.png"
---

> Survey weighting is a mess. --- Andrew Gelman

Recently, many clients have come to us with questions about weights -- how to create weighted statistics? how to conduct weighted regressions? When I ask about the specific weights they're referring to, many seem unsure. This confusion underscores the complexity of discussions surrounding weights. Motivated by these encounters, I started looking into different types of weights, and what weighting can and cannot do. In this blog post, I will address common misconceptions surrounding these questions and hopefully bring clarity to how weights should be understood and used in statistical practices.

## There exists more than one type of weight

The first common misconception is that when people talk about weights in statistics, they are all pointing to the same concept. This is not the case, as highlighted in the insightful discussions by [Thomas Lumley](https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/) and [Andrew Gelman](https://statmodeling.stat.columbia.edu/2021/01/17/weights-in-statistics/). Building upon their work, I will provide visual and code demonstrations below to illustrate the differences between various types of weights.

![Types of Weights in Statistics](weights_types.png)

Lumley and Gelman identify four primary types of weights in statistics: sampling weights, precision weights, frequency weights, and importance weights, which are highlighted in yellow boxes in this graph. I included alternative names that are commonly used to describe each type of weights.

What does a weight of 100 mean for each type of weights?

-   A \textbf{sampling weight} of 100 indicates that the observation represents 100 individuals in the population. Alternatively, the probability that this observation was selected from the population to the sample is 1/100. This helps to ensure the sample is representative of the population.

-   A \textbf{precision weight} of 100 means that the observation's variance is 100 times less than that of an observation with a precision weight of 1. This weighting is used to enhance the estimation efficiency like OLS, where typically each observation is equally weighted. By using precision weights, observations with lower variance (and thus higher precision) have a greater influence on determining the line of best fit.

-   A \textbf{frequency weight} of 100 suggests that there are 100 identical observations in the sample. To save space and processing power, only one value is used to represent these observations.

Importance weights are different from the other types. Lumley provides a good example related to [dual-frame sampling](https://notstatschat.rbind.io/2024/04/19/importance-weights/). When your sample comes from two overlapping sampling frames, it's crucial to property account for individuals who appear in both frames to prevent double-counting. In such cases, importance weights are applied to adjust for the overlap, typically by dividing the original sampling weights by 2 of those duplicated individuals.

Additionally, I introduced a fifth category -- adjustment weights -- marked in an orange box. Adjustment weights are often used to rectify discrepancies between the sample and the population, addressing issues such as respondent attrition and non-response through techniques like post-stratification and raking. You might also have heard of terms like ["longitudinal weights" or "cross-sectional weights"](https://www.understandingsociety.ac.uk/documentation/mainstage/user-guides/main-survey-user-guide/longitudinal-and-cross-sectional-weights/#:~:text=Unlike%20in%20a%20cross%2Dsectional,used%20cross%2Dsectionally%20years%20later.) which often appeared in public health research. These terms fall under the umbrella of adjustment weights. While adjustment weights typically complement sampling weights, they can extend to non-probability samples as well.

Below I turn to each type of weight using the California Academic Performance Index data from the `survey` package. We'll work with the `apisrs` dataset, a simple random sample of 200 schools where sampling weights are included (`pw`). Our goal is to analyze the relationship between academic performance in the year 2000 (`api00`) and percentage of economically disadvantaged students (`meals`).

### Sampling weights

We incorporate sampling weights `pw` into our survey design, which serves as the foundation for further analysis.

```{r, message=FALSE, warning=FALSE}
library(survey)
library(tidyverse)
data(api)

# Set options to adjust for potential issues with single primary sampling units (PSUs)
options(survey.lonely.psu = "adjust")
options(survey.adjust.domain.lonely = TRUE)

# Define the survey design
srs.design <- svydesign(id =~ 1, # no clustering (all data has equal PSU)
                        weights = ~pw, # sampling weights
                        data = apisrs)

# Run a linear regression with sampling weights
ols_sampling_weights <- svyglm(api00 ~ meals, design = srs.design)
```

### Precision weights

We manually create precision weights and incorporate them in the `weights=` argument in the `glm()` function. The basic idea is that if the variance is heteroscedastic (unequal across observations) and depends linearly on the fitted values, we could use the inverse of these fitted values as weights. Alternatively, if we already have an estimate of the variance for each observation from prior knowledge or experimental design, we could also directly use those values to compute the inverse variance weights.

```{r}
# Fit a traditional OLS model
ols <- glm(api00 ~ meals, data = apisrs)

# Estimate variance (assuming variance of residuals might depend on the magnitude of fitted values)
ols_var <- glm(I(residuals(ols)^2) ~ fitted(ols), data = apisrs)

# Compute precision weights as the inverse of the estimated variance
precision_weights <- 1 / fitted(ols_var)

# Fit the weighted least squares model
ols_precision_weights <- glm(api00 ~ meals, data = apisrs, weights = precision_weights)
```

### Frequency weights

Assuming we want to store data at the student level rather than the school level, we could generate frequency weights that represent the number of students in each school, and run our analysis on the uncompressed data.

```{r, warning=FALSE}
set.seed(123)

# For simplicity, assume the number of students per school ranges from 1 to 3
apisrs$frequency_weights <- sample(1:3, size = nrow(apisrs), replace = TRUE)

# Construct individual-level dataset
apisrs_individual <- apisrs %>%
  uncount(weights = frequency_weights, .id = "cds")

# Run a linear model on the uncompressed data
ols_frequency_weights <- glm(api00 ~ meals, data = apisrs_individual)
```

### Adjustment weights

Assume that our data is from a simple random sample

```{r}
# Assume we know population totals for each school type
pop_totals <- data.frame(
  stype = c("E", "M", "H"),
  Freq = c(5000, 3000, 2000)  # hypothetical population sizes for Elementary, Middle, High
)

# Post-stratification
post_design <- postStratify(srs.design, strata = ~stype, population = pop_totals)

# Extract the post-stratified weights
adjusted_weights <- weights(post_design)

# Update the original design with new weights
srs.design.updated <- update(srs.design, weights = adjusted_weights)
```

## Non-probability samples hold promise when paired with careful weighting

Another misconception is that many tend to hold a dismissive stance towards non-probability sampling, including American Association of Public Opinion Research ([AAPOR](https://www.washingtonpost.com/news/monkey-cage/wp/2014/08/04/modern-polling-requires-both-sampling-and-adjustment/?next_url=https://www.washingtonpost.com/news/monkey-cage/wp/2014/08/04/modern-polling-requires-both-sampling-and-adjustment/)) in 2014. A frequently cited example of non-probability sampling's pitfalls is the 1936 Literary Digest poll, which incorrectly predicted the outcome of the presidential election due to its unrepresentative sample.

![1936 Literary Digest](literary_digest.jpg)

However, the benefit of probability samples might not come true for every project. In scenarios where response rates are extremely low, a probability sample might not offer more accuracy than a non-probability convenience sample. Moreover, the application of weighting techniques can significantly enhance the accuracy of non-probability samples, making them viable for reliable estimates.

Selecting appropriate variables for weighting is crucial. For instance, Lohr and Brick's 2017 [analysis](https://gwern.net/doc/statistics/bias/2017-lohr.pdf) demonstrated that using voter data from the 1932 election to weight the Literary Digest's sample would have accurately predicted Roosevelt's victory in the 1936 election.

Identifying a suitable auxiliary variable for weighting is not always straightforward. Gelman's [project](https://www.washingtonpost.com/news/monkey-cage/wp/2014/04/09/tracking-public-opinion-with-biased-polls/) using an opt-in poll from the Xbox gaming platform before the 2012 presidential election. This sample was unrepresentative, skewed towards younger and more male respondents. Gelman and his team chose to use party ID to reweight the data. Partisanship is not often used for weighting because it varies over time and is not readily available from census data where demographic variables are often chosen for weighting. Despite these challenges, the results are impressive -- the adjusted estimates aligned well with forecasts from leading poll analysts!

## Weighting does not solve everything

Despite the amazing power that weighting can bring in, it is not a solution to every problem.

## Unweighted data can be preferred to weighted data

To weight or not to weight, here are some considerations:

**References:**

-   Andrew Gelman and David Rothschild, 2014. "Modern polling needs innovation, not traditionalism". The Washington Post.

-   Andrew Gelman, 2014. "Tracking public opinion with biased polls". The Washington Post.

-   Sharon L. Lohr and J. Michael Brick, 2017. "Roosevelt Predicted to Win: Revisiting the 1936 Literary Digest Poll". Statistical, Politics and Policy, 8(1): 65-84.

-   Thomas Lumley, 2024. "[Importance weights](https://notstatschat.rbind.io/2024/04/19/importance-weights/)"

-   Thomas Lumley, 2020. "[Weights in statistics](https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/)"
