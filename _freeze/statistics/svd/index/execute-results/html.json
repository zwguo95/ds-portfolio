{
  "hash": "2bf76b509120fff7b4815fe820d71c5f",
  "result": {
    "markdown": "---\ntitle: \"The Power of Singular Vector Decomposition: A Beginner's Guide\"\ndate: 03/17/2024\nauthor:\n  - name: Zhaowen Guo\n    url: \"https://www.linkedin.com/in/zhaowen-guo-20535312a/\"\ntitle-block-banner: true\nformat:\n  html:\n    theme: flatly\n    code-fold: false\n    code-tools: false\n    toc: false\n    number-sections: false\nlink-citations: true\ncategories: [code, machine learning]\nimage: \"svd_youtube.jpg\"\n---\n\n::: {.cell}\n\n:::\n\n\nSVD is not nearly as famous as it should be. --- Gilbert Strang\n\nSVD is a great 1-stop shop for data analysis. --- Daniela Witten\n\n## Introduction\n\nSingular Vector Decomposition (SVD) is a matrix factorization technique that has become a cornerstone in the field of machine learning (ML). It not only allows for efficiently calculating the inverse of a matrix (if it exists) by multiplying the inverse of each decomposed simpler matrices, but also opens the door to a wide array of applications in ML and beyond.\n\nIn what follows, I will start by the definition and properties of SVD, and establish its connection with Principal Component Analysis (PCA). Then I will demonstrate different applications of SVD in ML, including but not limited to missing value imputation and latent feature extraction.\n\n### Definition and properties of SVD\n\nSVD decomposes a data matrix $X_{m \\times n}$ into three matrices $U_{m\\times r}$, $D_{r\\times r}$, and $V_{n\\times r}$, regardless of the characteristics of the original matrix.\n\n$$\nX = UDV^T\n$$ where\n\n-   U and V are orthogonal matrices ($U^T U = I$ and $V^T V = I$), which are called [left singular vector]{style=\"text-decoration:underline\"}, and [right singular vector]{style=\"text-decoration:underline\"}, respectively\n-   D is a diagonal matrix with non-negative and decreasing elements, which are called [singular values]{style=\"text-decoration:underline\"}\n\nLet's first check dimensions of the resulting matrices after applying SVD to a toy matrix X.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a matrix\nX <- matrix(c(1:12),\n            nrow = 4,\n            ncol = 3,\n            byrow = T)\n\n# Apply SVD\nsvd_result <- svd(X)\n\n# Extract U, D, and V matrices\nU <- svd_result$u\nD <- diag(svd_result$d)\nV <- svd_result$v\nprint(paste0(\"The dimension for U matrix: \", dim(U)[1], \" X \", dim(U)[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The dimension for U matrix: 4 X 3\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"The dimension for D matrix: \", dim(D)[1], \" X \", dim(D)[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The dimension for D matrix: 3 X 3\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"The dimension for V matrix: \", dim(V)[1], \" X \", dim(V)[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The dimension for V matrix: 3 X 3\"\n```\n:::\n:::\n\n\nWe can then check matrix properties of SVD. As we can observe, matrices U and V are orthogonal, and matrix D is diagonal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check properties of U and V (orthogonal matrix)\nis_orthogonal <- function(A){\n  A_T <- t(A)\n  dot_product_1 <- A %*% A_T\n  dot_product_2 <- A_T %*% A\n  identity_matrix_1 <- diag(nrow(A))\n  identity_matrix_2 <- diag(ncol(A))\n  \n  result <- isTRUE(all.equal(dot_product_1, identity_matrix_1)) +\n            isTRUE(all.equal(dot_product_2, identity_matrix_2)) # all.equal checks \"nearly equal\"\n  \n  return(result>=1)\n}\n\nis_orthogonal(U) # TRUE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nis_orthogonal(V) # TRUE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# Check properties of D\ndiag(D) # diagonal values (or singular values in this case)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.546241e+01 1.290662e+00 2.311734e-15\n```\n:::\n\n```{.r .cell-code}\nD[!row(D) == col(D)] # off-diagonal values are 0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 0 0 0 0 0\n```\n:::\n:::\n\n\n### Connection between SVD and PCA\n\nNow, knowing that SVD can be used to approximate any matrix, it's an opportune moment to revisit Principal Component Analysis (PCA), an unsupervised ML method that we might be more familiar with. As we will see, [SVD on a de-meaned (centered) data matrix is the same as PCA]{style=\"text-decoration:underline\"}.\n\nRecall that PCA seeks to find principal components, or the direction in the feature space with maximum variance in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Center the data matrix (column means are 0)\nX_centered <- scale(X, center = T, scale = T)\ncolMeans(X_centered) # check if centered\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 0 0\n```\n:::\n\n```{.r .cell-code}\n# Apply SVD to the centered data matrix\nsvd_result <- svd(X_centered)\n\n# Apply PCA to the data\npca_result <- prcomp(X, scale. = T)\n```\n:::\n\n\nAs we can see, columns of the right singular vector V correspond to principal components extracted from PCA, and SVD also yields less elapsed time than PCA. A key advantage of SVD is that it does not require a preliminary step of constructing a covariance as PCA does, providing greater computational efficiency in extracting principal components.\n\nThis efficiency becomes particularly prominent when handling\n\n-   High-dimensional datasets: when a data matrix possess too many features, the computational cost for constructing its covariance matrix can be huge\n\n-   Full-rank data matrix: when the data matrix is full-rank, it often implies that many singular values will be non-negligible, and many principal components will be needed to reconstruct the original matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(svd_result$v) # right singular vectors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]       [,2]       [,3]\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n```\n:::\n\n```{.r .cell-code}\nprint(pca_result$rotation) # principal components\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           PC1        PC2        PC3\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct a high-dimensional and sparse data matrix\nn_rows <- 1000\nn_cols <- 500\n\nsparse_matrix <- matrix(0, nrow = n_rows, ncol = n_cols)\n\n# Manually add some non-zero elements to mimic sparsity\nset.seed(123)\nnon_zero_elements <- 200\nfor (i in 1:non_zero_elements) {\n  row_index <- sample(n_rows, 1)\n  col_index <- sample(n_cols, 1)\n  sparse_matrix[row_index, col_index] <- runif(1)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute every possible rank approximations\nsystem.time({\n  svd_result <- svd(sparse_matrix)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n   0.19    0.00    0.58 \n```\n:::\n\n```{.r .cell-code}\nsystem.time({\n  pca_res <- prcomp(sparse_matrix)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n   0.34    0.00    0.72 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute top 10 rank approximations\nsystem.time({\n  svd_result <- irlba(sparse_matrix, nv = 10)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n   0.02    0.00    0.03 \n```\n:::\n\n```{.r .cell-code}\nsystem.time({\n  pca_res <- prcomp(sparse_matrix, rank. = 10)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n   0.39    0.00    0.61 \n```\n:::\n:::\n\n\n\n## Application: Impute Missing Values\n\nOne popular application of SVD is to impute missing values. Without keeping all singular values and vectors, we can just retain the first d largest singular values to approximate the matrix A. The intuition is that the approximated matrix $A_d$ being a dense matrix that captures the primary structure and patterns in the original data.\n\nThis procedure is also called lower-rank approximation, which can be implemented in the following steps:\n\n-   Matrix approximation: fill in NAs with an initial guess (e.g. column means, zeros) and apply SVD with rank d, meaning that we only keep top d singular values and vectors\n\n-   Missingness imputation: use the approximated matrix $A_d$ to fill in NAs in the original matrix\n\nLet's use the following example for illustration:\n\nWe start by creating a toy data matrix A and call it our ground truth matrix. Then we manually add sparsity by replacing certain elements with NAs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a toy dataset (sparse matrix)\nset.seed(123)\nA <- matrix(sample(c(NA, 1:5), 25, replace = T), 5, 5)\nground_truth_matrix <- A\nA[c(2, 8, 10, 14, 20)] <- NA\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]   NA    2   NA    2    4\n[3,]    2   NA    1   NA    2\n[4,]    1    3   NA    3    1\n[5,]    1   NA    4   NA    1\n```\n:::\n\n```{.r .cell-code}\nground_truth_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]    5    2   NA    2    4\n[3,]    2    4    1   NA    2\n[4,]    1    3    2    3    1\n[5,]    1    5    4   NA    1\n```\n:::\n:::\n\n\nNext, we apply SVD with varying d, which indicates the number of singular values/vectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define svd\nimpute_svd <- function(matrix, d){\n  \n  # fill in missingness with column means\n  column_means <- colMeans(matrix, na.rm = T)\n  matrix_filled <- matrix\n  na_indices <- is.na(matrix)\n  matrix_filled[na_indices] <- column_means[col(matrix)[na_indices]]\n  \n  # perform svd\n  svd_res <- svd(matrix_filled)\n  svd_res$d <- c(svd_res$d[1:d], rep(0, length(svd_res$d) - d))\n  \n  # reconstruct the matrix\n  approx_matrix <- svd_res$u %*% diag(svd_res$d) %*% t(svd_res$v)\n  imputed_vals <- approx_matrix\n  imputed_vals[!is.na(matrix)] <- NA\n  return(imputed_vals)\n}\n```\n:::\n\n\nWe can use the approximated matrix $A_d$ to reconstruct the original matrix and impute missing values. We can evaluate the performance of missingness imputation by mean squared error (MSE).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct the metric MSE\nmse <- function(predicted, truth) {\n  mean((predicted - truth)^2, na.rm = TRUE)\n}\n\n# Display MSE for different d values (rank, or number of dimensions to define the reduced matrix)\nsvd_errors <- numeric(5)\nfor (d in 1:5) {\n  imputed_values <- impute_svd(A, d)\n  svd_errors[d] <- mse(imputed_values, ground_truth_matrix)\n}\n```\n:::\n\n\nHow does SVD perform? As a baseline, consider a simple approach by replacing missing values with column means. It seems that rank-2 approximation is an optimal choice, which yields the lowest MSE. However, it's important to note that it is not always the case that SVD approximation would outperform simple column mean imputation. We might need to consider other matrix decomposition techniques for missingness imputation, such as Non-negative Matrix Factorization (NMF), Alternating Least Squares (ALS), etc..\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create baseline imputation from column means\nna_indices <- is.na(A)\ncolmean_matrix <- A\ncolmean_matrix[na_indices] <- colMeans(A, na.rm = T)[col(A)[na_indices]]\ncolmean_errors <- mse(colmean_matrix[na_indices], ground_truth_matrix[na_indices])\n\n# Report comparison of performance\ncomparison <- tibble(\"Method\" = c(\"Column Means\", \n                                  \"Rank-1 Approximation\",\n                                  \"Rank-2 Approximation\",\n                                  \"Rank-3 Approximation\",\n                                  \"Rank-4 Approximation\",\n                                  \"Rank-5 Approximation\"),\n                     \"MSE\" = c(colmean_errors, svd_errors))\n\ncomparison %>%\n  kbl() %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Method </th>\n   <th style=\"text-align:right;\"> MSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Column Means </td>\n   <td style=\"text-align:right;\"> 4.312500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rank-1 Approximation </td>\n   <td style=\"text-align:right;\"> 4.754839 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rank-2 Approximation </td>\n   <td style=\"text-align:right;\"> 4.094591 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rank-3 Approximation </td>\n   <td style=\"text-align:right;\"> 4.146353 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rank-4 Approximation </td>\n   <td style=\"text-align:right;\"> 4.319335 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rank-5 Approximation </td>\n   <td style=\"text-align:right;\"> 4.312500 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nIn line with the idea of missingness imputation, SVD can also be leveraged to enhance recommendation systems! The goal is to predict unknown preferences or ratings of users for items (e.g. movies, products, or services) based on existing ratings. A notable example is [Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize#:~:text=On%20September%2021%2C%202009%2C%20the,for%20predicting%20ratings%20by%2010.06%25.), where Netflix offered \\$1 million award to anyone who could improve the accuracy of its movie recommendation system by 10%. The [winning team](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009_BPC_BigChaos.pdf) just used SVD, along with techniques that incorporate other metadata, achieving a 10.06% improvement!\n\n## Application: Topic Modeling\n\nSVD is a powerful and generalizable technique that provides us another perspective on topic modeling. We begin by first transforming documents into a document-term matrix, where each row represents a document, each column reflects a term, and each cell denotes frequency. To refine this step further, we can also apply Term Frequency-Inverse Document Frequency [(TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to reweigh the cell values, adjusting for the uniqueness of each term for a given document.\n\nSVD can then be perceived as decomposing a document-term matrix $X_{m \\times n}$ into\n\n-   $U_{m \\times r}$: document-topic matrix\n\n-   $D_{r \\times r}$: diagonal elements represent topic importance\n\n-   $V_{n \\times r}$: term-topic matrix\n\nFor topic modeling, a crucial hyperparameter that requires tuning is the number of topics (often denoted by k). In the context of SVD, the idea is equivalent to selecting the top k singular values and their corresponding singular vectors in order to approximate the original data matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct a document-term matrix\nlibrary(tidytext)\nlibrary(tm)\ndocuments <- tibble(\n  doc_id = 1:8,\n  text = c(\"The sky is blue and beautiful.\",\n           \"Love this blue and beautiful sky!\",\n           \"The quick brown fox jumps over the lazy dog.\",\n           \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n           \"I love green eggs, ham, sausages, and bacon!\",\n           \"The brown fox is quick and the blue dog is lazy!\",\n           \"The sky is very blue and the sky is very beautiful today\",\n           \"The dog is lazy but the brown fox is quick!\")\n)\n\ntidy_documents <- documents %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\n\ndtm <- tidy_documents %>%\n  count(doc_id, word) %>%\n  cast_dtm(doc_id, word, n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply SVD and examine each decomposed matrix\nsvd_result <- svd(as.matrix(dtm))\n\nk <- 2 # choose k=2 for simplicity\nUk <- svd_result$u[, 1:k]\nDk <- svd_result$d[1:k]\nVk <- svd_result$v[, 1:k]\n```\n:::\n\n\nAs we can see, the decomposed $U_k$ matrix captures documents by topics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUs <- tibble(`Document ID` = 1:8,\n             `Topic 1` = Uk[,1],\n             `Topic 2` = Uk[,2])\nUs %>%\n  kbl() %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Document ID </th>\n   <th style=\"text-align:right;\"> Topic 1 </th>\n   <th style=\"text-align:right;\"> Topic 2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -0.1294362 </td>\n   <td style=\"text-align:right;\"> 0.4303175 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> -0.1392703 </td>\n   <td style=\"text-align:right;\"> 0.4926330 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> -0.5597761 </td>\n   <td style=\"text-align:right;\"> -0.1933906 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> -0.0088357 </td>\n   <td style=\"text-align:right;\"> 0.2551944 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> -0.0175544 </td>\n   <td style=\"text-align:right;\"> 0.2534139 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> -0.5889438 </td>\n   <td style=\"text-align:right;\"> -0.0537521 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> -0.1672636 </td>\n   <td style=\"text-align:right;\"> 0.6091753 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> -0.5246738 </td>\n   <td style=\"text-align:right;\"> -0.1772371 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe singular values are stored in the following matrix $D_k$, which correspond to how important each topic is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD_matrix <- diag(Dk)\nrownames(D_matrix) <- c(\"Topic 1\", \"Topic 2\")\ncolnames(D_matrix) <- c(\"Topic 1\", \"Topic 2\")\n\nD_matrix %>%\n  kbl() %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Topic 1 </th>\n   <th style=\"text-align:right;\"> Topic 2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Topic 1 </td>\n   <td style=\"text-align:right;\"> 3.993368 </td>\n   <td style=\"text-align:right;\"> 0.000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Topic 2 </td>\n   <td style=\"text-align:right;\"> 0.000000 </td>\n   <td style=\"text-align:right;\"> 3.460071 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe $V_k$ matrix represents terms by topics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterms <- colnames(dtm)\nV_matrix <- tibble(`Term` = terms,\n                   `Topic 1` = Vk[,1],\n                   `Topic 2` = Vk[,2])\n\nV_matrix %>%\n  kbl() %>%\n  kable_styling()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:right;\"> Topic 1 </th>\n   <th style=\"text-align:right;\"> Topic 2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> beautiful </td>\n   <td style=\"text-align:right;\"> -0.1091735 </td>\n   <td style=\"text-align:right;\"> 0.4428019 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> blue </td>\n   <td style=\"text-align:right;\"> -0.2566540 </td>\n   <td style=\"text-align:right;\"> 0.4272669 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sky </td>\n   <td style=\"text-align:right;\"> -0.1510589 </td>\n   <td style=\"text-align:right;\"> 0.6188605 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> love </td>\n   <td style=\"text-align:right;\"> -0.0392713 </td>\n   <td style=\"text-align:right;\"> 0.2156161 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> brown </td>\n   <td style=\"text-align:right;\"> -0.4190432 </td>\n   <td style=\"text-align:right;\"> -0.1226506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dog </td>\n   <td style=\"text-align:right;\"> -0.4190432 </td>\n   <td style=\"text-align:right;\"> -0.1226506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> fox </td>\n   <td style=\"text-align:right;\"> -0.4190432 </td>\n   <td style=\"text-align:right;\"> -0.1226506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> jumps </td>\n   <td style=\"text-align:right;\"> -0.1401764 </td>\n   <td style=\"text-align:right;\"> -0.0558921 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> lazy </td>\n   <td style=\"text-align:right;\"> -0.4190432 </td>\n   <td style=\"text-align:right;\"> -0.1226506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> quick </td>\n   <td style=\"text-align:right;\"> -0.4190432 </td>\n   <td style=\"text-align:right;\"> -0.1226506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> bacon </td>\n   <td style=\"text-align:right;\"> -0.0066085 </td>\n   <td style=\"text-align:right;\"> 0.1469936 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> beans </td>\n   <td style=\"text-align:right;\"> -0.0022126 </td>\n   <td style=\"text-align:right;\"> 0.0737541 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> breakfast </td>\n   <td style=\"text-align:right;\"> -0.0022126 </td>\n   <td style=\"text-align:right;\"> 0.0737541 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> eggs </td>\n   <td style=\"text-align:right;\"> -0.0066085 </td>\n   <td style=\"text-align:right;\"> 0.1469936 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ham </td>\n   <td style=\"text-align:right;\"> -0.0066085 </td>\n   <td style=\"text-align:right;\"> 0.1469936 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> king's </td>\n   <td style=\"text-align:right;\"> -0.0022126 </td>\n   <td style=\"text-align:right;\"> 0.0737541 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sausages </td>\n   <td style=\"text-align:right;\"> -0.0066085 </td>\n   <td style=\"text-align:right;\"> 0.1469936 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> toast </td>\n   <td style=\"text-align:right;\"> -0.0022126 </td>\n   <td style=\"text-align:right;\"> 0.0737541 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> green </td>\n   <td style=\"text-align:right;\"> -0.0043959 </td>\n   <td style=\"text-align:right;\"> 0.0732395 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nNow, we can examine top 5 terms associated with each topic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_terms <- apply(Vk, 2, function(x) terms[order(abs(x), decreasing = TRUE)[1:5]])\nprint(top_terms)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]    [,2]       \n[1,] \"brown\" \"sky\"      \n[2,] \"dog\"   \"beautiful\"\n[3,] \"fox\"   \"blue\"     \n[4,] \"lazy\"  \"love\"     \n[5,] \"quick\" \"bacon\"    \n```\n:::\n:::\n\n\nBeyond what has been discussed, some other cool applications of SVD in NLP include: information retrieval via [Latent Semantic Analysis](https://link.springer.com/chapter/10.1007/978-981-99-3243-6_45) and word co-occurrence detection in [word embeddings](https://smltar.com/embeddings.html) and other downstream tasks (e.g. [text classification](https://ideas.repec.org/p/sek/iacpro/0702094.html)). Feel free to explore!\n\n**References and additional resources:**\n\n-   A wonderful twitter [thread](https://twitter.com/WomenInStat/status/1285612667839885312) on SVD by Daniela Witten (a nice summary can be found [here](https://www.govindgnair.com/post/svd-is-almost-all-you-need/))\n\n-   A cool geometric [interpretation](https://www.youtube.com/watch?v=vSczTbgc8Rc) of SVD\n\n-   A nice [tutorial](https://www.youtube.com/watch?v=lRZ4aMaXPBI&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=3) illustrating the connection between SVD and topic modeling using Python\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}