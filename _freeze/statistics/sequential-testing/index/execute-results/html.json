{
  "hash": "f0cb325525d79c834989615232d029b0",
  "result": {
    "markdown": "---\ntitle: \"Sequential Testing and Early Stopping\"\ndate: 2/24/2025\nauthor:\n  - name: Zhaowen Guo\n    url: \"https://www.linkedin.com/in/zhaowen-guo-20535312a/\"\ntitle-block-banner: true\nformat:\n  html:\n    theme: flatly\n    code-fold: true\n    code-tools: false\n    toc: false\n    number-sections: false\nlink-citations: true\ncategories: [code, statistics, experimental design]\nimage: \"peeking_illustration.png\"\n---\n\n\nA/B testing is a cornerstone of data-driven decision-making, allowing business to evaluate new features and optimize user experiences. Whether it's tweaking a website's layout, or testing new content recommendations, A/B testing provides insightful feedback on what works and what doesn't.\n\nConventionally, A/B tests follow a ***fixed sample size approach***---running until a set number of users have been exposed before analyzing results. However, waiting for the full sample can be inefficient and costly in many real-world scenarios.\n\nFor example, if a new recommendation algorithm significantly boosts engagement, delaying its rollout until the test is complete means lost revenue and user satisfaction. In contrast, if an update unintentionally promotes harmful content, running the test to completion could expose more users to harm before corrective action is taken.\n\nThis is where sequential testing and early stopping come in. Instead of waiting for the full sample, this allows continuous monitoring of test results. Experiments can stop early when there's ***enough*** evidence to confirm a winner---or to prevent harm.\n\n# Peeking Problem\n\nThe question then becomes: what counts as \"enough\" evidence?\n\nIf we repeatedly apply fixed-sample-size tests (e.g. t-tests, Kolmogorov-Smirnov test, Mann-Whitney tests) at multiple points until significance is reached, we increase the likelihood of false positives---a phenomenon known as the peeking problem.\n\nTo illustrate, suppose we set our significance level at $\\alpha=0.05$, meaning there's a 5% chance of detecting a significant effect when no effect exists. If we check results once at the end of the test, this error rate holds. However, if we peek multiple times ($k$ peeks), the probability of making at least one false positive error grows exponentially, following:\n\n$$\n\\alpha_{\\text{inflated}} = 1 - (1 - \\alpha)^k\n$$\n\nThis becomes even clearer with a simulation. Assuming no real difference between treatment and control, I generated 1000 observations per group and run 100 simulations. At regular checkpoints every 10 observations, I applied t-tests, Mann-Whitney tests, and Kolmogorov-Smirnov tests, recording how often their p-values fall below $\\alpha=0.05$.\n\nIn the plot below, red lines represent paths of simulated p-values, while black dots indicate points where significance was falsely detected. Y axis is log-transformed to those black dots to be seen. Across all three tests, the false positive rates are highly inflated---30% for t-tests, 32% for Mann-Whitney tests, and 26% for Kolmogorov-Smirnov tests---far exceeding the 5% threshold.\n\n![](peeking_illustration.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nlibrary(tidyverse)\n\n# Simulation parameters\nn_simulations <- 100      \nn_observations <- 1000    \nalpha <- 0.05\ncheckpoints <- seq(10, n_observations, by=10)  \n\n# Store p-values\np_values_t <- matrix(NA, nrow=n_simulations, ncol=length(checkpoints))  \np_values_mw <- matrix(NA, nrow=n_simulations, ncol=length(checkpoints)) \np_values_ks <- matrix(NA, nrow=n_simulations, ncol=length(checkpoints)) \n\nfalse_positive_count_t <- 0   \nfalse_positive_count_mw <- 0  \nfalse_positive_count_ks <- 0  \n\n# Run A/A test simulations\nfor (sim in 1:n_simulations) {\n\n  control <- rnorm(n_observations, mean=0, sd=1)\n  treatment <- rnorm(n_observations, mean=0, sd=1)\n  \n  for (i in seq_along(checkpoints)) {\n    n <- checkpoints[i]\n    \n    p_values_t[sim, i]  <- t.test(control[1:n], treatment[1:n])$p.value\n    p_values_mw[sim, i] <- wilcox.test(control[1:n], treatment[1:n])$p.value\n    p_values_ks[sim, i] <- ks.test(control[1:n], treatment[1:n])$p.value\n  }\n  \n  if (any(p_values_t[sim, ] < alpha)) false_positive_count_t <- false_positive_count_t + 1\n  if (any(p_values_mw[sim, ] < alpha)) false_positive_count_mw <- false_positive_count_mw + 1\n  if (any(p_values_ks[sim, ] < alpha)) false_positive_count_ks <- false_positive_count_ks + 1\n}\n\n# Compute false positive rates\nfalse_positive_rate_t <- false_positive_count_t / n_simulations\nfalse_positive_rate_mw <- false_positive_count_mw / n_simulations\nfalse_positive_rate_ks <- false_positive_count_ks / n_simulations\n\n# Convert to long format for visualization\ndf_t <- as.data.frame(p_values_t)\ncolnames(df_t) <- checkpoints\ndf_t$Simulation <- 1:n_simulations\ndf_long_t <- df_t %>%\n  pivot_longer(cols = -Simulation, names_to = \"Sample_Size\", values_to = \"P_Value\") %>%\n  mutate(Sample_Size = as.numeric(Sample_Size))\n\n# Plot the peeking problem\nggplot(df_long_t, aes(x=Sample_Size, y=P_Value, group=Simulation)) +\n  geom_line(color=\"red\", alpha=0.3) +\n  geom_point(data=df_long_t %>% filter(P_Value < 0.05), \n             aes(x=Sample_Size, y=P_Value), color=\"black\", size=1) +\n  geom_hline(yintercept=0.05, linetype=\"dashed\", color=\"black\") +\n  scale_y_log10() + \n  labs(title=\"Illustration of the Peeking Problem\",\n       x=\"Sample Size\",\n       y=\"p-value\") +\n  theme_minimal()\n\nggsave(\"peeking_illustration.png\", width = 7, height = 6)\n```\n:::\n\n\n# Sequential Testing\n\nHow do sequential testing methods reduce false positive rates while allowing early stopping when evidence is strong? This table below provides a non-exhaustive overview of different sequential testing approaches, focusing on how they determine when evidence is sufficient to stop, and how they mitigate Type I error in the meantime.\n\n| Method                         | Example                                                                             | Stopping Criterion                                                                           | Control Type I Error                                                                                                                    |\n|------------------|------------------|------------------|-------------------|\n| Likelihood Ratio-Based Methods | Wald's Sequential Probability Ratio Test (SPRT), 2-SPRT, Sequential Triangular Test | Compare likelihood ratios of $H_0$ vs. $H_1$ and stop when it crosses a predefined threshold | Set decision boundaries (A, B) for likelihood ratios                                                                                    |\n| Group Sequential Testing       | Alpha Spending Functions (Pocock Test, O'Brien-Fleming Test, Lan-DeMets)            | Preplanned interim analyses adjust $\\alpha$ and stop when a checkpoint reaches significance  | Preallocate $\\alpha$ over checkpoints (e.g. constant $\\alpha$, from conservative to liberal, dynamic adjustment based on observed data) |\n| Distribution-Based Testing     | Confidence Sequences (Sequential P-Values) for Continuous or Count Data             | Monitor distributional changes and stop when distribution-difference bands exclude zero      | Construct confidence consequences that shrink dynamically, ensuring anytime-valid inference without alpha-spending                      |\n\nLikelihood ratio-based and group sequential testing methods rely on classical statistical tests that compare means or proportions over time. They control false positive error rates by predefining when to analyze the data---either by setting decision boundaries or by adjusting significance levels ($\\alpha$ ) at interim checkpoints. However, both approaches require assumptions about the underlying data distribution and a predefined schedule for checkpoints.\n\nIn contrast, distribution-based testing tracks entire distributions rather than just summary statistics like means or proportions. This approach is more generalizable without depending on assumptions about finite moments---an issue for mean-based tests when applied to heavy-tailed distributions like the Cauchy. Additionally, distribution-based methods allow for continuous monitoring without the need for preplanned interim analyses or alpha-spending adjustments, making them particularly effective for real-time decision making.\n\n## Illustration of Early Stopping\n\nTo demonstration how sequential testing methods enable early stopping, I conducted a simulation reflecting a common A/B testing scenario: user click behavior. The goal is to compare sequential testing approaches---Group Sequential Testing (GST) and Distribution-Based Testing using sequential p-values---against a fixed sample size approach determined via traditional power analysis.\n\nI consider a simple two-arm experiment where users are randomly assigned to either a control or treatment group. The treatment effect is assumed to increase the probability of a user clicking by 5 percentage points---from 30% in control to 35% in treatment.\n\nHere's how each sequential testing method works:\n\n-   Group Sequential Testing (GST)\n\n    -   I assume 10 interim peeks and apply Pocock's alpha spending function, which equally distributes the total significance level (α=0.05) across all peeks. The test stops at the first interim check where the treatment effect reaches significance.\n\n-   Distribution-Based Testing\n\n    -   Click rates are monitored continuously at every observation, without predefined checkpoints or alpha spending. The test stops as soon as the Dirichlet sequential p-value falls below $\\alpha = 0.05$\n\nAs shown in the graph below, both sequential testing methods stopped early, requiring fewer samples to detect the effect compared to a fixed-sample approach with no interim analysis.\n\n![](illustration_early_stopping.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pwr)\n\n# Define clicking probabilities\npA <- 0.30 \npB <- 0.35\n\n# Cohen's h for difference in two proportions\ncohen_h <- function(p1, p2) {\n  2 * (asin(sqrt(p1)) - asin(sqrt(p2)))\n}\n\neffect_size_h <- cohen_h(pA, pB)\neffect_size_h\n\n# Compute required sample size\npower_calc <- pwr.2p.test(h = effect_size_h,\n                          sig.level = 0.05,\n                          power = 0.8,\n                          alternative = \"two.sided\")\n\npower_calc # ~1376 required per group\n\n# Generate synthetic click data with cumulative counts per arm\ngenerate_click_data <- function(max_n, pA, pB) {\n  A_raw <- rbinom(max_n, size = 1, prob = pA) \n  B_raw <- rbinom(max_n, size = 1, prob = pB)\n  \n  data.frame(\n    A_clicks = cumsum(A_raw),\n    B_clicks = cumsum(B_raw),\n    Sample_Size = 1:max_n\n  )\n}\n\n# Group sequential test with Pocock function\ngst_test <- function(counts, check_index, total_checks, alpha) {\n  alpha_check <- alpha / total_checks\n  total <- sum(counts)\n  if (total < 2) return(FALSE)\n\n  # 2-proportion z-test\n  pA <- counts[1] / total\n  pB <- counts[2] / total\n  p_hat <- (counts[1] + counts[2]) / (2 * total)\n  se <- sqrt(p_hat * (1 - p_hat) * (1 / counts[1] + 1 / counts[2]))\n  if (se == 0) return(FALSE)\n\n  z_stat <- (pA - pB) / se\n  p_val <- 2 * (1 - pnorm(abs(z_stat)))\n  return(p_val < alpha_check)\n}\n\n# Distribution-based sequential p-values, adapted from https://gist.github.com/michaellindon/5ce04c744d20755c3f653fbb58c2f4dd\nsequential_p_value <- function(counts, assignment_probabilities = c(0.5, 0.5),\n                              dirichlet_alpha = NULL, alpha = 0.05) {\n  counts <- as.numeric(counts)\n  if (is.null(dirichlet_alpha)) {\n    dirichlet_alpha <- 100 * assignment_probabilities\n  }\n  total_counts <- sum(counts)\n  alpha_sum <- sum(dirichlet_alpha)\n  \n  lm1 <- (\n    lgamma(total_counts + 1) -\n      sum(lgamma(counts + 1)) +\n      lgamma(alpha_sum) -\n      sum(lgamma(dirichlet_alpha)) +\n      sum(lgamma(dirichlet_alpha + counts)) -\n      lgamma(alpha_sum + total_counts)\n  )\n  \n  lm0 <- lgamma(total_counts + 1) +\n    sum(counts * log(assignment_probabilities) - lgamma(counts + 1))\n  \n  pval <- exp(lm0 - lm1)\n  return(min(1, pval) < alpha)\n}\n\nsimulate_early_stopping <- function(max_n, alpha, pA, pB, num_gst_peeks = 10) {\n  df <- generate_click_data(max_n, pA, pB)\n  peeking_points <- seq(ceiling(max_n / num_gst_peeks), max_n, length.out = num_gst_peeks)\n\n  gst_stop <- NA\n  dirichlet_stop <- NA\n  \n  effect_est <- numeric(max_n)\n  ci_lower <- numeric(max_n)\n  ci_upper <- numeric(max_n)\n\n  for (i in seq_len(max_n)) {\n    counts <- c(df$A_clicks[i], df$B_clicks[i])\n\n    effect_est[i] <- log((counts[2] + 1) / (counts[1] + 1))\n\n    se <- sqrt((1 / (counts[1] + 1)) + (1 / (counts[2] + 1)))\n    ci_lower[i] <- effect_est[i] - 1.96 * se\n    ci_upper[i] <- effect_est[i] + 1.96 * se\n\n    if (i %in% peeking_points && is.na(gst_stop)) {\n      idx <- which(peeking_points == i)\n      if (gst_test(counts, idx, num_gst_peeks, alpha)) {\n        gst_stop <- i\n      }\n    }\n\n    if (is.na(dirichlet_stop)) {\n      if (sequential_p_value(counts, c(0.5, 0.5), alpha = alpha)) {\n        dirichlet_stop <- i\n      }\n    }\n  }\n  if (is.na(gst_stop)) gst_stop <- max_n\n  if (is.na(dirichlet_stop)) dirichlet_stop <- max_n\n\n  list(\n    df = data.frame(\n      Sample_Size = 1:max_n,\n      Effect_Est = effect_est,\n      CI_Lower = ci_lower,\n      CI_Upper = ci_upper\n    ),\n    gst_stop = gst_stop,\n    dirichlet_stop = dirichlet_stop\n  )\n}\n\nresults <- simulate_early_stopping(\n  max_n = 2000,\n  alpha = 0.05,\n  pA = 0.30,\n  pB = 0.35,\n  num_gst_peeks = 10\n)\n\ndf <- results$df\ngst_stop <- results$gst_stop\ndirichlet_stop <- results$dirichlet_stop\n\nggplot(df, aes(x = Sample_Size, y = Effect_Est)) +\n  geom_ribbon(aes(ymin = CI_Lower, ymax = CI_Upper),\n              fill = \"blue\", alpha = 0.2) +\n  geom_line(color = \"blue\") +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"black\") +\n  geom_vline(xintercept = gst_stop, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = dirichlet_stop, linetype = \"dashed\", color = \"green\") +\n  geom_vline(xintercept = 1376, linetype = \"dashed\", color = \"black\") +\n  annotate(\"text\", x = gst_stop, y = max(df$Effect_Est),\n           label = \"GST Stop\", color = \"red\", vjust = -1, hjust = 1.2, size = 4) +\n  annotate(\"text\", x = dirichlet_stop, y = max(df$Effect_Est),\n           label = \"Dirichlet Stop\", color = \"green\", vjust = -1, hjust = 0, size = 4) +\n  annotate(\"text\", x = 1376, y = max(df$Effect_Est),\n           label = \"Original Stop\", color = \"black\", vjust = -1, hjust = 0, size = 4) +\n  labs(\n    title = \"Illustration of Early Stopping\",\n    subtitle = \"Simulation: pA=0.30, pB=0.35, alpha=0.05\",\n    x = \"Sample Size\",\n    y = \"Estimated Log Click Rate Difference\"\n  ) +\n  theme_minimal()\n\nggsave(\"illustration_early_stopping.png\", width = 7, height = 6)\n```\n:::\n\n\n## Illustration of Mitigating False Positives\n\nSequential testing not only improves efficiency by enabling early stopping when a true effect exists, but it also helps mitigate false positives. To evaluate how different methods control false positive rates, I conducted another simulation under the same user clicking behavior setting—this time assuming no true effect. The goal is to track how false positive rates change as the number of peeks increases.\n\nIn addition to Group Sequential Testing (GST) and Distribution-Based Testing, I compare their performance against Naive Peeking, where a standard hypothesis test (e.g., a two-proportion z-test) is applied at each interim analysis without any correction for multiple looks.\n\nAs presented in the graph below, Naive Peeking quickly inflates false positives beyond 0.05, worsening as more peeks occur. In contrast, both GST and Distribution-Based Testing effectively control Type I error, keeping false positives at or below 0.05. Distribution-Based Testing provides even tighter error control, ensuring anytime-valid inference while allowing continuous monitoring.\n\n![](false_positive_comparison.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnaive_test <- function(counts) {\n  A_clicks <- counts[1]\n  B_clicks <- counts[2]\n  total_clicks <- A_clicks + B_clicks\n  if (total_clicks < 2) {\n    return(FALSE)\n  }\n  \n  pA <- A_clicks / total_clicks\n  pB <- B_clicks / total_clicks\n  p_hat <- (A_clicks + B_clicks) / (total_clicks + total_clicks)\n  \n  z_num <- pA - pB\n  z_den <- sqrt(p_hat * (1 - p_hat) * (1/total_clicks + 1/total_clicks))\n  if (z_den == 0) {\n    return(FALSE)\n  }\n  z_stat <- z_num / z_den\n  p_val <- 2 * (1 - pnorm(abs(z_stat)))\n  \n  (p_val < 0.05)\n}\n\nsimulate_one_run <- function(n_users = 2000, num_peeks = 10, alpha = 0.05, pA = 0.3, pB = 0.3) {\n  df <- generate_click_data(n_users, pA, pB)\n  \n  step <- floor(n_users / num_peeks)\n  if (step < 1) stop(\"num_peeks too large for n_users\")\n  peeking_points <- seq(step, n_users, by = step)\n  total_checks <- length(peeking_points)\n  \n  reject_dirichlet <- FALSE\n  reject_pocock <- FALSE\n  reject_naive <- FALSE\n  \n  for (i in 1:n_users) {\n    counts <- c(df$A_clicks[i], df$B_clicks[i])\n    \n    if (!reject_dirichlet && sequential_p_value(counts, c(0.5, 0.5), alpha = alpha)) {\n      reject_dirichlet <- TRUE\n    }\n    \n\n    if (any(peeking_points == i) && !reject_pocock) {\n      idx <- which(peeking_points == i)\n      if (gst_test(counts, idx, total_checks, alpha)) {\n        reject_pocock <- TRUE\n      }\n    }\n    \n    if (any(peeking_points == i) && !reject_naive) {\n      if (naive_test(counts)) {\n        reject_naive <- TRUE\n      }\n    }\n    \n    if (reject_dirichlet && reject_pocock && reject_naive) break\n  }\n  \n  list(\n    dirichlet = reject_dirichlet,\n    pocock = reject_pocock,\n    naive = reject_naive\n  )\n}\n\nnum_peeks_vec <- seq(2, 10, 2) \nn_sims <- 500                   \nn_users <- 2000                \nalpha <- 0.05                \n\nresults <- data.frame(num_peeks = integer(), method = character(), fp_rate = numeric())\n\nfor (np in num_peeks_vec) {\n  \n  rejections <- replicate(n_sims, {\n    res <- simulate_one_run(num_peeks = np, alpha = alpha)\n    c(dirichlet = res$dirichlet, pocock = res$pocock, naive = res$naive)\n  })\n  \n  rejections <- as.data.frame(t(rejections))\n  fp_rates <- colMeans(rejections)\n  \n  results <- bind_rows(\n    results,\n    data.frame(num_peeks = np, method = \"Dirichlet\", fp_rate = fp_rates[\"dirichlet\"]),\n    data.frame(num_peeks = np, method = \"GST\", fp_rate = fp_rates[\"pocock\"]),\n    data.frame(num_peeks = np, method = \"Naive Peek\", fp_rate = fp_rates[\"naive\"])\n  )\n}\n\nggplot(results, aes(x = factor(num_peeks), y = fp_rate,\n                color = method, group = method)) +\n  geom_line(size = 0.8) +\n  geom_point(size = 1.2) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Comparison of False Positive Rates\",\n    subtitle = \"Simulation: No Effect, Count Data\",\n    x = \"Number of Peeks\",\n    y = \"False Positive Rate\",\n    color = \"Method\"\n  ) +\n  theme_minimal()\n\nggsave(\"false_positive_comparison.png\", width = 7, height = 6)\n```\n:::\n\n\n# Summary\n\nAs we have seen, sequential testing strikes a balance between efficiency and accuracy, enabling faster effect detection while controlling false positives. Among these methods, distribution-based testing offers the most flexibility, without the need for assumptions about distribution shape, predefined interim checks, or alpha-spending functions.\n\n\n**References:**\n\nNetflix Technology Blog. 2019. Improving Experimentation Efficiency at Netflix with Meta Analysis and Optimal Stopping \\[[link](https://netflixtechblog.com/improving-experimentation-efficiency-at-netflix-with-meta-analysis-and-optimal-stopping-d8ec290ae5be)\\]\n\nAaditya Ramdas. 2019. \"Foundations of Large-Scale \"Doubly-Sequential\" Experimentation\" \\[[link](https://www.stat.cmu.edu/~aramdas/kdd19/ramdas-kdd-tut.pdf)\\]\n\nNetflix Technology Blog. 2024. Sequential A/B Testing Keeps the World Streaming Netflix Part 1: Continuous Data \\[[link](https://netflixtechblog.com/sequential-a-b-testing-keeps-the-world-streaming-netflix-part-1-continuous-data-cba6c7ed49df)\\]\n\nNetflix Technology Blog. 2024. Sequential A/B Testing Keeps the World Streaming Netflix Part 2: Counting Processes \\[[link](https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642)\\]\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}