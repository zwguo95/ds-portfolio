{
  "hash": "2c9090c95ea6fa587f184314e0be18d5",
  "result": {
    "markdown": "---\ntitle: \"Scraping Dynamic Websites with R: An Example from Media Bias Data\"\ndate: 04/05/2024\nauthor:\n  - name: Zhaowen Guo\n    url: \"https://www.linkedin.com/in/zhaowen-guo-20535312a/\"\ntitle-block-banner: true\nformat:\n  html:\n    theme: flatly\n    code-fold: false\n    code-tools: false\n    toc: false\n    number-sections: false\nlink-citations: true\ncategories: [code, natural language processing]\nimage: \"web-scraping.png\"\n---\n\n\n\n\n## Introduction\n\nIn my recent analysis of how news headlines on artificial intelligence (AI) vary across media outlets with different ideological leanings, I stumbled upon an interesting [data source](https://www.allsides.com/media-bias) provided by the AllSides organization, which contains over 1,000 human-curated ratings of media outlets' ideological leanings from left to right.\n\nSeveral studies (e.g. [Rozado et al. 2022](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9578611/), [Yi et al. 2023](https://www.researchgate.net/publication/373958720_Partisan_Media_Sentiment_Toward_Artificial_Intelligence)) have relied on its annual Media Bias Chart, as shown below, to capture a slice of the full spectrum of media perspectives. It displays around 60 exemplary media outlets for each category, enabling users to easily reference and document this ideological landscape with minimal effort.\n\n[![](media-bias-chart.png){fig-align=\"center\" width=\"333\"}](https://www.allsides.com/media-bias/media-bias-chart)\n\nHowever, a deeper dive into how media covers AI topics requires a more exhaustive list of media ratings beyond this snapshot. This task confronts several challenges. The webpage's dynamic nature, which prompts users to click the `See all 1400+ Media Bias Ratings` button to load additional content, introduces complexity to data collection. The absence of clear markers for the total number of pages, entries, or an endpoint further complicates this task.\n\nIn this blog post, I will share my experience scraping this dynamic webpage using R, with two distinct approaches.\n\n## Pattern Observation and Looping\n\nThe first approach involves observing the URL structure or pagination pattern of a website and looping through these patterns to scrape data. It's particularly effective for websites with a predictable and consistent structure, such as incrementing IDs or query parameters in URLs that lead to different pages of content.\n\nMy workflow includes the following steps:\n\n-   Inspect the website: We can right-click and select `Inspect` on a webpage, which allows us to access the webpage's structure and its network activities.\n\n-   Interact with the website and observe changes: By engaging with the website, such as clicking a button or scrolling down to load more content, we can observe how the website dynamically fetches additional data.\n\n-   Monitor network activity: Under the [Network]{.underline} and the [Fetch/XHR]{.underline} tabs, we can monitor asynchronous requests made by the webpage after the initial page load, which is particularly crucial for scraping dynamic websites where content is not available in the initial HTML.\n\n-   Identify patterns: We can then examine the Name column (or the request URLs) for patterns, especially those indicating page changes or content loading mechanisms.\n\nThe screenshot below shows the network activities I observed after inspecting the webpage. By navigating to the [Network]{.underline} and [Fetch/XHR]{.underline} tabs, I monitored the network requests that occurred when interacting with the website. My interactions involved scrolling down to the bottom of the page and clicking a button to load more content. During this process, I identified recurring URL patterns that indicate page changes (e.g. page=1,2,3...), highlighted in the red box. These patterns are key to extracting content by programmatically looping through the pages.\n\n![](network-log.png){fig-align=\"center\" width=\"431\"}\n\nI recorded these URL patterns below with the page number being the parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_url <- \"https://www.allsides.com/media-bias/ratings?page=\"\nparams <- \"&field_featured_bias_rating_value=All&field_news_source_type_tid%5B0%5D=2&field_news_bias_nid_1%5B1%5D=1&field_news_bias_nid_1%5B2%5D=2&field_news_bias_nid_1%5B3%5D=3&title=\"\npage_num <- 0\nhas_content <- TRUE\n```\n:::\n\n\nKnowing how to automatically load more pages, we can then switch to extracting the specific content of interest. In this case, I am interested in media outlets and their corresponding ideological leanings. To do this, I hover over the desired content, right-click, and choose `Inspect` to locate it under the [Elements]{.underline} tab.\n\nThe `rvest` package in R provides several useful functions to extract information after parsing HTML content. `html_elements()` is used to select elements based on their attributes, classes, IDs, and so on. `html_attr()` can extract the value of a specific attribute from an HTML element, which is useful for getting data held in attributes like \"href\" (links), \"src\" (images), or others.\n\nFor instance, this is what I observed upon inspecting the element related to media leanings.\n\n![](source_leaning_example.png){fig-align=\"center\"}\n\nI identified its parent class `.views-field-field-bias-image a img` and its attribute `alt`. The following code snippet demonstrates how to extract names and leanings of media outlets based on these identified elements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(tidyverse)\n\n# Load the webpage content\npage <- read_html(current_url)\n\n# Extract names of media outlets\nnews_source <- page %>%\n    html_elements(\".view-content .views-field-title a\") %>%\n    html_text()\n\n# Extract leanings of media outlets  \nmedia_rating <- page %>%\n    html_elements(\".views-field-field-bias-image a img\") %>%\n    html_attr(\"alt\")\n```\n:::\n\n\nOnce this is done, the final step is just to construct a stopping rule for this scraper when no more content is available. This can be done using a while-loop plus an if-else check. Here's the pseudo-code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhile(has_content){\n  page <- ...\n  \n  news_source <- ...\n  \n  media_rating <- ...\n  \n  # Check if the page has content\n  if (length(news_source) == 0) {\n    has_content <- FALSE\n    print(\"No more content.\")\n  } else {\n    news_sources[[page_num + 1]] <- news_source\n    media_ratings[[page_num + 1]] <- media_rating\n    \n    print(paste(\"Page\", page_num, \"scraped successfully.\"))\n    page_num <- page_num + 1\n  }\n}\n```\n:::\n\n\nThe whole process is extremely fast (I got 1609 entries in \\~20s!), and it's also straightforward to implement this approach once we identify page loading patterns and locations of relevant HTML elements. The complete code can be found [here](scrape_media_bias_loop.R).\n\n## Automation Tools Like RSelenium\n\nAn alternative approach is to use automation tools like `RSelenium`, which facilitates the automation of web browsers to mimic human interactions on websites, such as logging in, clicking buttons, and so on. This is my first time playing with this tool, and I found it more flexible compared to the former approach, especially when page loading patterns are not evident, and it typically does not require in-depth HTML inspection. However, a notable downside is the complexity of its setup, and it also tends to be slower and more resource-intensive as it involves launching and controlling a web browser session.\n\nThe process includes the following steps:\n\n-   Navigate to the webpage: We need to launch a web browser and direct it to the desired webpage.\n\n-   Interact with the webpage to load more content: We can program the browser to mimic user actions, such as scrolling through pages and clicking buttons, to ensure all relevant content is loaded.\n\n-   Extract the desired elements: Upon fully loading the pages, we can retrieve elements of interest from the webpage\n\nWhen setting up RSelenium, I found it helpful to (1) place the web browser driver (I used chromedriver.exe) in the same folder as the script, which makes it easier for R to locate and initiate the web browser; and (2) set `chromever = NULL`, which enables automatically detecting the appropriate version of the web driver installed.\n\n![](load-more-button.png){width=\"662\"}\n\nThe following code initiates a Chrome web browser session, navigate to the webpage of interest, and click a button to load more content. `remDr` (remote driver object) is used to interact with the web browser - to identify the button from the CSS selector and simulate a click action. As before, we can inspect the button and find its class, as depicted in the screenshot above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RSelenium)\n\n# Start a web browser\nrD <- rsDriver(browser = \"chrome\", port = 4544L, chromever = NULL)\nremDr <- rD[[\"client\"]]\n\n# Navigate to the webpage\nremDr$navigate(\"https://www.allsides.com/media-bias/ratings\")\n\n# Function to attempt clicking a \"Load More\" button\nattemptLoadMore <- function() {\n  tryCatch({\n    button <- remDr$findElement(using = 'css selector', value = '.changeFilter.btn.btn-large.btn-success, .load-more-button-selector') # Combine selectors if possible\n    button$clickElement()\n    Sys.sleep(2) # Wait for content to load\n    TRUE\n  }, error = function(e) { FALSE })\n}\n\n# Initial click to load more content\nattemptLoadMore()\n```\n:::\n\n\nThe next interaction we need to mimic is to scroll down the webpage and click the button until no new content is loaded. How can we determine when to stop? One way to do this is to record the current scrollable height of the webpage body and continue the clicking behavior until the height does not change, as presented below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scroll and attempt to load more until no new content loads\nrepeat {\n  last_height <- remDr$executeScript(\"return document.body.scrollHeight;\") \n  remDr$executeScript(\"window.scrollTo(0, document.body.scrollHeight);\") # Scroll to the bottom of the page\n  Sys.sleep(3)\n  \n  # Check for new scroll height and attempt to load more if scrolled to bottom\n  new_height <- remDr$executeScript(\"return document.body.scrollHeight;\")\n  if (last_height == new_height && !attemptLoadMore()) { # && prioritizes the first condition\n    break\n  }\n}\n```\n:::\n\n\nOnce all entries are loaded, we can use `rvest` as before to retrieve the elements of interest. The full script can be accessed [here](scrape_media_bias_selenium.R).\n\nTo summarize, the two web scraping approaches primarily differ in their sequence of actions and the logic behind page loading.\n\n-   The first method sequentially loads and extracts data page by page, leveraging identifiable patterns in page requests for navigation.\n\n-   The second method loads all relevant pages first before proceeding with data extraction, and simulates user interactions to trigger page loads.\n\nThe first approach can be faster but requires more in-depth observation of page loading patterns, and the second provides a more flexible solution for interacting with web pages, especially when direct patterns are not apparent. I hope you find this post informative and helpful!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}