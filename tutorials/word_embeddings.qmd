---
title: "Word Embeddings"
author: 
- name: Zhaowen Guo
  email: zwguo@uw.edu
  affiliations: University of Washington
format:
  revealjs:
    incremental: true
    smaller: true
    scrollable: true
    self-contained: true
    slide-number: false
    history: true
    #footer: "Word Embeddings"
    reference-location: "document"
    link-external-icon: false
    link-external-newwindow: true
    footnotes-hover: true
    auto-animation: true
    highlight-style: arrow
execute: 
  echo: true
  cache: true
  message: false
  warning: false
---

## Why using word embeddings?


- The corpus comes from [Congressional Record](https://www.congress.gov/congressional-record) which records all remarks made by senators and representatives on the floor of the US Congress
- Use congressional record for the 114th Congress as an example
  - Two variables: speech, speech_id
  - 67,557 observations



## Represent the text using BOW {auto-animate="true"}


``` r
library(tidytext)
library(SnowballC)
library(tidyverse)

data <- readRDS("data114.RDS") 
```


## Represent the text using BOW {auto-animate="true"}


``` r
library(tidytext)
library(SnowballC)
library(tidyverse)

data <- readRDS("data114.RDS") %>%
  unnest_tokens(word, speech) %>%
  anti_join(get_stopwords()) %>%
  mutate(stem = wordStem(word))
```


## Represent the text using BOW {auto-animate="true"}


``` r
library(tidytext)
library(SnowballC)
library(tidyverse)

data <- readRDS("data114.RDS") %>%
  unnest_tokens(word, speech) %>%
  anti_join(get_stopwords()) %>%
  mutate(stem = wordStem(word)) %>%
  count(speech_id, stem) %>%
  bind_tf_idf(stem, speech_id, n) %>%
  cast_dfm(speech_id, stem, tf_idf)
```


## What do we find?

![](images/sparse-matrix.JPG)

BOW representation is

- extremely sparse (99.92% sparse)

- high-dimensional (98,235 features)



## Represent the text using word embeddings

![](images/word-embedding-decision-tree.JPG)

\newline Source: Rodriguez & Spirling, 2022

## Represent the text using word embeddings

Why do we use them?

- Understand language use
  - across time
  - across groups
  - stereotype and bias

- Feed into downstream NLP implementations



## How to find word embeddings?

- [Singular Vector Decomposition (SVD)](https://twitter.com/WomenInStat/status/1285610321747611653)

- Neural network-based techniques 
  - [Word2Vec](https://arxiv.org/abs/1301.3781)
  - [GloVe](https://nlp.stanford.edu/projects/glove/)
  - language models with transformers such as [BERT](https://arxiv.org/abs/1810.04805)


## Implementation of SVD {.smaller .scrollable transition="slide"}

::: panel-tabset
#### skipgram probabilities

```{.r code-line-numbers="1-7|9-22|1-22"} 
nested_data <- readRDS("data114.RDS") %>%
  unnest_tokens(word, speech) %>%
  anti_join(get_stopwords()) %>%
  group_by(word) %>%
  filter(n() >= 50) %>%
  ungroup() %>%
  nest(words = c(word))
  
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl, ~.x, .after = window_size - 1, .step = 1, .complete = T
  )
  safe_mutate <- safely(mutate)
  
  out <- map2(skipgrams, 1:length(skipgrams), ~safe_mutate(.x, window_id = .y))
  
  out %>%
    transpose() %>%
    pluck("result") %>%
    compact() %>%
    bind_rows()
}
```


#### pointwise mutual information (PMI)

```{.r code-line-numbers="1-3|5-9|1-19"} 
library(widyr) 
library(furrr)
plan(multisession)

pmi <- nested_data %>%
  mutate(words = future_map(words, slide_windows, 4)) %>%
  unnest(words) %>%
  unite(window_id, speech_id, window_id) %>%
  pairwise_pmi(word, window_id)
```
:::


## Explore word vectors

The most similar words to "tax": 

. . .

::: columns
::: {.column width="60%"}
```  {.r code-line-numbers="1-2|4-10|12-13|1-13"} 
word_vectors <- pmi %>%
  widely_svd(item1, item2, pmi, nv = 100, maxit = 1000)
  
nearest_neighbors <- function(df, token){
  df %>% 
    widely(~. %*% (.[token, ]),
           sort = T,
           maximum_size = NULL)(item1, dimension, value) %>%
    select(-item2)
}

word_vectors %>%
  nearest_neighbors("tax")
```
::: 
::: {.column width="40%"}

![](images/similar-words-tax.JPG)

:::
:::

## Explore word vectors

The first 6 principal components with top 10 contributing words:

. . .

::: columns
::: {.column width="60%"}
```  {.r code-line-numbers="1-2|4-10|12-13|15-27|1-27"} 
word_vectors %>%
  filter(dimension <= 6) %>%
  group_by(dimension) %>%
  top_n(10, abs(value)) %>%
  ungroup() %>%
  mutate(dimension = as.factor(dimension),
         item1 = reorder_within(item1, value, dimension)) %>%
  ggplot(aes(item1, value, fill = dimension)) +
  geom_col(show.legend = F) +
  facet_wrap(~ dimension, scales = "free_y", ncol = 3) +
  coord_flip() +
  scale_x_reordered() + 
  theme_bw()
```
:::

::: {.column width="40%"}

![](images/dimension-words.png)
:::
:::

## Explore word vectors 

Do politicians discuss "tax" in different ways? 

. . .

::: columns
::: {.column width="50%"}
Democrats

\newline ![](images/similar-words-tax-dem.JPG)
:::

::: {.column width="50%"}
Republicans

\newline ![](images/similar-words-tax-rep.JPG)
:::
:::

## Implementation of GloVe {.smaller .scrollable transition="slide"}


::: panel-tabset
#### skipgram probabilities



## What else should be aware of?

- Embedding alignment 

- Boostrapping 


## References
[Chris Moody's tutorial on building word embeddings from scratch](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/)

[Dmitriy Selivanov's tutorial on GloVe Word Embeddings](http://text2vec.org/glove.html)

[Julia Silge's blog post on word vectors with tidy data principles](https://juliasilge.com/blog/tidy-word-vectors/)

[Chris Bail's tutorial on word embedings](https://cbail.github.io/textasdata/word2vec/rmarkdown/word2vec.html)

[Hvitfeldt and Silge, 2022. Supervised Machine Learning for Text Analysis in R](https://smltar.com/)
