[
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "The Power of Singular Vector Decomposition: A Beginner’s Guide\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2024\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nRevisiting the Monty Hall Problem: The Power of Simulations\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nExplaining the Bias-Variance Tradeoff\n\n\n\n\n\n\n\nstatistics\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Household Groupings: A Longitudinal Approach\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nUnveilling Model Equivalence: Linking the Rasch Model with the Hierarchical Linear Model\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nZhaowen Guo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/simulations/index.html",
    "href": "statistics/simulations/index.html",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "",
    "text": "Named after the host of the popular game show “Let’s Make a Deal,” the Monty Hall problem is a classic example of how our intuition can sometimes lead us astray when it comes to probabilities.\nThe problem goes like this: You’re a contestant on “Let’s Make a Deal” and are asked to choose one of three doors. Behind one of the doors is a brand new car, while behind the other two are goats. After you make your choice, Monty Hall opens one of the other two doors to reveal a goat. He then gives you the option to (1) stick with your original choice or (2) switch to the other unopened door.\nWhat should you do? Should you stick with your original choice or switch to the other door?\nMany people’s intuition tells them that it doesn’t matter whether they stick with their original choice or switch and the probability of winning the car is 1/3 no matter what.\nIs this correct? In this blog post, I will introduce various methods to solve this puzzle, including probability theory, causal diagrams, and simulations. All code can be accessed here."
  },
  {
    "objectID": "statistics/simulations/index.html#probability-theory",
    "href": "statistics/simulations/index.html#probability-theory",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Probability Theory",
    "text": "Probability Theory\nOne straightforward approach is to apply the Bayes’ theorem to calculate and compare the probability of winning between switching and not switching. To simplify the analysis, we can define two events: event A is when the car is behind the remaining door, and event B is when Monty chooses a door with a goat. Our objective is to determine the probability that the car is behind the remaining door given Monty’s exposure door. If the probability is high, then switching would be a better choice, and vice versa.\nAssuming our initial choice is Door 1 and Monty opens Door 2, we can use Bayes’ theorem to calculate the conditional probability of the car being behind Door 3 given that Monty opened Door 2. The calculation is as follows:\n\\[\nP(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n\\]\nNow, let’s figure out what each part represents. \\(P(B|A)\\) is 1 because Monty should never reveal a car by design and our choice of Door 1 rules out another option. We also have \\(P(A) = 1/3\\), since a car is equally likely to be assigned to one of the three doors. The marginal probability \\(P(B)\\) that Monty opens Door 2 without conditioning on what the remaining door contains is 1/2, as our initial choice leaves him two options to pick.\nTaken together, we have the probability of winning for switching is \\(P(A|B) = \\frac {1 \\times 1/3}{1/2} = 2/3\\) and the probability of wining for sticking to the initial choice is \\(1 - P(A|B) = 1/3\\). So yes, the correct answer is we should always switch!"
  },
  {
    "objectID": "statistics/simulations/index.html#causal-diagrams",
    "href": "statistics/simulations/index.html#causal-diagrams",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Causal Diagrams",
    "text": "Causal Diagrams\nJudea Pearl’s The Book of Way sheds a new light on this classic probability puzzle. The key is go beyond the data itself and think through how the data was generated. Let’s start with the question: what determines which door will be opened by Hall?\nOur choice of door excludes one option he could open. Knowing that which door that does have a goat behind it means that he has to choose a different one as well if possible. In this case, we end up with the following causal diagram where door to be opened is a collider.\nWhat will happen if we condition on a collider? In other words, what if we take action based on Monty’s choice of door? In causal graph terms, conditioning on a collider creates an association between previously independent variables (causal diagrams shifting from the left to the right as shown below), in this case, our choice of door and the location of the car. Let’s still assume that our initial choice is Door 1 and Monty opens Door 2. Translating this causal diagram into probabilities, the probability of switching (the car being behind Door 3) conditional on Monty’s choice becomes 2/3 because now the car should be behind one of the remaining two doors, which is greater than 1/3.\nMore intuitively, the reason why the probability of a car behind Door 1 changes from 1/3 to 2/3 when we condition on Monty’s choice of door is that his choice is not random: he has to pick a door without a car. Monty could not open Door 1 once we chose it – but he could have opened Door 3. The fact that he did not implies that he was forced to and thus there is more evidence than before that the car is behind Door 3."
  },
  {
    "objectID": "statistics/simulations/index.html#simulations",
    "href": "statistics/simulations/index.html#simulations",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Simulations",
    "text": "Simulations\nAs you may have noticed, solving the Monty Hall problem requires some effort to overcome our intuition. However, there is a more intuitive way to approach this problem: simulations. Using simulations, we can better understand the probabilities at play and verify the counter-intuitive result of the problem.\nThe basic idea is still to compute and compare the winning probability of switching and not switching, but now let’s play this game 10000 times and compare the relative frequency of winning.\nUsing exactly the same setup where our initial choice is Door 1, the probability of not switching after 10000 iterations of the game is 0.33.\n```{r}\n# The winning probability of sticking to the initial choice\nset.seed(123)\ndoors &lt;- c(1,2,3)\n\ncount &lt;- 0\nfor(i in 1:10000) {\n  car &lt;- sample(doors, 1)\n  initial_choice &lt;- 1\n  if(initial_choice == car){\n    count &lt;- count + 1\n  }\n}\n\np_stick &lt;- count/10000\n```\nA slightly tricky part is specifying which door Monty will reveal. Following the same logic, Monty will choose a door different from both our initial choice and the door with the car, we can write a simple function as below and compute the relative frequency of winning when we switch. The probability is 0.67, twice as larger as 0.33.\n```{r}\n# The winning probability of switching to another choice \nset.seed(123)\nreveal &lt;- function(doors, car, initial_choice) {\n  if(car == initial_choice){\n    reveal &lt;- sample(doors[-c(car,initial_choice)], 1)\n  } else {\n    reveal &lt;- doors[-c(car, initial_choice)]\n  }\n}\n\ncount &lt;- 0\nfor (i in 1:10000) {\n  car &lt;- sample(doors,1)\n  initial_choice &lt;- 1\n  revealed_door &lt;- reveal(doors, car, initial_choice)\n  final_choice &lt;- doors[-c(initial_choice, revealed_door)]\n  if(final_choice == car){\n    count = count + 1\n  }\n}\n\np_switch &lt;- count/10000\n```\nI hope this post has highlighted the usefulness of simulations as a tool to gain a more intuitive understanding of probability concepts. By running simulations of the Monty Hall problem multiple times, we can observe how the probability of winning changes when we switch doors, which can be helpful in building a deeper understanding of the problem. Additionally, simulations can also serve as a good way to verify counter-intuitive results and build confidence in other probability-based solutions."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html",
    "href": "statistics/dssg-tracking-poverty/index.html",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "",
    "text": "Poverty is a complex issue that is typically measured at the household level, taking into account the resources that are shared among multiple individuals. However, many existing studies rely solely on individual-level survey data to estimate poverty rates. This approach can lead to inaccurate estimates, particularly in cases where individuals within a household have differing levels of access to resources.\nThe use of administrative data is a promising approach to measuring poverty, as it provides a wealth of information on individual addresses. By aggregating information across individuals who are more likely to belong to the same household, we can develop more accurate estimates of poverty and better understand the ways in which poverty affects families and communities.\nIn our 2022 Data Science for Social Good (DSSG) project, we used the Washington Merged Longitudinal Administrative Dataset (WMLAD) to predict household groupings in the Washington State. This database merges administrative data from various state agencies including the Employment Security Department (ESD), Department of Social and Health Services (DSHS), Department of Health (DOH), Secretary of State, Department of Licensing (DOL), and WA State Patrol, covering the period from 2010 to 2017. Each time an individual interacts with any of these agencies, they are assigned an anonymized address code, which is further augmented by an imputation algorithm developed by previous WMLAD users (more details here).\nIn this blog post, I will introduce a longitudinal approach that I proposed for our team to improve the imputed addresses and construct more reliable household groupings. Don’t forget to also check out our presentation and media coverage :)"
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-1-lack-of-interactions",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-1-lack-of-interactions",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 1: Lack of interactions",
    "text": "Scenario 1: Lack of interactions\nThis occurs when certain household members, especially children, have less interactions with government agencies than adults, resulting in fewer recorded addresses for imputation. For example, addresses for children may only be recorded when they register to vote or obtain a driver’s license, which implies that the observation of some one-person residences may simply be due to the absence of their children in previous records.\nTo address this circumstance, I recommended adjusting certain one-person residences to reflect their actual co-residence with children. For example, in the scenario illustrated below, a child (purple face) began to co-reside with others (blue face) in month 4, while the address records showing only one person living at that address previously. In such cases, we need to reflect their co-residence accurately."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-2-imputation-error",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-2-imputation-error",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 2: Imputation error",
    "text": "Scenario 2: Imputation error\nAnother pattern that drew my attention was frequent movements into and out of the same place, which may indicate an imputation error.\nA plausible fix was in cases where we observe a person living alone and co-residing with others alternately at the same address, we could adjust the one-person residences to reflect the nearest co-residence that the individual belongs to."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-3-move-in-and-out",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-3-move-in-and-out",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 3: Move in and out",
    "text": "Scenario 3: Move in and out\nThe third scenario is more complex. One-person residences may occur during a transition period when a multi-person residence moves out simultaneously to a new location, but their addresses are not updated accordingly.\nIn cases where we observe distinct co-residence members before and after a one-person residence at a particular address, and the prior co-residence appears again in the future, we adjust the one-person residence to reflect the subsequent co-residence. As depicted in the illustration below, we should only modify one-person residences in month 3 when the co-residence composition in month 2 and month 4 matches.\n\n\nModifying one-person residences that fall under these scenarios did not conclude the analysis, as it could result in duplicates where a person appears at multiple addresses during the same time period. Therefore, we should eliminate duplicates based on recorded addresses, rather than imputed ones, which provide a more reliable indication of the address that the person belongs to.\nHow effective is this algorithm? After implementing the modifications, the percentage of one-person residences decreased from 38% to 31%, bringing it much closer to the census record of 27%. We are currently developing a methodology paper that details this approach along with more real-world data applications. We anticipate that our methodology and resulting household groupings will be beneficial in addressing significant policy questions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhaowen Guo",
    "section": "",
    "text": "I am a PhD candidate at the University of Washington, where I combine my expertise in both social and data science to develop and enhance metrics using innovative data collection and psychometric techniques.\nI enjoy connecting data from various sources, including administrative data, survey data, text data, image data, and spatial data, to deliver insightful analyses and support impact evaluations. My passion lies in communicating my findings through engaging data visualizations and promoting equity and inclusion in data science by knowledge sharing."
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html",
    "href": "dataviz/tree-gunshot/index.html",
    "title": "The Calendar of Gun Violence",
    "section": "",
    "text": "Despite being a small city, Washington, D.C. has the highest homicide rate among all U.S. states, with 226 deaths and 1330 emergency department visits due to gunshot wounds in 2021. As part of its efforts to combat gun violence, the city has adopted innovative strategies and technologies, including ShotSpotter which is a gunshot detection system that uses acoustic sensors to identify and locate gunfire in real-time.\nIn this blog post, we will examine 2021 gunshot data in D.C. and explore how data visualization can provide deeper insights into gun violence in the city. All code can be found here."
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html#data-cleaning",
    "href": "dataviz/tree-gunshot/index.html#data-cleaning",
    "title": "The Calendar of Gun Violence",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nSuppose that we are interested in the temporal patterns of gun violence in D.C., one common way to visualize time series data is through line charts. These charts can break down time points into intervals, such as months, to observe how the values of gun violence incidents change over time. While this approach offers valuable insight into high-level temporal trends, it has one limitation: it only allows us to focus on one time interval at a time. For example, we can only see the temporal changes over months OR weeks, but not both intervals together.\nInspired by GitHub’s contribution graph, we can use geom_tile() to create a similar calendar graph that effectively visualizes gun violence incidents. To situate a calendar within a data frame, we can observe clear parallels: the week of the month corresponds to the row number, the day of the week represents the column number, and each month is treated as a separate facet.\nOne challenge here is to figure out the week for each month. Unfortunately, week() in the lubridate package only returns the week for the year, requiring us to create a week incrementer to manually calculate the week for each month. In other words, we increment the week counter by 1 when we encounter a “Sunday” or when the day is the first of the month.\nMoving forward, we can consider whether to treat gunshot incidents as a continuous variable. Upon plotting the data distribution, it becomes apparent that the distribution is highly right-skewed, which means treating it as continuous would not allow for clear differentiation of color in the legend. Therefore, I categorize gunshot incidents to represent low, medium, and high levels of gun violence.\n```{r}\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(tidyverse)\ngunshot &lt;- read_csv(\"Shot_Spotter_Gun_Shots.csv\")\n\n# data cleaning\ngunshot_daily &lt;- gunshot %&gt;%\n  mutate(date = as_date(DATETIME),\n         year = year(date)) %&gt;%\n  filter((year == 2021) & (TYPE %in% c(\"Single_Gunshot\", \"Multiple_Gunshots\", \"Multiple Gunshots\", \"Single Gunshot\"))) %&gt;%\n  group_by(date) %&gt;%\n  summarise(shots = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(week_day = str_sub(weekdays(date), 1, 3),\n         month_day = day(date),\n         month = month(date),\n         week_start = ifelse(month_day == 1 | week_day == \"Sun\", 1, 0)) %&gt;% # set up when to increment the week\n  group_by(month) %&gt;%\n  mutate(week = cumsum(week_start),\n         month_name = months(date)) %&gt;%\n  ungroup() %&gt;%\n  mutate(shots_range = case_when(shots &lt;= 10 ~ \"1\",\n                                 shots &gt; 10 & shots &lt;= 20 ~ \"2\",\n                                 shots &gt; 20 ~ \"3\"))\n\nweek_day_code &lt;- c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\ngunshot_daily$week_day &lt;- factor(gunshot_daily$week_day, levels = week_day_code)\nmonth_code &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\") \ngunshot_daily$month_name &lt;- factor(gunshot_daily$month_name, levels = month_code)\n```"
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html#data-visualization",
    "href": "dataviz/tree-gunshot/index.html#data-visualization",
    "title": "The Calendar of Gun Violence",
    "section": "Data Visualization",
    "text": "Data Visualization\nNow, let’s visualize the data! Interestingly, the part I spent most time upon was making the month names appear above the weekday names, as they were initially positioned the opposite way. To resolve this issue, we have the option to adjust either the strips (weekday names) or the axes (month names). I decided to go for the former route by increasing the bottom parameter with margin(b=25), which enabled the weekday names to move upwards until they were placed above the month names.\n```{r}\n# theme\nfont_add_google(\"Pragati Narrow\")\nshowtext_auto()\n\n\n# customize theme\ntheme_set(theme_minimal(base_family = \"Pragati Narrow\"))\n\ntheme_update(\n  # legend\n  legend.title = element_blank(),\n  legend.position = 'bottom',\n  legend.direction = 'horizontal',\n  legend.key.width = unit(1.5, \"cm\"),\n  legend.text = element_text(color = \"black\",  size=35),\n  legend.box.margin = margin(t = 35),\n  legend.spacing.x = unit(1, \"cm\"),\n  legend.spacing.y = unit(0.5, \"cm\"),\n  \n  # axis\n  axis.text.y = element_blank(),\n  axis.text.x = element_text(vjust = 50),\n  text = element_text(size = 40),\n  strip.text.x = element_text(size = 43, margin = margin(b = 25)),\n  \n  # titles\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  plot.margin = margin(20, 50, 20, 50),\n  plot.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  panel.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  \n  #legend.title.align=1,\n  plot.title = element_text(\n    color = \"black\", \n    size = 70, \n    face = \"bold\",\n    margin = margin(t = 10),\n    hjust = 0.5\n  ),\n  plot.subtitle = element_text(\n    color = \"grey10\", \n    size = 45,\n    lineheight = 3,\n    margin = margin(t = 5, b = 30),\n    hjust = 0.5\n  ),\n  plot.title.position = \"plot\",\n  plot.caption.position = \"plot\",\n  plot.caption = element_text(\n    color = \"grey20\", \n    size = 35,\n    lineheight = 0.5, \n    hjust = 0.5,\n    margin = margin(t = 30))\n)\n\n\ngunshot_daily %&gt;%\n  ggplot(aes(x = week_day, y = week)) +\n  geom_tile(aes(fill = shots_range), color = \"white\") + \n  scale_fill_manual(values = MetBrewer::met.brewer(\"Tam\", n=3),\n                    labels = c(\"below 10\", \"10 to 20\", \"over 20\"),\n                    guide = guide_legend(label.position = \"bottom\", nrow = 1)) +\n  facet_wrap(~month_name, scales = \"free\") +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"bottom\") +\n  labs(x = \"\", y = \"\", title = \"GUNSHOT DETECTION CALENDAR\",\n       subtitle = \"Recorded shooting incidents in Washington D.C. during 2021\",\n       caption = str_wrap(\"Data comes from ShotSpotter gunshot detection system. Incidents of probable gunfires and firecrackers are excluded | Visualization by Zhaowen Guo\", width = 300))\n\nggsave(\"dc-gunshot-time.png\", width = 14, height = 14/1.618, units = \"in\")\n```"
  },
  {
    "objectID": "dataviz/tidytuesday-stock-market/index.html",
    "href": "dataviz/tidytuesday-stock-market/index.html",
    "title": "TidyTuesday: Tech Company Stock Prices",
    "section": "",
    "text": "TidyTuesday is a weekly data project aimed at improving skills in data manipulation, visualization and analysis using R programming language. This week, we are exploring daily stock prices and volume of 14 different tech companies. You are welcome to review my code here!"
  },
  {
    "objectID": "dataviz/tidytuesday-halloween/index.html",
    "href": "dataviz/tidytuesday-halloween/index.html",
    "title": "Getting Spooky with R",
    "section": "",
    "text": "Data source comes from The Shadowlands Haunted Place Index.\nTidyTuesday is a weekly data project aimed at improving skills in data manipulation, visualization and analysis using R programming language. This week, we are exploring daily stock prices and volume of 14 different tech companies. You are welcome to review my code here!"
  },
  {
    "objectID": "dataviz/tidytuesday-bob-ross/index.html",
    "href": "dataviz/tidytuesday-bob-ross/index.html",
    "title": "TidyTuesday: Bob Ross Paintings",
    "section": "",
    "text": "TidyTuesday is a weekly data project aimed at improving skills in data manipulation, visualization and analysis using R programming language. This week, we are exploring the paintings of Bob Ross featured in the TV show “The Joy of Painting”. Please find my code here!"
  },
  {
    "objectID": "dataviz/racial-disparity/index.html",
    "href": "dataviz/racial-disparity/index.html",
    "title": "Visualizing Racial Justice: Moving Beyond Bar Charts",
    "section": "",
    "text": "Gun violence disproportionately and overwhelmingly affects communities of color, resulting in significant racial disparities in exposure to gun violence. To tell stories of racial justice in gun violence, it’s crucial to use compelling visuals. While many reports have relied on bar charts and stacked bar charts to illustrate unequal gun exposure, using them exclusively may not always be visually engaging. In this blog post, we will explore some alternative ways to visualize racial justice in gun violence. My full code can be accessed here.\nOne effective way to display racial disparities in gun violence is using a parliament chart, which is suitable for categorical data in a two-dimensional grid format. Let’s use recorded gunshot incidents in Washington DC in 2021 as an example. After cleaning the data, it is evident that out of the 555 communities in the city, 293 experienced gun violence. Shockingly, over 70% of these communities were African-American, highlighting a clear instance of racial disparity.\nTo convey this message using a parliament chart, we can use the ggparliament package which just takes two steps to get our work done. Firstly, we need to create a data frame with the categorical data we want to visualize, which in this case is the number of communities per racial group that have been exposed to gun violence. Next, we can convert this data frame to an appropriate structure for creating a parliament chart using the parliament_data() function. In the second step, we can specify the shape of the parliament chart (circle, semicircle, square, or rectangle), the number of rows to display, and the counts for each category.\n```{r}\ncommunity_gunshot &lt;- data.frame(groups = c(\"White Community\", \"African-American Community\"),\n                                count = c(84, 209),\n                                colors = c(\"#d39a2d\",\"#591c19\"))\n\ncommunity_gunshot_data &lt;- parliament_data(election_data = community_gunshot,\n                                          type = \"semicircle\",\n                                          parl_rows = 6,\n                                          party_seats = community_gunshot$count)\n```\nOnce we have completed these two steps, we can then use the data frame with ggplot and add a layer called geom_parliament_seats(). We can also add whatever aesthetics we desire. And voila, our parliament chart is ready to be displayed!\n\nWhen attempting to replicate this graph by comparing Hispanic/Latino communities to others, an issue arose: there are only five Hispanic/Latino communities out of the 555 communities. If we display the raw counts of communities experiencing gun violence, this could give the false impression that non-Hispanic or non-Latino communities had greater exposure to gun violence. However, in reality, four out of the five Hispanic/Latino communities experienced gun violence. As a result, it is crucial to display proportions rather than raw counts when comparing ethnic communities.\nWhat alternative visualization options do we have besides using bar charts? One idea that came to mind was using filled icons, such as handguns, with different heights of filled colors to represent the proportions of communities that experienced gun violence. However, implementing this idea was more time-consuming than anticipated. I would appreciate any suggestions on how to streamline this process.\nMy approach was to combine the echarts4r and the ggplot workflows. echarts4r is a powerful tool for creating interactive visualizations and includes a handy function called e_pictorial(), which allows us to incorporate any images we want to plot, such as a handgun icon in our case. To use an image, we simply need to provide the path to the svg file.\n```{r}\nicon_path = \"path://M544 64h-16V56C528 42.74 517.3 32 504 32S480 42.74 480 56V64H43.17C19.33 64 0 83.33 0 107.2v89.66C0 220.7 19.33 240 43.17 240c21.26 0 36.61 20.35 30.77 40.79l-40.69 158.4C27.41 459.6 42.76 480 64.02 480h103.8c14.29 0 26.84-9.469 30.77-23.21L226.4 352h94.58c24.16 0 45.5-15.41 53.13-38.28L398.6 240h36.1c8.486 0 16.62-3.369 22.63-9.373L480 208h64c17.67 0 32-14.33 32-32V96C576 78.33 561.7 64 544 64zM328.5 298.6C327.4 301.8 324.4 304 320.9 304H239.1L256 240h92.02L328.5 298.6zM480 160H64V128h416V160z\"\n\nhispanic = data.frame(ethnic = c(\"Hispanic\", \"Others\"),\n                      ratio = c(40, 25), # scale down the numbers 80 and 53\n                      path = c(icon_path,\n                               icon_path))\n# create a filled image graph\nhispanic %&gt;% \n  e_charts(ethnic) %&gt;% \n  e_x_axis(splitLine=list(show = FALSE), \n           axisTick=list(show=FALSE),\n           axisLine=list(show=FALSE),\n           axisLabel = list(show=FALSE)) %&gt;%\n  e_y_axis(max=100, \n           splitLine=list(show = FALSE),\n           axisTick=list(show=FALSE),\n           axisLine=list(show=FALSE),\n           axisLabel=list(show=FALSE)) %&gt;%\n  e_color(color = c('#811e18','grey'), background = \"#f5f5f2\") %&gt;%\n  e_pictorial(ratio, symbol = path, z=10, name = \"\",\n              symbolBoundingData= 50, symbolClip= TRUE) %&gt;% \n  e_pictorial(ratio, symbol = path, name= '', \n              symbolBoundingData= 50) %&gt;%\n  e_legend(show = FALSE) %&gt;%\n  e_grid(bottom = \"35%\") \n```\nHowever, one drawback is that echarts4r does not work directly with ggplot themes, so I couldn’t adjust the aesthetics to match my other ggplot charts. To work around this, I created a graph with two filled handgun icons and used it as a background image in ggplot. I then added supporting annotations to enhance the visualization.\nTo annotate the previously downloaded graph, I used the magick and ggpubr packages, which allowed me to easily add text and other annotations in the style of ggplot. The process involved importing the graph as an image using magick, and then using ggpubr to overlay ggplot-style text annotations on top of the image. To ensure that the annotations were positioned correctly, I specified the appropriate x- and y-coordinates for each annotation.\n```{r}\nlibrary(magick)\nlibrary(ggpubr)\nbackground &lt;- image_read(\"ethnic-gun.png\")\nxaxis &lt;- data.frame(xaxis = c(1, 2, 3),\n                    labels = c(\"\", \"\", \"\"))\nyaxis &lt;- data.frame(yaxis = c(1, 2, 3),\n                    labels = c(\"\", \"\", \"\"))\n\nggplot() +\n  background_image(background) +\n  geom_text(data = xaxis, aes(x = xaxis, y = 0, label = labels)) +\n  geom_text(data = yaxis, aes(x = 0, y = yaxis, label = labels)) +\n  labs(x=\"\",y=\"\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.background = element_rect(fill = \"#f5f5f2\", color = NA)) +\n  annotate(geom = \"text\", x = 0.8, y = 0.8, label = \"80% of Hispanic or Latino Communities\",\n           size = 15,  family = \"Pragati Narrow\") +\n  annotate(geom = \"text\", x = 2.1, y = 0.8, label = \"53% of Other Communities\",\n           size = 15,  family = \"Pragati Narrow\") +\n  annotate(geom = \"text\", x = 1.45, y = 2.95, label = \"COMMUNITY EXPOSURE TO GUN VIOLENCE\",\n           size = 26,  family = \"Pragati Narrow\", fontface = \"bold\") +\n  annotate(geom = \"text\", x = 1.45, y = 2.82, label = \"Communities that experienced inidents of gunshots in 2021\",\n           size = 19,  family = \"Pragati Narrow\") +\n  annotate(geom = \"text\", x = 1.5, y = 0, label = \"Data comes from ShotSpotter gunshot detection system. Incidents of probable gunfires and firecrackers are excluded | Visualization by Zhaowen Guo\",\n           size = 12,  family = \"Pragati Narrow\",color = \"grey20\")\n\nggsave(\"dc_ethnic.png\", width = 14, height = 14/1.618, units = \"in\")\n```\nHere’s the final output! With this filled icon chart (sometimes referred to as a pictogram chart), it is easy to see that Hispanic/Latino communities are significantly impacted by gun violence compared to other ethnic groups."
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html",
    "href": "dataviz/geofacet-hexbin/index.html",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "",
    "text": "Last week, I introduced how to visualize disparities in tree equity scores using hexbin maps. Hexbin maps are a useful tool for visualizing dense data points by summarizing them in a compact and understandable format. By grouping data points into hexagonal bins, they provide a clear picture of the spatial distribution of the data, enabling the detection of patterns that may be hidden by overplotting.\nAlthough hexagonal bins provide a clean representation of data, they result in information loss regarding the underlying shape of the data. For example, my previous visual using hexbin maps suggests that Ohio has the largest tree equity gap, which is defined as the maximum difference across block-level tree equity scores. However, this does not provide a complete picture. When examining the distribution of tree equity scores across census blocks within each state, it is evident that Ohio has a considerable number of blocks with relatively high levels of tree equity, indicating that Ohio performs well in this regard.\nGeofacet offers a useful alternative to hexbin maps, allowing for the restoration of the original data distributions while preserving the spatial information. By maintaining individual data points, geofacet provides a more detailed representation of data to enable a more nuanced view of spatial relationships.\nIn this blog post, I will walk you through how to visualize tree equity score data using geofacet. All code can be found here."
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html#data-collection",
    "href": "dataviz/geofacet-hexbin/index.html#data-collection",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "Data Collection",
    "text": "Data Collection\nWe follow similar steps of data collection and the only difference is we keep the original variable - tree equity score.\n```{r}\n# prepare a function to read zip urls with shapefiles \nread_shape_URL &lt;- function(URL){\n  cur_tempfile &lt;- tempfile()\n  download.file(url = URL, destfile = cur_tempfile)\n  out_directory &lt;- tempfile()\n  unzip(cur_tempfile, exdir = out_directory)\n  \n  read_sf(dsn = out_directory)\n}\n\n# pull state, tes, priority from each dataframe\ndata_lists &lt;- list()\nfor (i in 1:nrow(states)){\n  state &lt;- states$lower_code[i]\n  print(state)\n  URL &lt;- paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\")\n  if (RCurl::url.exists(URL) == T) {\n    map &lt;- read_shape_URL(paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\"))\n    data_lists[[i]] &lt;- map %&gt;% select(tes, state, priority)\n  }\n}\ndata &lt;- do.call(rbind, data_lists) %&gt;% na.omit()\n```"
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html#data-visualization",
    "href": "dataviz/geofacet-hexbin/index.html#data-visualization",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "Data Visualization",
    "text": "Data Visualization\nThe function facet_geo() plays the magic, which creates the base map of the US that can be integrated with various data representations. By using the argument grid = us_state_grid1[c(-2, -11), ], we can exclude Hawaii and Alaska from the map as there is no data available for these states. If you prefer to have the full state name, you can add the argument label = \"name\".\nAnother thing to keep in mind is that the facet_geo() layer should integrate with a pre-existing data representation. In this example where I am interested in displaying the distribution of tree equity scores, I add geom_density() layer beforehand.\n```{r}\nggplot(data) +\n  geom_density(aes(x = tes), color = \"#466c4b\", fill = \"#7fa074\", alpha = 0.5) +\n  coord_cartesian(clip = \"off\") +\n  facet_geo(vars(state), scales = \"free_y\", grid = us_state_grid1[c(-2, -11), ], label = \"name\") +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  scale_y_continuous(\n    labels = scales::number_format(accuracy = 0.01)) +\n  labs(x = \"\", y = \"\",\n       title = \"ACCESS TO GREEN SPACE\",\n       subtitle = \"Distribution of tree equity scores across census blocks in the US\",\n       caption = str_wrap(\"\n       Tree Equity Score (TES) computes how much tree canopy and surface temperature align with income, \n       employment, race, age and health factors in the US, collected by American Forest | Visualization by Zhaowen Guo\", width = 300)) +\n  theme_void(base_family = \"Pragati Narrow\") +\n  theme(strip.text = element_text(face = \"bold\", color = \"grey20\", size = 30),\n        legend.position = \"none\",\n        axis.text = element_text(color = \"grey40\", size = 30),\n        strip.background = element_blank(),\n        plot.background = element_rect(fill = \"#f5f5f2\", color = NA),\n        plot.margin = margin(40, 15, 20, 15),\n        plot.title = element_text(face = \"bold\", size = 70, margin = margin(l=0, t=5)),\n        plot.subtitle = element_text(lineheight = 1, size = 50, margin(l=0, t=7)),\n        plot.caption = element_text(margin = margin(t=35), color = \"grey20\", size = 30),\n        plot.caption.position = \"plot\")\n\nggsave(\"tree-equity-geofacet.png\", dpi = 320, width = 14, height = 10)\n```"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html",
    "href": "dataviz/green-space-gunshot-map/index.html",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "",
    "text": "OpenStreetMap (OSM) is an open-source mapping platform that provides detailed maps of the world. It is built and maintained by a global community of volunteers who contribute data, such as roads, buildings, and points of interest, to create a free and open map of the world.\nOSM offers a comprehensive API that enables users to easily access and use the map data seamlessly in their own applications. Previously, I relied on the OpenLayers plugin in QGIS to access OSM data. While QGIS provides a nice graphical user interface for loading and visualizing OSM data, I had to switch between R and QGIS to ensure consistency in the resulting graphs. Here comes the good news! Now we can directly access OSM from R using the osmdata package. By specifying the coordinates or the name of the geographic area of interest, we can easily obtain OSM data and perform analyses in R.\nContinuing on my previous articles about green spaces, in this blog post I will introduce how to map green spaces using OSM. Check out my full code here."
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html#data-collection",
    "href": "dataviz/green-space-gunshot-map/index.html#data-collection",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "Data Collection",
    "text": "Data Collection\nOnce we have installed the osmdata package, we can begin gathering our data. The first step is to define a specific area of interest using coordinates or place names. For example, to download all OSM data for Washington DC, we can try either of the following approaches to specify the bounding box:\n```{r}\n# first approach\nopq(getbb(\"Washington, District of Columbia\"))\n\n# second approach\nopq(bbox = c(-77.10, 38.80, 76.90, 39.00))\n```\nDepending on how we define green spaces, we can use add_osm_feature() to set specific key and value attributes and call osmdata_sf() to convert the output to a simple features (sf) object which will simplify the plotting process later. The same approach can be used to display other map elements, such as streets and rivers. The following code will extract the previously obtained OSM data that matches the defined attribute tags.\n```{r}\n# green spaces defined as parks, nature reserve, and golf course\ngreens &lt;- opq(getbb(\"Washington, District of Columbia\")) %&gt;%                \n  add_osm_feature(key = \"leisure\", \n                  value = c(\"park\", \"nature_reserve\", \"golf_course\")) %&gt;%\n  osmdata_sf()\n  \n# green spaces defined as grass  \ngreens &lt;- opq(getbb(\"Washington, District of Columbia\")) %&gt;%\n  add_osm_feature(key = \"landuse\", \n                  value = \"grass\") %&gt;%\n  osmdata_sf()\n```"
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html#data-visualization",
    "href": "dataviz/green-space-gunshot-map/index.html#data-visualization",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "Data Visualization",
    "text": "Data Visualization\nWith our data now downloaded, we can begin visualizing it using ggplot2. To visualize spatial data, we simply add a layer by geom_sf(). We can include an argument inherit.aes = FALSE to customize each layer, making its aesthetics (i.e. color, size) not inherit from previous layers.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1)\n```\nWe can also set the coordinate reference system (CRS) for the spatial data being plotted using coord_sf(). By default, the CRS is set to WGS 84 (EPSG code 4326), which I used in this example, and You can adjust it as needed. We can also specify the longitude and latitude in the same layer to “zoom in” the area of interest.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1) +\n  coord_sf(crs = st_crs(4326), xlim = c(-77.12, -76.90), ylim = c(38.79, 39.01)) \n```\nWe can also add additional layers to enrich our spatial visualization by displaying other types of geographic data, such as point locations. For example, in this illustration, I incorporated a geom_point() layer highlighting the locations of 2021 gunshot incidents.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1) +\n  geom_point(data = gunshot, aes(x = LONGITUDE, y = LATITUDE), color = \"#c62320\", size = 0.1, inherit.aes = FALSE) +\n  coord_sf(crs = st_crs(4326), xlim = c(-77.12, -76.90), ylim = c(38.79, 39.01))\n```\nJust with a few additional aesthetic touches, I was able to create a plot that effectively visualizes both green spaces in the DC area and the recorded gunshot incidents that occurred in 2021."
  },
  {
    "objectID": "dataviz/tidytuesday-african-sentiments/index.html",
    "href": "dataviz/tidytuesday-african-sentiments/index.html",
    "title": "TidyTuesday: African Language Sentiments",
    "section": "",
    "text": "TidyTuesday is a weekly data project aimed at improving skills in data manipulation, visualization and analysis using R programming language. This week, we are exploring the African language sentiments. I experimented with a different way to represent proportion by using a measuring cup! I adjusted the positioning and size of the cup so that the actual proportion aligned with the markings on the cup. Please find my code here!"
  },
  {
    "objectID": "dataviz/tidytuesday-drug-market/index.html",
    "href": "dataviz/tidytuesday-drug-market/index.html",
    "title": "TidyTuesday: European Drug Development",
    "section": "",
    "text": "TidyTuesday is a weekly data project aimed at improving skills in data manipulation, visualization and analysis using R programming language. This week, we are exploring the the development of European drugs scraped by Miquel Anglada Girotto from European Medicines Agency. I found that drugs developed for COVID-19 were brought to market much faster than non-COVID drugs. To emphasize this key finding, I incorporated a magnifying glass logo into my visualization to make it more eye-catching. Please find my code here!"
  },
  {
    "objectID": "dataviz/tidytuesday-hollywood-age/index.html",
    "href": "dataviz/tidytuesday-hollywood-age/index.html",
    "title": "TidyTuesday: Hollywood Age Gap",
    "section": "",
    "text": "TidyTuesday is a weekly data project aimed at improving skills in data manipulation, visualization and analysis using R programming language. This week, we are exploring the age gap between movie love interests. Check out my code here!"
  },
  {
    "objectID": "dataviz/tree-equity/index.html",
    "href": "dataviz/tree-equity/index.html",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "",
    "text": "Urban green spaces, such as parks, gardens, and forests, provide numerous benefits in creating livable cities. They help improve air and water quality, reduce heat islands, and increase physical activity and mental health. However, not all communities have equal access to these benefits, leading to the concept of “tree equity.”\nTree equity refers to the fair distribution of urban green spaces and trees, regardless of a community’s socio-economic status, race, or ethnicity. American Forests, a non-profit organization dedicated to protecting and restoring forests, has been working to measure and improve tree equity across the United States.\nIn this blog post, I will show you how to visualize tree equity score data to tell a compelling story. All code can be found here."
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-collection",
    "href": "dataviz/tree-equity/index.html#data-collection",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Collection",
    "text": "Data Collection\nTree equity score data are currently hosted on this webpage, where users are required to manually download zip files containing geospatial data for each state. Furthermore, each shapefile includes not only tree equity scores, but also several other variables. Thus, we need to find an efficient method to automate the process of downloading files with minimal manual intervention and extract the desired variable of interest - the tree equity score - from the shapefiles.\nUpon checking the web addresses of several files, a pattern has become apparent: each URL shares a similar structure in its file name, while also possessing a unique postal code. This discovery suggests that it is possible to create a script that can automatically scrape the files. Fortunately, unzip() in base R and sf package offer a convenient solution for unzipping files and reading shapefiles, and we can easily write a function to automate this process.\n```{r}\nlibrary(sf)\nlibrary(tidyverse)\n\nread_shape_URL &lt;- function(URL){\n  cur_tempfile &lt;- tempfile()\n  download.file(url = URL, destfile = cur_tempfile)\n  out_directory &lt;- tempfile()\n  unzip(cur_tempfile, exdir = out_directory)\n  \n  read_sf(dsn = out_directory)\n}\n```\nThe question that I am interested in is: which state has the greatest disparity in tree equity score across counties, as measured by the difference between the maximum and minimum tree equity scores within each state. Also, it is important to note that data availability may vary as not all states have available tree equity score data. To verify the existence of a URL, we can use Rcurl package and write a for-loop to produce the key variable of interest, the tree equity gap.\n```{r}\nstates &lt;- read.csv(\"state-names.csv\") # downloaded https://worldpopulationreview.com/states/state-abbreviations\n\nstate_names &lt;- rep(NA, 51)\ntes_gaps &lt;- rep(NA, 51)\nfor (i in 1:nrow(states)){\n  state &lt;- states$lower_code[i]\n  state_names[i] &lt;- state\n  print(state)\n  URL &lt;- paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\")\n  if (RCurl::url.exists(URL) == T) {\n    map &lt;- read_shape_URL(paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\"))\n    tes_gap &lt;- max(map$tes) - min(map$tes)\n    tes_gaps[i] &lt;- tes_gap\n  } else {\n    tes_gaps[i] &lt;- NA\n  }\n}\n\ndata &lt;- data.frame(lower_code = state_names,\n                   gap = tes_gaps) %&gt;%\n  cbind(states[1])\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-cleaning",
    "href": "dataviz/tree-equity/index.html#data-cleaning",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nGreat, we now have our data! The next step is to clean and prepare it for visualization. With the consideration of working with spatial data across multiple regions and the fact that the tree equity gap is a continuous variable, hexbin maps become a desirable choice. Hexbin maps divide the map area into small hexagonal bins and consolidate the data points within them, presenting a clear and compact depiction of the vast amount of data.\n```{r}\nlibrary(geojsonio)\nlibrary(rgeos)\n\n# create a base hexbin map of US\nhex_states &lt;- geojson_read(\"us_states_hexgrid.geojson\", what = \"sp\") \n\n# extract state names\nhex_states@data &lt;- hex_states@data %&gt;%\n  mutate(google_name = str_replace(google_name, \" \\\\(United States\\\\)\", \"\"))\n\n# create a data frame for hexbin map\nhex_states_fortify &lt;- broom::tidy(hex_states, region = \"google_name\")\n\n# match state names\ndata_map &lt;- hex_states_fortify %&gt;%\n  right_join(data, by = c(\"id\" = \"state\")) %&gt;%\n  mutate(id = state.abb[match(id, state.name)])\ndata_map$id[data_map$group == \"District of Columbia.1\"] &lt;- \"DC\"\n\nlabels &lt;- cbind.data.frame(data.frame(gCentroid(hex_states, byid = T),\n                                      id = hex_states@data$iso3166_2))\ndata_map &lt;- data_map %&gt;%\n  right_join(labels, by = \"id\") %&gt;%\n  filter(is.na(gap) == F)\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-visualization",
    "href": "dataviz/tree-equity/index.html#data-visualization",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Visualization",
    "text": "Data Visualization\nWe now have our spatial polygon data frame ready and can visualize it! To make it visually appealing, I pick a custom Google font “Pragati Narrow” for the graph and a stunning color palette from the scico package. One trick is to adjust text colors that contrast with the background color. For instance, in the case of Washington D.C., which has the narrowest gap in tree equity scores, the bin’s background can be made lighter. However, if white text is still used as in other areas, the label may not be easily visible. To mitigate this, we can establish a threshold for switching the text color as necessary.\n```{r}\nlibrary(scico)\nlibrary(showtext)\ntheme_set(theme_minimal(base_family = \"Pragati Narrow\"))\n\ntheme_update(\n  # legend\n  legend.title = element_blank(),\n  legend.position = 'top',\n  legend.direction = 'horizontal',\n  legend.key.width = unit(1.5, \"cm\"),\n  legend.text = element_text(color = \"black\",  size=30),\n  \n  # axis\n  axis.text.x = element_blank(),\n  axis.text.y = element_blank(),\n  \n  # titles\n  panel.grid = element_blank(),\n  plot.margin = margin(15, 30, 15, 30),\n  plot.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  panel.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  legend.title.align=1,\n  plot.title = element_text(\n    color = \"black\", \n    size = 70, \n    face = \"bold\",\n    margin = margin(t = 15),\n    hjust = 0.5\n  ),\n  plot.subtitle = element_text(\n    color = \"grey10\", \n    size = 45,\n    lineheight = 3,\n    margin = margin(t = 5),\n    hjust = 0.5\n  ),\n  plot.title.position = \"plot\",\n  plot.caption.position = \"plot\",\n  plot.caption = element_text(\n    color = \"grey20\", \n    size = 20,\n    lineheight = 0.5, \n    hjust = 0.5,\n    margin = margin(t = 40))\n)\n\ndata_map %&gt;% \n  ggplot () +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = gap), linewidth = 0.5) +\n  scale_fill_scico(palette = \"lajolla\", direction = 1) + \n  geom_text(aes(x=x, y=y, label=id, color = gap &lt; 60), size = 8, alpha = 0.5, \n             show.legend = F) +\n  scale_color_manual(values = c(\"white\", \"black\")) +\n  coord_map(clip = \"off\") +\n  labs(title = \"TREE EQUITY GAP\",\n       subtitle = \"Block-level disparities in tree equity scores within each state\",\n       x = \"\", y = \"\",\n       caption= \n         str_wrap(\n       \"Data comes from the Green Space Data Challenge, \n       collected and shared by the American Forests. \n       Tree Equity Score (TES) computes how much tree canopy and surface temperature align with income, \n       employment, race, age and health factors in the U.S | Visualization by Zhaowen Guo\", width=150))\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#implications",
    "href": "dataviz/tree-equity/index.html#implications",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Implications",
    "text": "Implications\nWhat do we learn from this visualization? The graph clearly illustrates the unequal distribution of green spaces, particularly in states like Ohio and Minnesota, where the gap is much more pronounced. This calls for prompt and effective action to rectify this imbalance."
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Getting Spooky with R\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: European Drug Development\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: African Language Sentiments\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: Bob Ross Paintings\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing Racial Justice: Moving Beyond Bar Charts\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nMapping Green Spaces with OpenStreetMap in R\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: Hollywood Age Gap\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nThe Calendar of Gun Violence\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nRevisiting Tree Equity Gap: Hexbin or Geofacet?\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday: Tech Company Stock Prices\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nGreen Space for Everyone? Visualizing Tree Equity Gap\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/bias-variance/index.html",
    "href": "statistics/bias-variance/index.html",
    "title": "Explaining the Bias-Variance Tradeoff",
    "section": "",
    "text": "In order to assess the model performance on a given dataset, we need to measure how closely its predictions align with the actual data. In regression settings, we often use mean squared error (MSE) and a smaller MSE means the predicted responses are closer to the true responses.\nWhile we can get MSE from both training and the test data, what we really care is the model performance on unseen test data. This is where the bias-variance tradeoff comes into play. The process of selecting a model that minimizes MSE on the test data inherently involves a tradeoff between bias and variance. In this blog post, we will explore the concept of the bias-variance tradeoff with the help of visual aids and a mathematical proof.\nBias refers to the error that results from oversimplifying the model. A model with high bias may be too simple to capture the complexity of the underlying data or identify the relationship between input and outcome variables.\nOn the other side, variance refers to the amount by which the model would change if we estimated it using a different training set. A model with high variance means that the model is overly complex such that it captures noises in the data instead of the underlying patterns. This leads to the problem of over-fitting where our model performs too well on the training data but poorly on unseen data.\nAs shown in the following graph, model (1) which is a constant has extremely high bias but zero variance, while model (2) which looks extremely wiggly by just connecting observed data points has low bias but high variance.\nThe bias-variance tradeoff arises because as the flexibility (or complexity) of the model increases, we often have smaller bias but larger variance, and vice versa. The relative changes in bias and variance determine whether the test MSE increases or decreases.\nMathematically, we can decompose expected prediction error (expected test MSE) into two components: irreducible error and reducible error. Irreducible error is the variation that cannot be reduced by modifying the model, and is associated with unmeasured variables. Reducible error is comprised of the sum of squared bias and variance of the model. Our objective is to minimize reducible error while recognizing that we cannot surpass the irreducible error. A proof of this decomposition can be found at the end of this post.\nThe following graph contains all the concepts we’ve discussed in this post. In future posts, we will delve deeper into how to find an optimal balance between bias and variance using different machine learning models."
  },
  {
    "objectID": "statistics/bias-variance/index.html#proof",
    "href": "statistics/bias-variance/index.html#proof",
    "title": "Explaining the Bias-Variance Tradeoff",
    "section": "Proof",
    "text": "Proof\nSuppose that we have a simple model specified below, we can break down the expected prediction error (expected test MSE) \\(E(y_0) - \\hat f(x_0))^2\\) and see where the bias-variance tradeoff comes from.\n\\[\ny = f(x) + \\epsilon \\\\\n\\text{where } E(\\epsilon) = 0, \\epsilon \\perp x, x \\text{ is fixed }\n\\]\nWe can rewrite the expected prediction error as below \\[\nE(y_0) - \\hat f(x_0))^2 \\\\\n= E(y_0 \\color{red}{- f(x_0) + f(x_0)} \\color{blue}{- E\\hat f(x_0) + E\\hat f(X_0)} - \\hat f(x_0))^2\n\\] What’s within \\(E()^2\\) can be thought of as a summation of three parts a, b, and c where \\[\na = y_0 - f(x_0) \\\\\nb = f(x_0) - E\\hat f(x_0) \\\\\nc = E\\hat f(x_0) - \\hat f(x_0)\n\\] Recall that \\[\n(a+b+c)^2 = a^2 + b^2 + c^2 + 2ab + 2bc + 2ac\n\\] then we have \\[\nE(y_0) - \\hat f(x_0))^2 \\\\\n= E(a^2) + E(b^2) + E(c^2) + 2E(ab) + 2E(bc) + 2E(ac)\n\\] Let’s walk through the last three terms first.\nIt’s not hard to see \\(E(ab) = 0\\), \\(E(bc)=0\\), and \\(E(ac)=0\\) as highlighed below.\n\\[\n\\begin{aligned}\nE(ab) &= E((y_0 - f(x_0))(f(x_0) - E \\hat f(x_0))) \\\\\n&= \\color{red}{E(y_0 - f(x_0))}(f(x_0) - E \\hat f(x_0))\\\\\n&= 0\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(bc) &= (f(x_0) - E \\hat f(x_0)) E((E\\hat f(x_0) - \\hat f(x_0)))\\\\\n&= (f(x_0) - E \\hat f(x_0)) \\color{red}{(E \\hat f(x_0) - E \\hat f(x_0))} \\\\\n&= 0\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\nE(ac) &= E((y_0) - f(x_0)) \\color{red}{(E \\hat f(x_0) - \\hat f(x_0))}\\\\\n&= 0\n\\end{aligned}\n\\] Now, let’s turn to the first three terms.\nRecall that \\[\nE(E(\\hat \\theta) - \\hat \\theta)^2 = var(\\hat \\theta)\\\\\n\\text{Bias}(\\hat \\theta)^2 = (E(\\hat \\theta) - \\theta))^2\n\\]\nWe can rewrite these terms as follows: \\[\n\\begin{aligned}\nE(a^2) &= E(y_0 - f(x_0))^2 \\\\\n&= E(\\epsilon_0)^2 \\\\\n&= E(\\epsilon^2) - \\color{red}{(E(\\epsilon))^2} \\\\\n&= var(\\epsilon)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(b^2) &= E(f(x_0) - E \\hat f(x_0))^2 \\\\\n&= (f(x_0) - E \\hat f(x_0))^2 \\\\\n&= \\text{Bias}^2(\\hat f(x_0))\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\nE(c^2) &= E(E\\hat f(x_0) - \\hat f(x_0))^2 \\\\\n&= var(\\hat f(x_0))\n\\end{aligned}\n\\] Taken together, it becomes clear that the expected prediction error can be decomposed into three parts. The first term \\(var(\\epsilon)\\) is the irreducible error and the others constitute the reducible error that we try to minimize.\n\\[\nE(y_0) - \\hat f(x_0))^2 = \\color{red}{var(\\epsilon)} +  \\color{blue}{\\text{Bias}^2(\\hat f(x_0)) + var(\\hat f(x_0))}\n\\]"
  },
  {
    "objectID": "statistics/rasch-hlm/index.html",
    "href": "statistics/rasch-hlm/index.html",
    "title": "Unveilling Model Equivalence: Linking the Rasch Model with the Hierarchical Linear Model",
    "section": "",
    "text": "When learning more about statistical modeling, I find it helpful to consider the relationships and connections between different models. This allows me to gain a better understanding of the various approaches to model fitting and their respective advantages and disadvantages. In this blog post, we will explore the connection between two commonly used models in psychometrics: the Rasch model and the hierarchical linear model. For those interested, I have also included links to my slides and code.\nThe Rasch model, also known as the one-parameter logistic model (1PL), is a statistical model used to analyze responses to rating scale items. It is based on the idea that the probability of a person getting an item correct depends on the person’s ability and the difficulty of the item. Mathematically, the Rasch model can be expressed as follows.\n\\[\nP_i(X_{ij} = 1|\\theta_j, b_i) = \\frac{e^{(\\theta_j - b_i)}}{1 + e^{(\\theta_j - b_i)}}\n= \\frac{1}{1 + e^{-(\\theta_j - b_i)}}\n\\] where i means respondents, j represents items, \\(X_ij\\) refers to response of person j to item i and takes a value of 0 or 1, \\(\\theta_j\\) corresponds to ability for person j, and \\(b_i\\) is the difficulty parameter for item i.\nIntuitively, we can recover the hierarchical structure in item responses by treating persons as the higher level (level-2) and items as the lower level (level-1). This data structure has two characteristics that our model needs to capture:\n\nrepeated measures nested within each person\ninterdependent item responses from the same person\n\nNow let’s rewrite the Rasch model into two levels where the level 1 is the item level and level 2 is the person level.\nApplying logit link function, we have \\[\n\\begin{aligned}\nlog(\\frac{p_{ij}}{1 - p_{ij}}) &= \\beta_{0j} + \\beta_1X_{1ij} + ...+\\beta_{(k-1)j}X_{(k-1)ij} \\\\\n&= \\beta_0j + \\sum_{q=1}^{k-1}\\beta_{qj}X_{qij}\n\\end{aligned}\n\\] where q=1,…,k-1 since the dummy variable for the reference item is dropped, \\(X_ij\\) is the \\(i^{th}\\) term dummy indicator for person j, \\(\\beta_{0j}\\) is an intercept term of the expected effect of the reference item for person j, and \\(\\beta_{qj}\\) is the difference of effect for item q from \\(\\beta_{0j}\\).\nNow let’s turn to level 2, the person level. The basic idea is to introduce randomness to the reference term \\(\\beta_{0j}\\) in level 1, which means we want it to vary across persons. We don’t want to introduce randomness to the other \\(\\beta\\) terms because items answered by the same person j are interdependent. Therefore, level 2 model can be written as below:\n\\[\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\color{red}{u_{0j}} \\\\\n\\beta_{1j} &= \\gamma_{10} \\\\\n...\\\\\n\\beta_{(k-1)j} &= \\gamma_{(k-1)0}\n\\end{aligned}\n\\] The magic appears when we combine level-1 and level-2 models:\n\\[\nP_{ij} = \\frac{1}{1 + exp{-[\\color{red}{u_{0j}} - (\\color{blue}{-\\gamma_{i0} - \\gamma_{00}}})]}\n\\] which looks equivalent to the Rasch model we saw before\n\\[\nP_i(X_{ij} = 1 |\\theta_j, b_i) = \\frac{1}{1 + e^{(-\\theta_j - b_i)}} = \\frac{1}{1 + exp[-(\\color{red}{\\theta_j} - \\color{blue}{b_i})]}\n\\] where \\[\n\\theta_j = u_{0j} \\\\\nb_i = -\\gamma_{i0} - \\gamma_{00}\n\\] What can we learn from this model equivalence? One potential application is that we can now apply both the mirt package and the lme4 package, commonly used for fitting hierarchical models, to fit the Rasch model. The following example, which uses simulated LSAT data, demonstrates how this approach can be implemented.\nWhen using the mirt package, our item response data is typically organized such that each row represents a respondent, while each column represents a question. We can derive both person and item parameters by employing the following code:\n```{r}\n# fit a Rasch model\nrasch &lt;- mirt(data  = lsat,\n              model = 1,\n              itemtype = \"Rasch\",\n              SE = TRUE)\n\n# retrieve item parameter estimates\nrasch_coef &lt;- coef(rasch, IRTpars=TRUE, simplify=TRUE)\n\n# retrieve person parameter estimates\nrasch_fs &lt;- fscores(rasch, full.scores.SE = TRUE)\n```\nWe can also fit the same model using the lme4 package, although we must first restore its hierarchical data structure. Specifically, we need to convert the wide-format data into long-format data, where the first column denotes the individuals, the second column denotes the items, and the third column records the item responses.\n```{r}\n# reshape the data into a long format \nlsat_long &lt;- lsat %&gt;%\n  mutate(ID = row_number()) %&gt;%\n  pivot_longer(!ID, names_to = \"items\", values_to = \"responses\")\n\n# fit the model \nmlm &lt;- glmer(responses ~ -1 + items + (1|ID), \n             family = \"binomial\", \n             data = lsat_long, \n             control = control)\n```\nWe can now compare the estimates of item difficulty between the two approaches. The item difficulty estimates derived via the mirt package, represented by the b parameter, are compared with the fixed effects estimated via the lme4 package. One thing to note here is that fixed effects denote item easiness (because \\(b_i = \\gamma_{i0} - \\gamma_{00}\\)) and thus we need to invert the sign to accurately reflect item difficulty.\n```{r}\n# compare item estimates\ncoef(summary(mlm))[,1]\nrasch_items$b\ncor(coef(summary(mlm))[,1], rasch_items$b) # -1\n\n# compare person estimates\nunlist(ranef(mlm))\nfscores(rasch)\ncor(unlist(ranef(mlm)), fscores(rasch)) # 1\n\n```\nFor more details, please refer to Kamata (2001) which provides further insight into how this model equivalence can be extended to a third level."
  },
  {
    "objectID": "statistics/svd/index.html",
    "href": "statistics/svd/index.html",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "",
    "text": "SVD is not nearly as famous as it should be. — Gilbert Strang\nSVD is a great 1-stop shop for data analysis. — Daniela Witten"
  },
  {
    "objectID": "statistics/svd/index.html#introduction",
    "href": "statistics/svd/index.html#introduction",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Introduction",
    "text": "Introduction\nSingular Vector Decomposition (SVD) is a matrix factorization technique that has become a cornerstone in the field of machine learning (ML). It not only allows for efficiently calculating the inverse of a matrix (if it exists) by multiplying the inverse of each decomposed simpler matrices, but also opens the door to a wide array of applications in ML and beyond.\nIn what follows, I will start by the definition and properties of SVD, and establish its connection with Principal Component Analysis (PCA). Then I will demonstrate different applications of SVD in ML, including but not limited to missing value imputation and latent feature extraction.\nDefinition and properties of SVD\nSVD decomposes a data matrix \\(X_{m \\times n}\\) into three matrices \\(U_{m\\times r}\\), \\(D_{r\\times r}\\), and \\(V_{n\\times r}\\), regardless of the characteristics of the original matrix.\n\\[\nX = UDV^T\n\\] where\n\nU and V are orthogonal matrices (\\(U^T U = I\\) and \\(V^T V = I\\)), which are called left singular vector, and right singular vector, respectively\nD is a diagonal matrix with non-negative and decreasing elements, which are called singular values\n\n\nLet’s first check dimensions of the resulting matrices after applying SVD to a toy matrix X.\n\n# Define a matrix\nX &lt;- matrix(c(1:12),\n            nrow = 4,\n            ncol = 3,\n            byrow = T)\n\n# Apply SVD\nsvd_result &lt;- svd(X)\n\n# Extract U, D, and V matrices\nU &lt;- svd_result$u\nD &lt;- diag(svd_result$d)\nV &lt;- svd_result$v\nprint(paste0(\"The dimension for U matrix: \", dim(U)[1], \" X \", dim(U)[2]))\n\n[1] \"The dimension for U matrix: 4 X 3\"\n\nprint(paste0(\"The dimension for D matrix: \", dim(D)[1], \" X \", dim(D)[2]))\n\n[1] \"The dimension for D matrix: 3 X 3\"\n\nprint(paste0(\"The dimension for V matrix: \", dim(V)[1], \" X \", dim(V)[2]))\n\n[1] \"The dimension for V matrix: 3 X 3\"\n\n\nWe can then check matrix properties of SVD. As we can observe, matrices U and V are orthogonal, and matrix D is diagonal.\n\n# Check properties of U and V (orthogonal matrix)\nis_orthogonal &lt;- function(A){\n  A_T &lt;- t(A)\n  dot_product_1 &lt;- A %*% A_T\n  dot_product_2 &lt;- A_T %*% A\n  identity_matrix_1 &lt;- diag(nrow(A))\n  identity_matrix_2 &lt;- diag(ncol(A))\n  \n  result &lt;- isTRUE(all.equal(dot_product_1, identity_matrix_1)) +\n            isTRUE(all.equal(dot_product_2, identity_matrix_2)) # all.equal checks \"nearly equal\"\n  \n  return(result&gt;=1)\n}\n\nis_orthogonal(U) # TRUE\n\n[1] TRUE\n\nis_orthogonal(V) # TRUE\n\n[1] TRUE\n\n# Check properties of D\ndiag(D) # diagonal values (or singular values in this case)\n\n[1] 2.546241e+01 1.290662e+00 2.311734e-15\n\nD[!row(D) == col(D)] # off-diagonal values are 0\n\n[1] 0 0 0 0 0 0\n\n\nConnection between SVD and PCA\nNow, knowing that SVD can be used to approximate any matrix, it’s an opportune moment to revisit Principal Component Analysis (PCA), an unsupervised ML method that we might be more familiar with. As we will see, SVD on a de-meaned (centered) data matrix is the same as PCA.\nRecall that PCA seeks to find principal components, or the direction in the feature space with maximum variance in the data.\n\n# Center the data matrix (column means are 0)\nX_centered &lt;- scale(X, center = T, scale = T)\ncolMeans(X_centered) # check if centered\n\n[1] 0 0 0\n\n# Apply SVD to the centered data matrix\nsvd_result &lt;- svd(X_centered)\n\n# Apply PCA to the data\npca_result &lt;- prcomp(X, scale. = T)\n\nAs we can see, columns of the right singular vector V correspond to principal components extracted from PCA, and SVD also yields less elapsed time than PCA. A key advantage of SVD is that it does not require a preliminary step of constructing a covariance as PCA does, providing greater computational efficiency in extracting principal components.\nThis efficiency becomes particularly prominent when handling\n\nHigh-dimensional datasets: when a data matrix possess too many features, the computational cost for constructing its covariance matrix can be huge\nFull-rank data matrix: when the data matrix is full-rank, it often implies that many singular values will be non-negligible, and many principal components will be needed to reconstruct the original matrix\n\n\nprint(svd_result$v) # right singular vectors\n\n          [,1]       [,2]       [,3]\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n\nprint(pca_result$rotation) # principal components\n\n           PC1        PC2        PC3\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n\n\n\n# Construct a high-dimensional and sparse data matrix\nn_rows &lt;- 1000\nn_cols &lt;- 500\n\nsparse_matrix &lt;- matrix(0, nrow = n_rows, ncol = n_cols)\n\n# Manually add some non-zero elements to mimic sparsity\nset.seed(123)\nnon_zero_elements &lt;- 200\nfor (i in 1:non_zero_elements) {\n  row_index &lt;- sample(n_rows, 1)\n  col_index &lt;- sample(n_cols, 1)\n  sparse_matrix[row_index, col_index] &lt;- runif(1)\n}\n\n\n# Compute every possible rank approximations\nsystem.time({\n  svd_result &lt;- svd(sparse_matrix)\n})\n\n   user  system elapsed \n   0.02    0.00    0.59 \n\nsystem.time({\n  pca_res &lt;- prcomp(sparse_matrix)\n})\n\n   user  system elapsed \n   0.09    0.00    0.83 \n\n# Compute top 10 rank approximations\nsystem.time({\n  svd_result &lt;- irlba::irlba(sparse_matrix, nv = 10)\n})\n\n   user  system elapsed \n   0.02    0.02    0.80 \n\nsystem.time({\n  pca_res &lt;- prcomp(sparse_matrix, rank. = 10)\n})\n\n   user  system elapsed \n   0.00    0.00    0.66"
  },
  {
    "objectID": "statistics/svd/index.html#application-impute-missing-values",
    "href": "statistics/svd/index.html#application-impute-missing-values",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Application: Impute Missing Values",
    "text": "Application: Impute Missing Values\nOne popular application of SVD is to impute missing values. Without keeping all singular values and vectors, we can only retain the first d largest singular values to approximate the matrix A. The intuition is that the approximated matrix \\(A_d\\) being a dense matrix that captures the primary structure and patterns in the original data.\nThis procedure is also called lower-rank approximation, which can be implemented in the following steps:\n\nMatrix approximation: fill in NAs with an initial guess (e.g. column means, zeros) and apply SVD with rank d, meaning that we only keep top d singular values and vectors\nMissingness imputation: use the approximated matrix \\(A_d\\) to fill in NAs in the original matrix\n\nLet’s use the following example for illustration:\nWe start by creating a toy data matrix A and call it our ground truth matrix. Then we manually add sparsity by replacing certain elements with NAs.\n\n# Create a toy dataset (sparse matrix)\nset.seed(123)\nA &lt;- matrix(sample(c(NA, 1:5), 25, replace = T), 5, 5)\nground_truth_matrix &lt;- A\nA[c(2, 8, 10, 14, 20)] &lt;- NA\n\n\nA\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]   NA    2   NA    2    4\n[3,]    2   NA    1   NA    2\n[4,]    1    3   NA    3    1\n[5,]    1   NA    4   NA    1\n\nground_truth_matrix\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]    5    2   NA    2    4\n[3,]    2    4    1   NA    2\n[4,]    1    3    2    3    1\n[5,]    1    5    4   NA    1\n\n\nNext, we apply SVD with varying d, which indicates the number of singular values/vectors.\n\n# Define svd\nimpute_svd &lt;- function(matrix, d){\n  \n  # fill in missingness with column means\n  column_means &lt;- colMeans(matrix, na.rm = T)\n  matrix_filled &lt;- matrix\n  na_indices &lt;- is.na(matrix)\n  matrix_filled[na_indices] &lt;- column_means[col(matrix)[na_indices]]\n  \n  # perform svd\n  svd_res &lt;- svd(matrix_filled)\n  svd_res$d &lt;- c(svd_res$d[1:d], rep(0, length(svd_res$d) - d))\n  \n  # reconstruct the matrix\n  approx_matrix &lt;- svd_res$u %*% diag(svd_res$d) %*% t(svd_res$v)\n  imputed_vals &lt;- approx_matrix\n  imputed_vals[!is.na(matrix)] &lt;- NA\n  return(imputed_vals)\n}\n\nWe can use the approximated matrix \\(A_d\\) to reconstruct the original matrix and impute missing values. We can evaluate the performance of missingness imputation by mean squared error (MSE).\n\n# Construct the metric MSE\nmse &lt;- function(predicted, truth) {\n  mean((predicted - truth)^2, na.rm = TRUE)\n}\n\n# Display MSE for different d values (rank, or number of dimensions to define the reduced matrix)\nsvd_errors &lt;- numeric(5)\nfor (d in 1:5) {\n  imputed_values &lt;- impute_svd(A, d)\n  svd_errors[d] &lt;- mse(imputed_values, ground_truth_matrix)\n}\n\nHow does SVD perform? As a baseline, consider a simple approach by replacing missing values with column means. It seems that rank-2 approximation is an optimal choice, which yields the lowest MSE. However, it’s important to note that it is not always the case that SVD approximation would outperform simple column mean imputation. We might need to consider other matrix decomposition techniques for missingness imputation, such as Non-negative Matrix Factorization (NMF), Alternating Least Squares (ALS), etc..\n\n# Create baseline imputation from column means\nna_indices &lt;- is.na(A)\ncolmean_matrix &lt;- A\ncolmean_matrix[na_indices] &lt;- colMeans(A, na.rm = T)[col(A)[na_indices]]\ncolmean_errors &lt;- mse(colmean_matrix[na_indices], ground_truth_matrix[na_indices])\n\n# Report comparison of performance\ncomparison &lt;- tibble(\"Method\" = c(\"Column Means\", \n                                  \"Rank-1 Approximation\",\n                                  \"Rank-2 Approximation\",\n                                  \"Rank-3 Approximation\",\n                                  \"Rank-4 Approximation\",\n                                  \"Rank-5 Approximation\"),\n                     \"MSE\" = c(colmean_errors, svd_errors))\n\ncomparison %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\nMethod\nMSE\n\n\n\nColumn Means\n4.312500\n\n\nRank-1 Approximation\n4.754839\n\n\nRank-2 Approximation\n4.094591\n\n\nRank-3 Approximation\n4.146353\n\n\nRank-4 Approximation\n4.319335\n\n\nRank-5 Approximation\n4.312500\n\n\n\n\n\nIn line with the idea of missingness imputation, SVD can also be leveraged to enhance recommendation systems! The goal is to predict unknown preferences or ratings of users for items (e.g. movies, products, or services) based on existing ratings. A notable example is Netflix Prize competition, where Netflix offered $1 million award to anyone who could improve the accuracy of its movie recommendation system by 10%. The winning team just used SVD, along with techniques that incorporate other metadata, achieving a 10.06% improvement!"
  },
  {
    "objectID": "statistics/svd/index.html#application-topic-modeling",
    "href": "statistics/svd/index.html#application-topic-modeling",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Application: Topic Modeling",
    "text": "Application: Topic Modeling\nSVD is a powerful and generalizable technique that provides us another perspective on topic modeling. We begin by first transforming documents into a document-term matrix, where each row represents a document, each column reflects a term, and each cell denotes frequency. To refine this step further, we can also apply Term Frequency-Inverse Document Frequency (TF-IDF) to reweigh the cell values, adjusting for the uniqueness of each term for a given document.\nSVD can then be perceived as decomposing a document-term matrix \\(X_{m \\times n}\\) into\n\n\\(U_{m \\times r}\\): document-topic matrix\n\\(D_{r \\times r}\\): diagonal elements represent topic importance\n\\(V_{n \\times r}\\): term-topic matrix\n\nFor topic modeling, a crucial hyperparameter that requires tuning is the number of topics (often denoted by k). In the context of SVD, the idea is equivalent to selecting the top k singular values and their corresponding singular vectors in order to approximate the original data matrix.\n\n# Construct a document-term matrix\nlibrary(tidytext)\nlibrary(tm)\ndocuments &lt;- tibble(\n  doc_id = 1:8,\n  text = c(\"The sky is blue and beautiful.\",\n           \"Love this blue and beautiful sky!\",\n           \"The quick brown fox jumps over the lazy dog.\",\n           \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n           \"I love green eggs, ham, sausages, and bacon!\",\n           \"The brown fox is quick and the blue dog is lazy!\",\n           \"The sky is very blue and the sky is very beautiful today\",\n           \"The dog is lazy but the brown fox is quick!\")\n)\n\ntidy_documents &lt;- documents %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\ndtm &lt;- tidy_documents %&gt;%\n  count(doc_id, word) %&gt;%\n  cast_dtm(doc_id, word, n)\n\n\n# Apply SVD and examine each decomposed matrix\nsvd_result &lt;- svd(as.matrix(dtm))\n\nk &lt;- 2 # choose k=2 for simplicity\nUk &lt;- svd_result$u[, 1:k]\nDk &lt;- svd_result$d[1:k]\nVk &lt;- svd_result$v[, 1:k]\n\nAs we can see, the decomposed \\(U_k\\) matrix captures documents by topics.\n\nUs &lt;- tibble(`Document ID` = 1:8,\n             `Topic 1` = Uk[,1],\n             `Topic 2` = Uk[,2])\nUs %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\nDocument ID\nTopic 1\nTopic 2\n\n\n\n1\n-0.1294362\n0.4303175\n\n\n2\n-0.1392703\n0.4926330\n\n\n3\n-0.5597761\n-0.1933906\n\n\n4\n-0.0088357\n0.2551944\n\n\n5\n-0.0175544\n0.2534139\n\n\n6\n-0.5889438\n-0.0537521\n\n\n7\n-0.1672636\n0.6091753\n\n\n8\n-0.5246738\n-0.1772371\n\n\n\n\n\nThe singular values are stored in the following matrix \\(D_k\\), which correspond to how important each topic is.\n\nD_matrix &lt;- diag(Dk)\nrownames(D_matrix) &lt;- c(\"Topic 1\", \"Topic 2\")\ncolnames(D_matrix) &lt;- c(\"Topic 1\", \"Topic 2\")\n\nD_matrix %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nTopic 1\nTopic 2\n\n\n\nTopic 1\n3.993368\n0.000000\n\n\nTopic 2\n0.000000\n3.460071\n\n\n\n\n\nThe \\(V_k\\) matrix represents terms by topics.\n\nterms &lt;- colnames(dtm)\nV_matrix &lt;- tibble(`Term` = terms,\n                   `Topic 1` = Vk[,1],\n                   `Topic 2` = Vk[,2])\n\nV_matrix %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\nTerm\nTopic 1\nTopic 2\n\n\n\nbeautiful\n-0.1091735\n0.4428019\n\n\nblue\n-0.2566540\n0.4272669\n\n\nsky\n-0.1510589\n0.6188605\n\n\nlove\n-0.0392713\n0.2156161\n\n\nbrown\n-0.4190432\n-0.1226506\n\n\ndog\n-0.4190432\n-0.1226506\n\n\nfox\n-0.4190432\n-0.1226506\n\n\njumps\n-0.1401764\n-0.0558921\n\n\nlazy\n-0.4190432\n-0.1226506\n\n\nquick\n-0.4190432\n-0.1226506\n\n\nbacon\n-0.0066085\n0.1469936\n\n\nbeans\n-0.0022126\n0.0737541\n\n\nbreakfast\n-0.0022126\n0.0737541\n\n\neggs\n-0.0066085\n0.1469936\n\n\nham\n-0.0066085\n0.1469936\n\n\nking's\n-0.0022126\n0.0737541\n\n\nsausages\n-0.0066085\n0.1469936\n\n\ntoast\n-0.0022126\n0.0737541\n\n\ngreen\n-0.0043959\n0.0732395\n\n\n\n\n\nNow, we can examine top 5 terms associated with each topic.\n\ntop_terms &lt;- apply(Vk, 2, function(x) terms[order(abs(x), decreasing = TRUE)[1:5]])\nprint(top_terms)\n\n     [,1]    [,2]       \n[1,] \"brown\" \"sky\"      \n[2,] \"dog\"   \"beautiful\"\n[3,] \"fox\"   \"blue\"     \n[4,] \"lazy\"  \"love\"     \n[5,] \"quick\" \"bacon\"    \n\n\nBeyond what has been discussed, some other cool applications of SVD in NLP include: information retrieval via Latent Semantic Analysis and word co-occurrence detection in word embeddings and other downstream tasks (e.g. text classification). Feel free to explore!\nReferences and additional resources:\n\nA wonderful twitter thread on SVD by Daniela Witten (a nice summary can be found here)\nA cool geometric interpretation of SVD\nA nice tutorial illustrating the connection between SVD and topic modeling using Python"
  }
]