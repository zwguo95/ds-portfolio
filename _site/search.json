[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html",
    "href": "dataviz/geofacet-hexbin/index.html",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "",
    "text": "Last week, I introduced how to visualize disparities in tree equity scores using hexbin maps. Hexbin maps are a useful tool for visualizing dense data points by summarizing them in a compact and understandable format. By grouping data points into hexagonal bins, they provide a clear picture of the spatial distribution of the data, enabling the detection of patterns that may be hidden by overplotting.\nAlthough hexagonal bins provide a clean representation of data, they result in information loss regarding the underlying shape of the data. For example, my previous visual using hexbin maps suggests that Ohio has the largest tree equity gap, which is defined as the maximum difference across block-level tree equity scores. However, this does not provide a complete picture. When examining the distribution of tree equity scores across census blocks within each state, it is evident that Ohio has a considerable number of blocks with relatively high levels of tree equity, indicating that Ohio performs well in this regard.\nGeofacet offers a useful alternative to hexbin maps, allowing for the restoration of the original data distributions while preserving the spatial information. By maintaining individual data points, geofacet provides a more detailed representation of data to enable a more nuanced view of spatial relationships.\nIn this blog post, I will walk you through how to visualize tree equity score data using geofacet. All code can be found here."
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html#data-collection",
    "href": "dataviz/geofacet-hexbin/index.html#data-collection",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "Data Collection",
    "text": "Data Collection\nWe follow similar steps of data collection and the only difference is we keep the original variable - tree equity score.\n```{r}\n# prepare a function to read zip urls with shapefiles \nread_shape_URL <- function(URL){\n  cur_tempfile <- tempfile()\n  download.file(url = URL, destfile = cur_tempfile)\n  out_directory <- tempfile()\n  unzip(cur_tempfile, exdir = out_directory)\n  \n  read_sf(dsn = out_directory)\n}\n\n# pull state, tes, priority from each dataframe\ndata_lists <- list()\nfor (i in 1:nrow(states)){\n  state <- states$lower_code[i]\n  print(state)\n  URL <- paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\")\n  if (RCurl::url.exists(URL) == T) {\n    map <- read_shape_URL(paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\"))\n    data_lists[[i]] <- map %>% select(tes, state, priority)\n  }\n}\ndata <- do.call(rbind, data_lists) %>% na.omit()\n```"
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html#data-visualization",
    "href": "dataviz/geofacet-hexbin/index.html#data-visualization",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "Data Visualization",
    "text": "Data Visualization\nThe function facet_geo() plays the magic, which creates the base map of the US that can be integrated with various data representations. By using the argument grid = us_state_grid1[c(-2, -11), ], we can exclude Hawaii and Alaska from the map as there is no data available for these states. If you prefer to have the full state name, you can add the argument label = \"name\".\nAnother thing to keep in mind is that the facet_geo() layer should integrate with a pre-existing data representation. In this example where I am interested in displaying the distribution of tree equity scores, I add geom_density() layer beforehand.\n```{r}\nggplot(data) +\n  geom_density(aes(x = tes), color = \"#466c4b\", fill = \"#7fa074\", alpha = 0.5) +\n  coord_cartesian(clip = \"off\") +\n  facet_geo(vars(state), scales = \"free_y\", grid = us_state_grid1[c(-2, -11), ], label = \"name\") +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  scale_y_continuous(\n    labels = scales::number_format(accuracy = 0.01)) +\n  labs(x = \"\", y = \"\",\n       title = \"ACCESS TO GREEN SPACE\",\n       subtitle = \"Distribution of tree equity scores across census blocks in the US\",\n       caption = str_wrap(\"\n       Tree Equity Score (TES) computes how much tree canopy and surface temperature align with income, \n       employment, race, age and health factors in the US, collected by American Forest | Visualization by Zhaowen Guo\", width = 300)) +\n  theme_void(base_family = \"Pragati Narrow\") +\n  theme(strip.text = element_text(face = \"bold\", color = \"grey20\", size = 30),\n        legend.position = \"none\",\n        axis.text = element_text(color = \"grey40\", size = 30),\n        strip.background = element_blank(),\n        plot.background = element_rect(fill = \"#f5f5f2\", color = NA),\n        plot.margin = margin(40, 15, 20, 15),\n        plot.title = element_text(face = \"bold\", size = 70, margin = margin(l=0, t=5)),\n        plot.subtitle = element_text(lineheight = 1, size = 50, margin(l=0, t=7)),\n        plot.caption = element_text(margin = margin(t=35), color = \"grey20\", size = 30),\n        plot.caption.position = \"plot\")\n\nggsave(\"tree-equity-geofacet.png\", dpi = 320, width = 14, height = 10)\n```"
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html",
    "href": "dataviz/green-space-gunshot-map/index.html",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "",
    "text": "OpenStreetMap (OSM) is an open-source mapping platform that provides detailed maps of the world. It is built and maintained by a global community of volunteers who contribute data, such as roads, buildings, and points of interest, to create a free and open map of the world.\nOSM offers a comprehensive API that enables users to easily access and use the map data seamlessly in their own applications. Previously, I relied on the OpenLayers plugin in QGIS to access OSM data. While QGIS provides a nice graphical user interface for loading and visualizing OSM data, I had to switch between R and QGIS to ensure consistency in the resulting graphs. Here comes the good news! Now we can directly access OSM from R using the osmdata package. By specifying the coordinates or the name of the geographic area of interest, we can easily obtain OSM data and perform analyses in R.\nContinuing on my previous articles about green spaces, in this blog post I will introduce how to map green spaces using OSM. Check out my full code here."
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html#data-collection",
    "href": "dataviz/green-space-gunshot-map/index.html#data-collection",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "Data Collection",
    "text": "Data Collection\nOnce we have installed the osmdata package, we can begin gathering our data. The first step is to define a specific area of interest using coordinates or place names. For example, to download all OSM data for Washington DC, we can try either of the following approaches to specify the bounding box:\n```{r}\n# first approach\nopq(getbb(\"Washington, District of Columbia\"))\n\n# second approach\nopq(bbox = c(-77.10, 38.80, 76.90, 39.00))\n```\nDepending on how we define green spaces, we can use add_osm_feature() to set specific key and value attributes and call osmdata_sf() to convert the output to a simple features (sf) object which will simplify the plotting process later. The same approach can be used to display other map elements, such as streets and rivers. The following code will extract the previously obtained OSM data that matches the defined attribute tags.\n```{r}\n# green spaces defined as parks, nature reserve, and golf course\ngreens <- opq(getbb(\"Washington, District of Columbia\")) %>%                \n  add_osm_feature(key = \"leisure\", \n                  value = c(\"park\", \"nature_reserve\", \"golf_course\")) %>%\n  osmdata_sf()\n  \n# green spaces defined as grass  \ngreens <- opq(getbb(\"Washington, District of Columbia\")) %>%\n  add_osm_feature(key = \"landuse\", \n                  value = \"grass\") %>%\n  osmdata_sf()\n```"
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html#data-visualization",
    "href": "dataviz/green-space-gunshot-map/index.html#data-visualization",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "Data Visualization",
    "text": "Data Visualization\nWith our data now downloaded, we can begin visualizing it using ggplot2. To visualize spatial data, we simply add a layer by geom_sf(). We can include an argument inherit.aes = FALSE to customize each layer, making its aesthetics (i.e. color, size) not inherit from previous layers.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1)\n```\nWe can also set the coordinate reference system (CRS) for the spatial data being plotted using coord_sf(). By default, the CRS is set to WGS 84 (EPSG code 4326), which I used in this example, and You can adjust it as needed. We can also specify the longitude and latitude in the same layer to “zoom in” the area of interest.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1) +\n  coord_sf(crs = st_crs(4326), xlim = c(-77.12, -76.90), ylim = c(38.79, 39.01)) \n```\nWe can also add additional layers to enrich our spatial visualization by displaying other types of geographic data, such as point locations. For example, in this illustration, I incorporated a geom_point() layer highlighting the locations of 2021 gunshot incidents.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1) +\n  geom_point(data = gunshot, aes(x = LONGITUDE, y = LATITUDE), color = \"#c62320\", size = 0.1, inherit.aes = FALSE) +\n  coord_sf(crs = st_crs(4326), xlim = c(-77.12, -76.90), ylim = c(38.79, 39.01))\n```\nJust with a few additional aesthetic touches, I was able to create a plot that effectively visualizes both green spaces in the DC area and the recorded gunshot incidents that occurred in 2021."
  },
  {
    "objectID": "dataviz/tree-equity/index.html",
    "href": "dataviz/tree-equity/index.html",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "",
    "text": "Urban green spaces, such as parks, gardens, and forests, provide numerous benefits in creating livable cities. They help improve air and water quality, reduce heat islands, and increase physical activity and mental health. However, not all communities have equal access to these benefits, leading to the concept of “tree equity.”\nTree equity refers to the fair distribution of urban green spaces and trees, regardless of a community’s socio-economic status, race, or ethnicity. American Forests, a non-profit organization dedicated to protecting and restoring forests, has been working to measure and improve tree equity across the United States.\nIn this blog post, I will show you how to visualize tree equity score data to tell a compelling story. All code can be found here."
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-collection",
    "href": "dataviz/tree-equity/index.html#data-collection",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Collection",
    "text": "Data Collection\nTree equity score data are currently hosted on this webpage, where users are required to manually download zip files containing geospatial data for each state. Furthermore, each shapefile includes not only tree equity scores, but also several other variables. Thus, we need to find an efficient method to automate the process of downloading files with minimal manual intervention and extract the desired variable of interest - the tree equity score - from the shapefiles.\nUpon checking the web addresses of several files, a pattern has become apparent: each URL shares a similar structure in its file name, while also possessing a unique postal code. This discovery suggests that it is possible to create a script that can automatically scrape the files. Fortunately, unzip() in base R and sf package offer a convenient solution for unzipping files and reading shapefiles, and we can easily write a function to automate this process.\n```{r}\nlibrary(sf)\nlibrary(tidyverse)\n\nread_shape_URL <- function(URL){\n  cur_tempfile <- tempfile()\n  download.file(url = URL, destfile = cur_tempfile)\n  out_directory <- tempfile()\n  unzip(cur_tempfile, exdir = out_directory)\n  \n  read_sf(dsn = out_directory)\n}\n```\nThe question that I am interested in is: which state has the greatest disparity in tree equity score across counties, as measured by the difference between the maximum and minimum tree equity scores within each state. Also, it is important to note that data availability may vary as not all states have available tree equity score data. To verify the existence of a URL, we can use Rcurl package and write a for-loop to produce the key variable of interest, the tree equity gap.\n```{r}\nstates <- read.csv(\"state-names.csv\") # downloaded https://worldpopulationreview.com/states/state-abbreviations\n\nstate_names <- rep(NA, 51)\ntes_gaps <- rep(NA, 51)\nfor (i in 1:nrow(states)){\n  state <- states$lower_code[i]\n  state_names[i] <- state\n  print(state)\n  URL <- paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\")\n  if (RCurl::url.exists(URL) == T) {\n    map <- read_shape_URL(paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\"))\n    tes_gap <- max(map$tes) - min(map$tes)\n    tes_gaps[i] <- tes_gap\n  } else {\n    tes_gaps[i] <- NA\n  }\n}\n\ndata <- data.frame(lower_code = state_names,\n                   gap = tes_gaps) %>%\n  cbind(states[1])\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-cleaning",
    "href": "dataviz/tree-equity/index.html#data-cleaning",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nGreat, we now have our data! The next step is to clean and prepare it for visualization. With the consideration of working with spatial data across multiple regions and the fact that the tree equity gap is a continuous variable, hexbin maps become a desirable choice. Hexbin maps divide the map area into small hexagonal bins and consolidate the data points within them, presenting a clear and compact depiction of the vast amount of data.\n```{r}\nlibrary(geojsonio)\nlibrary(rgeos)\n\n# create a base hexbin map of US\nhex_states <- geojson_read(\"us_states_hexgrid.geojson\", what = \"sp\") \n\n# extract state names\nhex_states@data <- hex_states@data %>%\n  mutate(google_name = str_replace(google_name, \" \\\\(United States\\\\)\", \"\"))\n\n# create a data frame for hexbin map\nhex_states_fortify <- broom::tidy(hex_states, region = \"google_name\")\n\n# match state names\ndata_map <- hex_states_fortify %>%\n  right_join(data, by = c(\"id\" = \"state\")) %>%\n  mutate(id = state.abb[match(id, state.name)])\ndata_map$id[data_map$group == \"District of Columbia.1\"] <- \"DC\"\n\nlabels <- cbind.data.frame(data.frame(gCentroid(hex_states, byid = T),\n                                      id = hex_states@data$iso3166_2))\ndata_map <- data_map %>%\n  right_join(labels, by = \"id\") %>%\n  filter(is.na(gap) == F)\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-visualization",
    "href": "dataviz/tree-equity/index.html#data-visualization",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Visualization",
    "text": "Data Visualization\nWe now have our spatial polygon data frame ready and can visualize it! To make it visually appealing, I pick a custom Google font “Pragati Narrow” for the graph and a stunning color palette from the scico package. One trick is to adjust text colors that contrast with the background color. For instance, in the case of Washington D.C., which has the narrowest gap in tree equity scores, the bin’s background can be made lighter. However, if white text is still used as in other areas, the label may not be easily visible. To mitigate this, we can establish a threshold for switching the text color as necessary.\n```{r}\nlibrary(scico)\nlibrary(showtext)\ntheme_set(theme_minimal(base_family = \"Pragati Narrow\"))\n\ntheme_update(\n  # legend\n  legend.title = element_blank(),\n  legend.position = 'top',\n  legend.direction = 'horizontal',\n  legend.key.width = unit(1.5, \"cm\"),\n  legend.text = element_text(color = \"black\",  size=30),\n  \n  # axis\n  axis.text.x = element_blank(),\n  axis.text.y = element_blank(),\n  \n  # titles\n  panel.grid = element_blank(),\n  plot.margin = margin(15, 30, 15, 30),\n  plot.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  panel.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  legend.title.align=1,\n  plot.title = element_text(\n    color = \"black\", \n    size = 70, \n    face = \"bold\",\n    margin = margin(t = 15),\n    hjust = 0.5\n  ),\n  plot.subtitle = element_text(\n    color = \"grey10\", \n    size = 45,\n    lineheight = 3,\n    margin = margin(t = 5),\n    hjust = 0.5\n  ),\n  plot.title.position = \"plot\",\n  plot.caption.position = \"plot\",\n  plot.caption = element_text(\n    color = \"grey20\", \n    size = 20,\n    lineheight = 0.5, \n    hjust = 0.5,\n    margin = margin(t = 40))\n)\n\ndata_map %>% \n  ggplot () +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = gap), linewidth = 0.5) +\n  scale_fill_scico(palette = \"lajolla\", direction = 1) + \n  geom_text(aes(x=x, y=y, label=id, color = gap < 60), size = 8, alpha = 0.5, \n             show.legend = F) +\n  scale_color_manual(values = c(\"white\", \"black\")) +\n  coord_map(clip = \"off\") +\n  labs(title = \"TREE EQUITY GAP\",\n       subtitle = \"Block-level disparities in tree equity scores within each state\",\n       x = \"\", y = \"\",\n       caption= \n         str_wrap(\n       \"Data comes from the Green Space Data Challenge, \n       collected and shared by the American Forests. \n       Tree Equity Score (TES) computes how much tree canopy and surface temperature align with income, \n       employment, race, age and health factors in the U.S | Visualization by Zhaowen Guo\", width=150))\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#implications",
    "href": "dataviz/tree-equity/index.html#implications",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Implications",
    "text": "Implications\nWhat do we learn from this visualization? The graph clearly illustrates the unequal distribution of green spaces, particularly in states like Ohio and Minnesota, where the gap is much more pronounced. This calls for prompt and effective action to rectify this imbalance."
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html",
    "href": "dataviz/tree-gunshot/index.html",
    "title": "The Calendar of Gun Violence",
    "section": "",
    "text": "Despite being a small city, Washington, D.C. has the highest homicide rate among all U.S. states, with 226 deaths and 1330 emergency department visits due to gunshot wounds in 2021. As part of its efforts to combat gun violence, the city has adopted innovative strategies and technologies, including ShotSpotter which is a gunshot detection system that uses acoustic sensors to identify and locate gunfire in real-time.\nIn this blog post, we will examine 2021 gunshot data in D.C. and explore how data visualization can provide deeper insights into gun violence in the city. All code can be found here."
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html#data-cleaning",
    "href": "dataviz/tree-gunshot/index.html#data-cleaning",
    "title": "The Calendar of Gun Violence",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nSuppose that we are interested in the temporal patterns of gun violence in D.C., one common way to visualize time series data is through line charts. These charts can break down time points into intervals, such as months, to observe how the values of gun violence incidents change over time. While this approach offers valuable insight into high-level temporal trends, it has one limitation: it only allows us to focus on one time interval at a time. For example, we can only see the temporal changes over months OR weeks, but not both intervals together.\nInspired by GitHub’s contribution graph, we can use geom_tile() to create a similar calendar graph that effectively visualizes gun violence incidents. To situate a calendar within a data frame, we can observe clear parallels: the week of the month corresponds to the row number, the day of the week represents the column number, and each month is treated as a separate facet.\nOne challenge here is to figure out the week for each month. Unfortunately, week() in the lubridate package only returns the week for the year, requiring us to create a week incrementer to manually calculate the week for each month. In other words, we increment the week counter by 1 when we encounter a “Sunday” or when the day is the first of the month.\nMoving forward, we can consider whether to treat gunshot incidents as a continuous variable. Upon plotting the data distribution, it becomes apparent that the distribution is highly right-skewed, which means treating it as continuous would not allow for clear differentiation of color in the legend. Therefore, I categorize gunshot incidents to represent low, medium, and high levels of gun violence.\n```{r}\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(tidyverse)\ngunshot <- read_csv(\"Shot_Spotter_Gun_Shots.csv\")\n\n# data cleaning\ngunshot_daily <- gunshot %>%\n  mutate(date = as_date(DATETIME),\n         year = year(date)) %>%\n  filter((year == 2021) & (TYPE %in% c(\"Single_Gunshot\", \"Multiple_Gunshots\", \"Multiple Gunshots\", \"Single Gunshot\"))) %>%\n  group_by(date) %>%\n  summarise(shots = n()) %>%\n  ungroup() %>%\n  mutate(week_day = str_sub(weekdays(date), 1, 3),\n         month_day = day(date),\n         month = month(date),\n         week_start = ifelse(month_day == 1 | week_day == \"Sun\", 1, 0)) %>% # set up when to increment the week\n  group_by(month) %>%\n  mutate(week = cumsum(week_start),\n         month_name = months(date)) %>%\n  ungroup() %>%\n  mutate(shots_range = case_when(shots <= 10 ~ \"1\",\n                                 shots > 10 & shots <= 20 ~ \"2\",\n                                 shots > 20 ~ \"3\"))\n\nweek_day_code <- c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\ngunshot_daily$week_day <- factor(gunshot_daily$week_day, levels = week_day_code)\nmonth_code <- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\") \ngunshot_daily$month_name <- factor(gunshot_daily$month_name, levels = month_code)\n```"
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html#data-visualization",
    "href": "dataviz/tree-gunshot/index.html#data-visualization",
    "title": "The Calendar of Gun Violence",
    "section": "Data Visualization",
    "text": "Data Visualization\nNow, let’s visualize the data! Interestingly, the part I spent most time upon was making the month names appear above the weekday names, as they were initially positioned the opposite way. To resolve this issue, we have the option to adjust either the strips (weekday names) or the axes (month names). I decided to go for the former route by increasing the bottom parameter with margin(b=25), which enabled the weekday names to move upwards until they were placed above the month names.\n```{r}\n# theme\nfont_add_google(\"Pragati Narrow\")\nshowtext_auto()\n\n\n# customize theme\ntheme_set(theme_minimal(base_family = \"Pragati Narrow\"))\n\ntheme_update(\n  # legend\n  legend.title = element_blank(),\n  legend.position = 'bottom',\n  legend.direction = 'horizontal',\n  legend.key.width = unit(1.5, \"cm\"),\n  legend.text = element_text(color = \"black\",  size=35),\n  legend.box.margin = margin(t = 35),\n  legend.spacing.x = unit(1, \"cm\"),\n  legend.spacing.y = unit(0.5, \"cm\"),\n  \n  # axis\n  axis.text.y = element_blank(),\n  axis.text.x = element_text(vjust = 50),\n  text = element_text(size = 40),\n  strip.text.x = element_text(size = 43, margin = margin(b = 25)),\n  \n  # titles\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  plot.margin = margin(20, 50, 20, 50),\n  plot.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  panel.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  \n  #legend.title.align=1,\n  plot.title = element_text(\n    color = \"black\", \n    size = 70, \n    face = \"bold\",\n    margin = margin(t = 10),\n    hjust = 0.5\n  ),\n  plot.subtitle = element_text(\n    color = \"grey10\", \n    size = 45,\n    lineheight = 3,\n    margin = margin(t = 5, b = 30),\n    hjust = 0.5\n  ),\n  plot.title.position = \"plot\",\n  plot.caption.position = \"plot\",\n  plot.caption = element_text(\n    color = \"grey20\", \n    size = 35,\n    lineheight = 0.5, \n    hjust = 0.5,\n    margin = margin(t = 30))\n)\n\n\ngunshot_daily %>%\n  ggplot(aes(x = week_day, y = week)) +\n  geom_tile(aes(fill = shots_range), color = \"white\") + \n  scale_fill_manual(values = MetBrewer::met.brewer(\"Tam\", n=3),\n                    labels = c(\"below 10\", \"10 to 20\", \"over 20\"),\n                    guide = guide_legend(label.position = \"bottom\", nrow = 1)) +\n  facet_wrap(~month_name, scales = \"free\") +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"bottom\") +\n  labs(x = \"\", y = \"\", title = \"GUNSHOT DETECTION CALENDAR\",\n       subtitle = \"Recorded shooting incidents in Washington D.C. during 2021\",\n       caption = str_wrap(\"Data comes from ShotSpotter gunshot detection system. Incidents of probable gunfires and firecrackers are excluded | Visualization by Zhaowen Guo\", width = 300))\n\nggsave(\"dc-gunshot-time.png\", width = 14, height = 14/1.618, units = \"in\")\n```"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Mapping Green Spaces with OpenStreetMap in R\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nThe Calendar of Gun Violence\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nRevisiting Tree Equity Gap: Hexbin or Geofacet?\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nGreen Space for Everyone? Visualizing Tree Equity Gap\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\ngreenspace\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhaowen Guo",
    "section": "",
    "text": "I am a PhD candidate at the University of Washington, where I combine my expertise in both social and data science to develop and enhance metrics using innovative data collection and psychometric techniques.\nI enjoy connecting data from various sources, including administrative data, survey data, text data, image data, and spatial data, to deliver insightful analyses and support impact evaluations. My passion lies in communicating my findings through engaging data visualizations and promoting equity and inclusion in data science by knowledge sharing."
  },
  {
    "objectID": "statistics/rasch-hlm/index.html",
    "href": "statistics/rasch-hlm/index.html",
    "title": "Unveilling Model Equivalence: Linking the Rasch Model with the Hierarchical Linear Model",
    "section": "",
    "text": "You can find my slides here."
  },
  {
    "objectID": "statistics/simulations/index.html",
    "href": "statistics/simulations/index.html",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "",
    "text": "Named after the host of the popular game show “Let’s Make a Deal,” the Monty Hall problem is a classic example of how our intuition can sometimes lead us astray when it comes to probabilities.\nThe problem goes like this: You’re a contestant on “Let’s Make a Deal” and are asked to choose one of three doors. Behind one of the doors is a brand new car, while behind the other two are goats. After you make your choice, Monty Hall opens one of the other two doors to reveal a goat. He then gives you the option to (1) stick with your original choice or (2) switch to the other unopened door.\nWhat should you do? Should you stick with your original choice or switch to the other door?\nMany people’s intuition tells them that it doesn’t matter whether they stick with their original choice or switch and the probability of winning the car is 1/3 no matter what.\nIs this correct? In this blog post, I will introduce various methods to solve this puzzle, including probability theory, causal diagrams, and simulations. All code can be accessed here."
  },
  {
    "objectID": "statistics/simulations/index.html#probability-theory",
    "href": "statistics/simulations/index.html#probability-theory",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Probability Theory",
    "text": "Probability Theory\nOne straightforward approach is to apply the Bayes’ theorem to calculate and compare the probability of winning between switching and not switching. To simplify the analysis, we can define two events: event A is when the car is behind the remaining door, and event B is when Monty chooses a door with a goat. Our objective is to determine the probability that the car is behind the remaining door given Monty’s exposure door. If the probability is high, then switching would be a better choice, and vice versa.\nAssuming our initial choice is Door 1 and Monty opens Door 2, we can use Bayes’ theorem to calculate the conditional probability of the car being behind Door 3 given that Monty opened Door 2. The calculation is as follows:\n\\[\nP(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n\\]\nNow, let’s figure out what each part represents. \\(P(B|A)\\) is 1 because Monty should never reveal a car by design and our choice of Door 1 rules out another option. We also have \\(P(A) = 1/3\\), since a car is equally likely to be assigned to one of the three doors. The marginal probability \\(P(B)\\) that Monty opens Door 2 without conditioning on what the remaining door contains is 1/2, as our initial choice leaves him two options to pick.\nTaken together, we have the probability of winning for switching is \\(P(A|B) = \\frac {1 \\times 1/3}{1/2} = 2/3\\) and the probability of wining for sticking to the initial choice is \\(1 - P(A|B) = 1/3\\). So yes, the correct answer is we should always switch!"
  },
  {
    "objectID": "statistics/simulations/index.html#causal-diagrams",
    "href": "statistics/simulations/index.html#causal-diagrams",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Causal Diagrams",
    "text": "Causal Diagrams\nJudea Pearl’s The Book of Way sheds a new light on this classic probability puzzle. The key is go beyond the data itself and think through how the data was generated. Let’s start with the question: what determines which door will be opened by Hall?\nOur choice of door excludes one option he could open. Knowing that which door that does have a goat behind it means that he has to choose a different one as well if possible. In this case, we end up with the following causal diagram where door to be opened is a collider.\nWhat will happen if we condition on a collider? In other words, what if we take action based on Monty’s choice of door? In causal graph terms, conditioning on a collider creates an association between previously independent variables (causal diagrams shifting from the left to the right as shown below), in this case, our choice of door and the location of the car. Let’s still assume that our initial choice is Door 1 and Monty opens Door 2. Translating this causal diagram into probabilities, the probability of switching (the car being behind Door 3) conditional on Monty’s choice becomes 2/3 because now the car should be behind one of the remaining two doors, which is greater than 1/3.\nMore intuitively, the reason why the probability of a car behind Door 1 changes from 1/3 to 2/3 when we condition on Monty’s choice of door is that his choice is not random: he has to pick a door without a car. Monty could not open Door 1 once we chose it – but he could have opened Door 3. The fact that he did not implies that he was forced to and thus there is more evidence than before that the car is behind Door 3."
  },
  {
    "objectID": "statistics/simulations/index.html#simulations",
    "href": "statistics/simulations/index.html#simulations",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Simulations",
    "text": "Simulations\nAs you may have noticed, solving the Monty Hall problem requires some effort to overcome our intuition. However, there is a more intuitive way to approach this problem: simulations. Using simulations, we can better understand the probabilities at play and verify the counter-intuitive result of the problem.\nThe basic idea is still to compute and compare the winning probability of switching and not switching, but now let’s play this game 10000 times and compare the relative frequency of winning.\nUsing exactly the same setup where our initial choice is Door 1, the probability of not switching after 10000 iterations of the game is 0.33.\n```{r}\n# The winning probability of sticking to the initial choice\nset.seed(123)\ndoors <- c(1,2,3)\n\ncount <- 0\nfor(i in 1:10000) {\n  car <- sample(doors, 1)\n  initial_choice <- 1\n  if(initial_choice == car){\n    count <- count + 1\n  }\n}\n\np_stick <- count/10000\n```\nA slightly tricky part is specifying which door Monty will reveal. Following the same logic, Monty will choose a door different from both our initial choice and the door with the car, we can write a simple function as below and compute the relative frequency of winning when we switch. The probability is 0.67, twice as larger as 0.33.\n```{r}\n# The winning probability of switching to another choice \nset.seed(123)\nreveal <- function(doors, car, initial_choice) {\n  if(car == initial_choice){\n    reveal <- sample(doors[-c(car,initial_choice)], 1)\n  } else {\n    reveal <- doors[-c(car, initial_choice)]\n  }\n}\n\ncount <- 0\nfor (i in 1:10000) {\n  car <- sample(doors,1)\n  initial_choice <- 1\n  revealed_door <- reveal(doors, car, initial_choice)\n  final_choice <- doors[-c(initial_choice, revealed_door)]\n  if(final_choice == car){\n    count = count + 1\n  }\n}\n\np_switch <- count/10000\n```\nI hope this post has highlighted the usefulness of simulations as a tool to gain a more intuitive understanding of probability concepts. By running simulations of the Monty Hall problem multiple times, we can observe how the probability of winning changes when we switch doors, which can be helpful in building a deeper understanding of the problem. Additionally, simulations can also serve as a good way to verify counter-intuitive results and build confidence in other probability-based solutions."
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Revisiting the Monty Hall Problem: The Power of Simulations\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Household Groupings: A Longitudinal Approach\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nUnveilling Model Equivalence: Linking the Rasch Model with the Hierarchical Linear Model\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nZhaowen Guo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html",
    "href": "statistics/dssg-tracking-poverty/index.html",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "",
    "text": "Poverty is a complex issue that is typically measured at the household level, taking into account the resources that are shared among multiple individuals. However, many existing studies rely solely on individual-level survey data to estimate poverty rates. This approach can lead to inaccurate estimates, particularly in cases where individuals within a household have differing levels of access to resources.\nThe use of administrative data is a promising approach to measuring poverty, as it provides a wealth of information on individual addresses. By aggregating information across individuals who are more likely to belong to the same household, we can develop more accurate estimates of poverty and better understand the ways in which poverty affects families and communities.\nIn our 2022 Data Science for Social Good (DSSG) project, we used the Washington Merged Longitudinal Administrative Dataset (WMLAD) to predict household groupings in the Washington State. This database merges administrative data from various state agencies including the Employment Security Department (ESD), Department of Social and Health Services (DSHS), Department of Health (DOH), Secretary of State, Department of Licensing (DOL), and WA State Patrol, covering the period from 2010 to 2017. Each time an individual interacts with any of these agencies, they are assigned an anonymized address code, which is further augmented by an imputation algorithm developed by previous WMLAD users (more details here).\nIn this blog post, I will introduce a longitudinal approach that I proposed for our team to improve the imputed addresses and construct more reliable household groupings. Don’t forget to also check out our presentation and media coverage :)"
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-1-lack-of-interactions",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-1-lack-of-interactions",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 1: Lack of interactions",
    "text": "Scenario 1: Lack of interactions\nThis occurs when certain household members, especially children, have less interactions with government agencies than adults, resulting in fewer recorded addresses for imputation. For example, addresses for children may only be recorded when they register to vote or obtain a driver’s license, which implies that the observation of some one-person residences may simply be due to the absence of their children in previous records.\nTo address this circumstance, I recommended adjusting certain one-person residences to reflect their actual co-residence with children. For example, in the scenario illustrated below, a child (purple face) began to co-reside with others (blue face) in month 4, while the address records showing only one person living at that address previously. In such cases, we need to reflect their co-residence accurately."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-2-imputation-error",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-2-imputation-error",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 2: Imputation error",
    "text": "Scenario 2: Imputation error\nAnother pattern that drew my attention was frequent movements into and out of the same place, which may indicate an imputation error.\nA plausible fix was in cases where we observe a person living alone and co-residing with others alternately at the same address, we could adjust the one-person residences to reflect the nearest co-residence that the individual belongs to."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-3-move-in-and-out",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-3-move-in-and-out",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 3: Move in and out",
    "text": "Scenario 3: Move in and out\nThe third scenario is more complex. One-person residences may occur during a transition period when a multi-person residence moves out simultaneously to a new location, but their addresses are not updated accordingly.\nIn cases where we observe distinct co-residence members before and after a one-person residence at a particular address, and the prior co-residence appears again in the future, we adjust the one-person residence to reflect the subsequent co-residence. As depicted in the illustration below, we should only modify one-person residences in month 3 when the co-residence composition in month 2 and month 4 matches.\n\n\nModifying one-person residences that fall under these scenarios did not conclude the analysis, as it could result in duplicates where a person appears at multiple addresses during the same time period. Therefore, we should eliminate duplicates based on recorded addresses, rather than imputed ones, which provide a more reliable indication of the address that the person belongs to.\nHow effective is this algorithm? After implementing the modifications, the percentage of one-person residences decreased from 38% to 31%, bringing it much closer to the census record of 27%. We are currently developing a methodology paper that details this approach along with more real-world data applications. We anticipate that our methodology and resulting household groupings will be beneficial in addressing significant policy questions."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-lack-of-interactions",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-lack-of-interactions",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "1.1 Scenario: Lack of interactions",
    "text": "1.1 Scenario: Lack of interactions\nThis occurs when certain household members, especially children, have less interactions with government agencies than adults, resulting in fewer recorded addresses for imputation. For example, addresses for children may only be recorded when they register to vote or obtain a driver’s license, which implies that the observation of some one-person residences may simply be due to the absence of their children in previous records.\nTo address this circumstance, I recommended adjusting certain one-person residences to reflect their actual co-residence with children. For example, in the scenario illustrated below, a child (purple face) began to co-reside with others (blue face) in month 4, while the address records showing only one person living at that address previously. In such cases, we need to reflect their co-residence accurately."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-imputation-error",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-imputation-error",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "1.2 Scenario: Imputation error",
    "text": "1.2 Scenario: Imputation error\nAnother pattern that drew my attention was frequent movements into and out of the same place, which may indicate an imputation error.\nA plausible fix was in cases where we observe a person living alone and co-residing with others alternately at the same address, we could adjust the one-person residences to reflect the nearest co-residence that the individual belongs to."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-move-in-and-out",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-move-in-and-out",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "1.3 Scenario: Move in and out",
    "text": "1.3 Scenario: Move in and out\nThe third scenario is more complex. One-person residences may occur during a transition period when a multi-person residence moves out simultaneously to a new location, but their addresses are not updated accordingly.\nIn cases where we observe distinct co-residence members before and after a one-person residence at a particular address, and the prior co-residence appears again in the future, we adjust the one-person residence to reflect the subsequent co-residence. As depicted in the illustration below, we should only modify one-person residences in month 3 when the co-residence composition in month 2 and month 4 matches.\n\n\nModifying one-person residences that fall under these scenarios did not conclude the analysis, as it could result in duplicates where a person appears at multiple addresses during the same time period. Therefore, we should eliminate duplicates based on recorded addresses, rather than imputed ones, which provide a more reliable indication of the address that the person belongs to.\nHow effective is this algorithm? After implementing the modifications, the percentage of one-person residences decreased from 38% to 31%, bringing it much closer to the census record of 27%. We are currently developing a methodology paper that details this approach along with more real-world data applications. We anticipate that our methodology and resulting household groupings will be beneficial in addressing significant policy questions."
  }
]