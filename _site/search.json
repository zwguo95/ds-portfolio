[
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWeights in Statistics: What Do People Often Get Wrong?\n\n\n\ncode\n\n\nsurvey methodology\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nApr 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHype or Harm: Examining Media Coverage of AI\n\n\n\nmachine learning\n\n\nnatural language processing\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nApr 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScraping Dynamic Websites with R: An Example from Media Bias Data\n\n\n\ncode\n\n\nnatural language processing\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Power of Singular Vector Decomposition: A Beginner’s Guide\n\n\n\ncode\n\n\nmachine learning\n\n\nnatural language processing\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nMar 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevisiting the Monty Hall Problem: The Power of Simulations\n\n\n\ncode\n\n\nstatistics\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining the Bias-Variance Tradeoff\n\n\n\nstatistics\n\n\nmachine learning\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Household Groupings: A Longitudinal Approach\n\n\n\nstatistics\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnveilling Model Equivalence: Linking the Rasch Model with the Hierarchical Linear Model\n\n\n\ncode\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/web-scraping/index.html",
    "href": "statistics/web-scraping/index.html",
    "title": "Scraping Dynamic Websites with R: An Example from Media Bias Data",
    "section": "",
    "text": "In my recent analysis of how news headlines on artificial intelligence (AI) vary across media outlets with different ideological leanings, I stumbled upon an interesting data source provided by the AllSides organization, which contains over 1,000 human-curated ratings of media outlets’ ideological leanings from left to right.\nSeveral studies (e.g. Rozado et al. 2022, Yi et al. 2023) have relied on its annual Media Bias Chart, as shown below, to capture a slice of the full spectrum of media perspectives. It displays around 60 exemplary media outlets for each category, enabling users to easily reference and document this ideological landscape with minimal effort.\n\n\n\n\nHowever, a deeper dive into how media covers AI topics requires a more exhaustive list of media ratings beyond this snapshot. This task confronts several challenges. The webpage’s dynamic nature, which prompts users to click the See all 1400+ Media Bias Ratings button to load additional content, introduces complexity to data collection. The absence of clear markers for the total number of pages, entries, or an endpoint further complicates this task.\nIn this blog post, I will share my experience scraping this dynamic webpage using R, with two distinct approaches."
  },
  {
    "objectID": "statistics/web-scraping/index.html#introduction",
    "href": "statistics/web-scraping/index.html#introduction",
    "title": "Scraping Dynamic Websites with R: An Example from Media Bias Data",
    "section": "",
    "text": "In my recent analysis of how news headlines on artificial intelligence (AI) vary across media outlets with different ideological leanings, I stumbled upon an interesting data source provided by the AllSides organization, which contains over 1,000 human-curated ratings of media outlets’ ideological leanings from left to right.\nSeveral studies (e.g. Rozado et al. 2022, Yi et al. 2023) have relied on its annual Media Bias Chart, as shown below, to capture a slice of the full spectrum of media perspectives. It displays around 60 exemplary media outlets for each category, enabling users to easily reference and document this ideological landscape with minimal effort.\n\n\n\n\nHowever, a deeper dive into how media covers AI topics requires a more exhaustive list of media ratings beyond this snapshot. This task confronts several challenges. The webpage’s dynamic nature, which prompts users to click the See all 1400+ Media Bias Ratings button to load additional content, introduces complexity to data collection. The absence of clear markers for the total number of pages, entries, or an endpoint further complicates this task.\nIn this blog post, I will share my experience scraping this dynamic webpage using R, with two distinct approaches."
  },
  {
    "objectID": "statistics/web-scraping/index.html#pattern-observation-and-looping",
    "href": "statistics/web-scraping/index.html#pattern-observation-and-looping",
    "title": "Scraping Dynamic Websites with R: An Example from Media Bias Data",
    "section": "Pattern Observation and Looping",
    "text": "Pattern Observation and Looping\nThe first approach involves observing the URL structure or pagination pattern of a website and looping through these patterns to scrape data. It’s particularly effective for websites with a predictable and consistent structure, such as incrementing IDs or query parameters in URLs that lead to different pages of content.\nMy workflow includes the following steps:\n\nInspect the website: We can right-click and select Inspect on a webpage, which allows us to access the webpage’s structure and its network activities.\nInteract with the website and observe changes: By engaging with the website, such as clicking a button or scrolling down to load more content, we can observe how the website dynamically fetches additional data.\nMonitor network activity: Under the Network and the Fetch/XHR tabs, we can monitor asynchronous requests made by the webpage after the initial page load, which is particularly crucial for scraping dynamic websites where content is not available in the initial HTML.\nIdentify patterns: We can then examine the Name column (or the request URLs) for patterns, especially those indicating page changes or content loading mechanisms.\n\nThe screenshot below shows the network activities I observed after inspecting the webpage. By navigating to the Network and Fetch/XHR tabs, I monitored the network requests that occurred when interacting with the website. My interactions involved scrolling down to the bottom of the page and clicking a button to load more content. During this process, I identified recurring URL patterns that indicate page changes (e.g. page=1,2,3…), highlighted in the red box. These patterns are key to extracting content by programmatically looping through the pages.\n\n\n\n\nI recorded these URL patterns below with the page number being the parameter.\n\nbase_url &lt;- \"https://www.allsides.com/media-bias/ratings?page=\"\nparams &lt;- \"&field_featured_bias_rating_value=All&field_news_source_type_tid%5B0%5D=2&field_news_bias_nid_1%5B1%5D=1&field_news_bias_nid_1%5B2%5D=2&field_news_bias_nid_1%5B3%5D=3&title=\"\npage_num &lt;- 0\nhas_content &lt;- TRUE\n\nKnowing how to automatically load more pages, we can then switch to extracting the specific content of interest. In this case, I am interested in media outlets and their corresponding ideological leanings. To do this, I hover over the desired content, right-click, and choose Inspect to locate it under the Elements tab.\nThe rvest package in R provides several useful functions to extract information after parsing HTML content. html_elements() is used to select elements based on their attributes, classes, IDs, and so on. html_attr() can extract the value of a specific attribute from an HTML element, which is useful for getting data held in attributes like “href” (links), “src” (images), or others.\nFor instance, this is what I observed upon inspecting the element related to media leanings.\n\n\n\n\nI identified its parent class .views-field-field-bias-image a img and its attribute alt. The following code snippet demonstrates how to extract names and leanings of media outlets based on these identified elements.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\n# Load the webpage content\npage &lt;- read_html(current_url)\n\n# Extract names of media outlets\nnews_source &lt;- page %&gt;%\n    html_elements(\".view-content .views-field-title a\") %&gt;%\n    html_text()\n\n# Extract leanings of media outlets  \nmedia_rating &lt;- page %&gt;%\n    html_elements(\".views-field-field-bias-image a img\") %&gt;%\n    html_attr(\"alt\")\n\nOnce this is done, the final step is just to construct a stopping rule for this scraper when no more content is available. This can be done using a while-loop plus an if-else check. Here’s the pseudo-code:\n\nwhile(has_content){\n  page &lt;- ...\n  \n  news_source &lt;- ...\n  \n  media_rating &lt;- ...\n  \n  # Check if the page has content\n  if (length(news_source) == 0) {\n    has_content &lt;- FALSE\n    print(\"No more content.\")\n  } else {\n    news_sources[[page_num + 1]] &lt;- news_source\n    media_ratings[[page_num + 1]] &lt;- media_rating\n    \n    print(paste(\"Page\", page_num, \"scraped successfully.\"))\n    page_num &lt;- page_num + 1\n  }\n}\n\nThe whole process is extremely fast (I got 1609 entries in ~20s!), and it’s also straightforward to implement this approach once we identify page loading patterns and locations of relevant HTML elements. The complete code can be found here."
  },
  {
    "objectID": "statistics/web-scraping/index.html#automation-tools-like-rselenium",
    "href": "statistics/web-scraping/index.html#automation-tools-like-rselenium",
    "title": "Scraping Dynamic Websites with R: An Example from Media Bias Data",
    "section": "Automation Tools Like RSelenium",
    "text": "Automation Tools Like RSelenium\nAn alternative approach is to use automation tools like RSelenium, which facilitates the automation of web browsers to mimic human interactions on websites, such as logging in, clicking buttons, and so on. This is my first time playing with this tool, and I found it more flexible compared to the former approach, especially when page loading patterns are not evident, and it typically does not require in-depth HTML inspection. However, a notable downside is the complexity of its setup, and it also tends to be slower and more resource-intensive as it involves launching and controlling a web browser session.\nThe process includes the following steps:\n\nNavigate to the webpage: We need to launch a web browser and direct it to the desired webpage.\nInteract with the webpage to load more content: We can program the browser to mimic user actions, such as scrolling through pages and clicking buttons, to ensure all relevant content is loaded.\nExtract the desired elements: Upon fully loading the pages, we can retrieve elements of interest from the webpage\n\nWhen setting up RSelenium, I found it helpful to (1) place the web browser driver (I used chromedriver.exe) in the same folder as the script, which makes it easier for R to locate and initiate the web browser; and (2) set chromever = NULL, which enables automatically detecting the appropriate version of the web driver installed.\n\nThe following code initiates a Chrome web browser session, navigate to the webpage of interest, and click a button to load more content. remDr (remote driver object) is used to interact with the web browser - to identify the button from the CSS selector and simulate a click action. As before, we can inspect the button and find its class, as depicted in the screenshot above.\n\nlibrary(RSelenium)\n\n# Start a web browser\nrD &lt;- rsDriver(browser = \"chrome\", port = 4544L, chromever = NULL)\nremDr &lt;- rD[[\"client\"]]\n\n# Navigate to the webpage\nremDr$navigate(\"https://www.allsides.com/media-bias/ratings\")\n\n# Function to attempt clicking a \"Load More\" button\nattemptLoadMore &lt;- function() {\n  tryCatch({\n    button &lt;- remDr$findElement(using = 'css selector', value = '.changeFilter.btn.btn-large.btn-success, .load-more-button-selector') # Combine selectors if possible\n    button$clickElement()\n    Sys.sleep(2) # Wait for content to load\n    TRUE\n  }, error = function(e) { FALSE })\n}\n\n# Initial click to load more content\nattemptLoadMore()\n\nThe next interaction we need to mimic is to scroll down the webpage and click the button until no new content is loaded. How can we determine when to stop? One way to do this is to record the current scrollable height of the webpage body and continue the clicking behavior until the height does not change, as presented below.\n\n# Scroll and attempt to load more until no new content loads\nrepeat {\n  last_height &lt;- remDr$executeScript(\"return document.body.scrollHeight;\") \n  remDr$executeScript(\"window.scrollTo(0, document.body.scrollHeight);\") # Scroll to the bottom of the page\n  Sys.sleep(3)\n  \n  # Check for new scroll height and attempt to load more if scrolled to bottom\n  new_height &lt;- remDr$executeScript(\"return document.body.scrollHeight;\")\n  if (last_height == new_height && !attemptLoadMore()) { # && prioritizes the first condition\n    break\n  }\n}\n\nOnce all entries are loaded, we can use rvest as before to retrieve the elements of interest. The full script can be accessed here.\nTo summarize, the two web scraping approaches primarily differ in their sequence of actions and the logic behind page loading.\n\nThe first method sequentially loads and extracts data page by page, leveraging identifiable patterns in page requests for navigation.\nThe second method loads all relevant pages first before proceeding with data extraction, and simulates user interactions to trigger page loads.\n\nThe first approach can be faster but requires more in-depth observation of page loading patterns, and the second provides a more flexible solution for interacting with web pages, especially when direct patterns are not apparent. I hope you find this post informative and helpful!"
  },
  {
    "objectID": "statistics/simulations/index.html",
    "href": "statistics/simulations/index.html",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "",
    "text": "Named after the host of the popular game show “Let’s Make a Deal,” the Monty Hall problem is a classic example of how our intuition can sometimes lead us astray when it comes to probabilities.\nThe problem goes like this: You’re a contestant on “Let’s Make a Deal” and are asked to choose one of three doors. Behind one of the doors is a brand new car, while behind the other two are goats. After you make your choice, Monty Hall opens one of the other two doors to reveal a goat. He then gives you the option to (1) stick with your original choice or (2) switch to the other unopened door.\nWhat should you do? Should you stick with your original choice or switch to the other door?\nMany people’s intuition tells them that it doesn’t matter whether they stick with their original choice or switch and the probability of winning the car is 1/3 no matter what.\nIs this correct? In this blog post, I will introduce various methods to solve this puzzle, including probability theory, causal diagrams, and simulations. All code can be accessed here."
  },
  {
    "objectID": "statistics/simulations/index.html#probability-theory",
    "href": "statistics/simulations/index.html#probability-theory",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Probability Theory",
    "text": "Probability Theory\nOne straightforward approach is to apply the Bayes’ theorem to calculate and compare the probability of winning between switching and not switching. To simplify the analysis, we can define two events: event A is when the car is behind the remaining door, and event B is when Monty chooses a door with a goat. Our objective is to determine the probability that the car is behind the remaining door given Monty’s exposure door. If the probability is high, then switching would be a better choice, and vice versa.\nAssuming our initial choice is Door 1 and Monty opens Door 2, we can use Bayes’ theorem to calculate the conditional probability of the car being behind Door 3 given that Monty opened Door 2. The calculation is as follows:\n\\[\nP(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n\\]\nNow, let’s figure out what each part represents. \\(P(B|A)\\) is 1 because Monty should never reveal a car by design and our choice of Door 1 rules out another option. We also have \\(P(A) = 1/3\\), since a car is equally likely to be assigned to one of the three doors. The marginal probability \\(P(B)\\) that Monty opens Door 2 without conditioning on what the remaining door contains is 1/2, as our initial choice leaves him two options to pick.\nTaken together, we have the probability of winning for switching is \\(P(A|B) = \\frac {1 \\times 1/3}{1/2} = 2/3\\) and the probability of wining for sticking to the initial choice is \\(1 - P(A|B) = 1/3\\). So yes, the correct answer is we should always switch!"
  },
  {
    "objectID": "statistics/simulations/index.html#causal-diagrams",
    "href": "statistics/simulations/index.html#causal-diagrams",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Causal Diagrams",
    "text": "Causal Diagrams\nJudea Pearl’s The Book of Way sheds a new light on this classic probability puzzle. The key is go beyond the data itself and think through how the data was generated. Let’s start with the question: what determines which door will be opened by Hall?\nOur choice of door excludes one option he could open. Knowing that which door that does have a goat behind it means that he has to choose a different one as well if possible. In this case, we end up with the following causal diagram where door to be opened is a collider.\nWhat will happen if we condition on a collider? In other words, what if we take action based on Monty’s choice of door? In causal graph terms, conditioning on a collider creates an association between previously independent variables (causal diagrams shifting from the left to the right as shown below), in this case, our choice of door and the location of the car. Let’s still assume that our initial choice is Door 1 and Monty opens Door 2. Translating this causal diagram into probabilities, the probability of switching (the car being behind Door 3) conditional on Monty’s choice becomes 2/3 because now the car should be behind one of the remaining two doors, which is greater than 1/3.\nMore intuitively, the reason why the probability of a car behind Door 1 changes from 1/3 to 2/3 when we condition on Monty’s choice of door is that his choice is not random: he has to pick a door without a car. Monty could not open Door 1 once we chose it – but he could have opened Door 3. The fact that he did not implies that he was forced to and thus there is more evidence than before that the car is behind Door 3."
  },
  {
    "objectID": "statistics/simulations/index.html#simulations",
    "href": "statistics/simulations/index.html#simulations",
    "title": "Revisiting the Monty Hall Problem: The Power of Simulations",
    "section": "Simulations",
    "text": "Simulations\nAs you may have noticed, solving the Monty Hall problem requires some effort to overcome our intuition. However, there is a more intuitive way to approach this problem: simulations. Using simulations, we can better understand the probabilities at play and verify the counter-intuitive result of the problem.\nThe basic idea is still to compute and compare the winning probability of switching and not switching, but now let’s play this game 10000 times and compare the relative frequency of winning.\nUsing exactly the same setup where our initial choice is Door 1, the probability of not switching after 10000 iterations of the game is 0.33.\n```{r}\n# The winning probability of sticking to the initial choice\nset.seed(123)\ndoors &lt;- c(1,2,3)\n\ncount &lt;- 0\nfor(i in 1:10000) {\n  car &lt;- sample(doors, 1)\n  initial_choice &lt;- 1\n  if(initial_choice == car){\n    count &lt;- count + 1\n  }\n}\n\np_stick &lt;- count/10000\n```\nA slightly tricky part is specifying which door Monty will reveal. Following the same logic, Monty will choose a door different from both our initial choice and the door with the car, we can write a simple function as below and compute the relative frequency of winning when we switch. The probability is 0.67, twice as larger as 0.33.\n```{r}\n# The winning probability of switching to another choice \nset.seed(123)\nreveal &lt;- function(doors, car, initial_choice) {\n  if(car == initial_choice){\n    reveal &lt;- sample(doors[-c(car,initial_choice)], 1)\n  } else {\n    reveal &lt;- doors[-c(car, initial_choice)]\n  }\n}\n\ncount &lt;- 0\nfor (i in 1:10000) {\n  car &lt;- sample(doors,1)\n  initial_choice &lt;- 1\n  revealed_door &lt;- reveal(doors, car, initial_choice)\n  final_choice &lt;- doors[-c(initial_choice, revealed_door)]\n  if(final_choice == car){\n    count = count + 1\n  }\n}\n\np_switch &lt;- count/10000\n```\nI hope this post has highlighted the usefulness of simulations as a tool to gain a more intuitive understanding of probability concepts. By running simulations of the Monty Hall problem multiple times, we can observe how the probability of winning changes when we switch doors, which can be helpful in building a deeper understanding of the problem. Additionally, simulations can also serve as a good way to verify counter-intuitive results and build confidence in other probability-based solutions."
  },
  {
    "objectID": "statistics/nlp-ai-news/index.html",
    "href": "statistics/nlp-ai-news/index.html",
    "title": "Hype or Harm: Examining Media Coverage of AI",
    "section": "",
    "text": "Artificial Intelligence (AI) and Large Language Models (LLMs) have become buzzwords, sparking significant media attention. I found myself intrigued by how AI is portrayed in the media – what issues capture public interest, what sentiments news stories tend to evoke, and how key players like tech leaders and companies shape these narratives. To tackle these questions, my friend and I analyzed 10,000 AI-related news headlines from May to November 2023. In this blog post, I’ll share the behind-the-scenes process of our project, discussing how we approached these questions. If you’re more interested in the results than the technical details, you can check out our presentation slides and my blog post that highlights our findings with 10 graphs."
  },
  {
    "objectID": "statistics/nlp-ai-news/index.html#research-design",
    "href": "statistics/nlp-ai-news/index.html#research-design",
    "title": "Hype or Harm: Examining Media Coverage of AI",
    "section": "Research design",
    "text": "Research design\nTo begin with, I outlined a flowchart to guide our analysis, as shown below. Each question was broken down into general tasks (indicated by blue boxes), such as topic modeling, sentiment analysis, and named entity recognition. For each task, we clarified the information we aimed to obtain and started to identify language models or methods (represented by pink boxes) that could help address these questions. Complex tasks were further decomposed into downstream activities (marked by green boxes), including model fine-tuning and more detailed classification. Expected outputs were highlighted in yellow boxes.\n\n\n\nFlowchart of Research Design\n\n\nOne key lesson I learned during this process is the importance of keeping humans involved at every stage. Our role is not just as human annotators for preparing training data, but also as evaluators and improvers of the results generated by language models, discovering new avenues for exploration, and interpreting the results. By maintaining this human-in-the-loop approach, we ensure the outcomes are accurate and meaningful, leading to deeper insights and better decision-making."
  },
  {
    "objectID": "statistics/nlp-ai-news/index.html#what-are-dominant-themes",
    "href": "statistics/nlp-ai-news/index.html#what-are-dominant-themes",
    "title": "Hype or Harm: Examining Media Coverage of AI",
    "section": "What are dominant themes?",
    "text": "What are dominant themes?\n\nTopic identification\nFor topic extraction, Structural Topic Modeling (STM) and BERTopic were the first methods that came to our mind. STM, with its capability to integrate metadata, is well-suited for exploring how external factors influence topic prevalence – particularly in cases where media coverage might be shaped by the type of media source or country-specific characteristics. To optimize performance, we experimented with different numbers of topics and varying metadata inputs. We settled on 15 topics, ensuring that the topics are sufficiently distinct from one another while capturing the overall content of the text data.\nBERTopic, on the other hand, uses embeddings and clustering to identify topics, minimizing outliers that don’t fit in any defined topics and enabling more accurate semantic representation. What’s also nice about BERTopic is its flexibility to incorporate user guidance, known as guided BERTopic. This allows users to provide a set of keywords aligned with specific themes, guiding the clustering process to improve performance.\nHow well do these methods perform in identifying topics? To find out, we randomly selected 1,000 news headlines and (1) examined the topic of the highest probability for each headline, and (2) checked for any missing topics of interest. Here’s what we found:\n\nBERTopic performs better at capturing more specific topics than STM\nFor example, when analyzing AI companies, STM might group all AI companies into a single topic. In contrast, BERTopic can distinguish between companies with a competitive focus (with keywords like “competition” and “winner”) and those that are more collaborative (with keywords such as “acquisition” and “partnership”).\nBERTopic can over-consolidate when words frequently appear together\nThis can lead to topics being grouped too narrowly. For example, news about Humane’s wearable AI pin was categorized as a unique topic, despite it fitting better within a broader AI products category that was also identified.\nBoth methods can miss topics of interest\nBoth STM and BERTopic identified major topics like job displacement and ethical concerns, which were part of AI safety and risks. However, upon closer examination of 1,000 headlines, we found additional subtopics such as advanced AI concerns and AI hallucinations. This indicates that these unsupervised or semi-supervised approaches may not well capture the full range of subtopics, especially when there’s significant variation.\n\nTherefore, we decided to keep the broader topics or dominant themes that were identified by both methods, including AI potentials, AI products, AI companies, AI industrial development, AI policies and regulations, AI stocks, AI research, and AI safety and risks.\n\n\n\nOverview of Dominant Themes\n\n\n\n\nText classification\nOur next step is to address the limitation of STM and BERTopic – their inability to reveal specific subtopics within broader themes. This is where we turned to few-shot learning with GPT-4 Turbo, one of the most advanced LLMs. Here are our key steps:\n\nSubtopic generation and manual review\nWe used GPT-4 Turbo to generate potential subtopics from each group of texts with dominant themes. Afterward, we manually reviewed the suggested subtopics, selecting those that were distinct and relevant. For instance, within the broader theme of AI safety and risk, we identified subtopics like job displacement, copyright infringement, cybersecurity risks, ethical concerns, AI fakery, AI attribution, AI misuse, AI accuracy, AI privacy, and concerns about advanced AI.\nText classification and manual adjustments\nOnce we had the subtopics, we asked GPT-4 Turbo to classify the texts, providing examples for guidance. We specified that if a text was difficult to categorize, it should be assigned to “others”. We then manually reviewed the “others” category (~100), where any text that didn’t fit a specific topic was discarded, and assigned the rest to the most appropriate existing subtopic."
  },
  {
    "objectID": "statistics/nlp-ai-news/index.html#what-sentiments-and-emotions-tend-to-be-evoked",
    "href": "statistics/nlp-ai-news/index.html#what-sentiments-and-emotions-tend-to-be-evoked",
    "title": "Hype or Harm: Examining Media Coverage of AI",
    "section": "What sentiments and emotions tend to be evoked?",
    "text": "What sentiments and emotions tend to be evoked?\nMoving on to the second question, we first broke down sentiment analysis into two components: (1) sentiment polarity (positive, negative, neutral), (2) specific emotions (anger, fear, sadness, surprise, joy, neutral). The first provides an overview of the general mood of media coverage, while the second offers more detailed emotional undertones.\n\nIdentify sentiment polarity\nTo accurately label sentiment polarity, having robust labeled data for training classifiers is key. We developed our labeled data by (1) manually labeling sentiment polarity for 1,000 news headlines, (2) automated classification using popular Python libraries – Vader, TextBlob, and Flair, and (3) validating these labels with manual adjustments. During this process, we identified a couple of common issues and took corrective actions:\n\nConservative positive labels\nSome headlines that seemed to trigger positive sentiments were labeled as neutral by us. To address this, we added two more human annotators to vote on the appropriate sentiment, with the majority vote determining the final label.\nMisinterpretation of positive keywords\nWords like “revolution” or “revolutionize” often implied positive sentiment, but were sometimes misinterpreted as negative by these models. We manually corrected these cases to ensure accuracy.\n\nTo guarantee enough training data for training our classifier, as this data will need to be further divided into training and test sets, we randomly selected additional news headlines and repeated the process until we had at least 1,000 labeled headlines with consensus between human annotators and the models.\nTo train our sentiment classifier, we fine-tuned a pre-trained language model RoBERTa using PyTorch. We allocated 90% of the data as training set and 10% as the test set, ensuring the test set remained untouched throughout training. Our model parameters included a learning rate of 1e-5, a batch size of 32, 10 epochs, and AdamW as the optimizer. To address challenges like unstable gradients, we further implemented several techniques, including\n\nGradient clipping: limited the gradients to prevent them from becoming too large\nStep decay: reduced the learning rate after each epoch to avoid overshooting during training\n\nAfter hours of training, we achieved impressive metrics, with a validation accuracy of 0.9658, and a validation F1 score of 0.9663!\n\n\n\nDaily Trends of Sentiment Polarity\n\n\n\n\nIdentify emotions\nTo dig deeper into emotions, we used DistilBERT, which can (1) identify Ekman’s six fundamental emotions – anger, disgust, fear, sadness, joy, and surprise – along with a neutral category, and (2) determine whether a text is fear-mongering.\nFrom the automated classification results, we manually reviewed 1,000 observations and decided to focus on fear as a key emotion for further analysis. This decision was driven by both technical and theoretical considerations. DistilBERT’s binary classification for fear was more reliable compared to its multi-label classification of other emotions. Furthermore, since our earlier analysis showed an overall hype with a majority of positive sentiments, exploring negatively charged news headlines could offer insights into how media highlights potential risks and challenges related to AI.\nSo the next task was to improve the identification of fear in news headlines. To better validate these binary labels, we turned to Llama 2 with few-shot learning. We started by running Llama 2 on 1,000 news headlines and refined our prompt by specifying the role of the LLM, the task, rationale for classifying fear, and examples of fear and non-fear, while allowing for uncertainty with a “don’t know” category. For this round, we asked Llama 2 to provide a rationale for its judgement to ensure the model’s reasoning aligned with the task.\nAfter examining the classified results, we identified several issues:\n\nIncorrect judgments despite correct reasoning\nLlama 2 sometimes provided correct reasoning, but the final decision was wrong. We iterated the prompt to correct these cases.\nNon-response due to safeguards\nLlama 2 didn’t respond to news headlines containing sensitive topics like child pornography, likely due to model safeguards. We manually classified these cases as fear-mongering in the end.\n\nWith an improved prompt, we ran Llama 2 on the entire dataset, but without requiring reasoning in the output to reduce computational costs. One limitation of this approach though is its inability to distinguish whether a news headline is categorized as fear-mongering because the reporter intentionally incited fear through specific words or styles, or because the public might inherently associate the topic with fear, which opens avenues for further research.\nWe also connected topic extraction with sentiment analysis by examining sentiment breakdown for each topic and incorporating richer metadata – ideological leanings of media outlets. To do this, I scraped the data for us from the AllSides organization and the detailed process is documented in another post."
  },
  {
    "objectID": "statistics/nlp-ai-news/index.html#which-companies-and-individuals-dominate-media-attention",
    "href": "statistics/nlp-ai-news/index.html#which-companies-and-individuals-dominate-media-attention",
    "title": "Hype or Harm: Examining Media Coverage of AI",
    "section": "Which companies and individuals dominate media attention?",
    "text": "Which companies and individuals dominate media attention?\nOpinion leaders and tech companies represent another group of key entities that can shape AI coverage. This was prompted by the presence of influential figures like Bill Gates and Elon Musk in topic-related keywords. AI companies formed a dominant theme with two distinct subtopics – company-wise competition and collaboration – providing a fascinating comparison to investigate.\nThe task at hand involved named entity recognition (NER) to identify and categorize these important entities, of which I took entire ownership. Using distilbert-NER, a DistilBERT variant specifically fine-tuned for NER tasks, I extracted news headlines mentioning individuals and organizations. The top three opinion leaders, based on the amount of media coverage, were Elon Musk, Bill Gates, and Sam Altman. To identify tech companies, I applied NER to news headlines categorized as AI companies. Since startups might not always make headlines, I also included keyword matching with terms like “startup” and “company” to capture a broader range of tech-related entities.\nTo visualize these insights, I created data visualizations to represent the critical narratives around these opinion leaders and tech companies. Using GPT-4 Turbo, I identified key themes in the news headlines and color-coded them – red for predominantly positive sentiments and blue for negative ones. For the common narrative of AI safety and risks, I used a larger font to ensure it stood out and added pencil-sketch headshots of key figures to enhance the visual appeal.\n\n\n\nKey Narratives from Opinion Leaders\n\n\nTo explore the media coverage of company-wise competition and collaboration, I used the igraph package in R to create network diagrams. These diagrams showed the co-occurrence of company names in various text corpora, with the thickness of the connecting lines representing the frequency of co-occurrence. This method helped to highlight the relationships between companies and offered insights into the competitive and collaborative dynamics within the AI industry.\n\n\n\nNetwork of Company-Wise Collaboration\n\n\nTo conclude, working with NLP models hosted at Hugging Face and large language models (LLMs) has been a fascinating experience. Here are a few key takeaways:\n\nLLMs excel at summarization tasks\nLLMs require more fine-tuning and clever prompt engineering for classification tasks\nGPT-4 Turbo is the best-performing LLM for similar tasks but usually cost more due to its closed-source nature\nAlways keep humans in the loop"
  },
  {
    "objectID": "statistics/bias-variance/index.html",
    "href": "statistics/bias-variance/index.html",
    "title": "Explaining the Bias-Variance Tradeoff",
    "section": "",
    "text": "In order to assess the model performance on a given dataset, we need to measure how closely its predictions align with the actual data. In regression settings, we often use mean squared error (MSE) and a smaller MSE means the predicted responses are closer to the true responses.\nWhile we can get MSE from both training and the test data, what we really care is the model performance on unseen test data. This is where the bias-variance tradeoff comes into play. The process of selecting a model that minimizes MSE on the test data inherently involves a tradeoff between bias and variance. In this blog post, we will explore the concept of the bias-variance tradeoff with the help of visual aids and a mathematical proof.\nBias refers to the error that results from oversimplifying the model. A model with high bias may be too simple to capture the complexity of the underlying data or identify the relationship between input and outcome variables.\nOn the other side, variance refers to the amount by which the model would change if we estimated it using a different training set. A model with high variance means that the model is overly complex such that it captures noises in the data instead of the underlying patterns. This leads to the problem of over-fitting where our model performs too well on the training data but poorly on unseen data.\nAs shown in the following graph, model (1) which is a constant has extremely high bias but zero variance, while model (2) which looks extremely wiggly by just connecting observed data points has low bias but high variance.\nThe bias-variance tradeoff arises because as the flexibility (or complexity) of the model increases, we often have smaller bias but larger variance, and vice versa. The relative changes in bias and variance determine whether the test MSE increases or decreases.\nMathematically, we can decompose expected prediction error (expected test MSE) into two components: irreducible error and reducible error. Irreducible error is the variation that cannot be reduced by modifying the model, and is associated with unmeasured variables. Reducible error is comprised of the sum of squared bias and variance of the model. Our objective is to minimize reducible error while recognizing that we cannot surpass the irreducible error. A proof of this decomposition can be found at the end of this post.\nThe following graph contains all the concepts we’ve discussed in this post. In future posts, we will delve deeper into how to find an optimal balance between bias and variance using different machine learning models."
  },
  {
    "objectID": "statistics/bias-variance/index.html#proof",
    "href": "statistics/bias-variance/index.html#proof",
    "title": "Explaining the Bias-Variance Tradeoff",
    "section": "Proof",
    "text": "Proof\nSuppose that we have a simple model specified below, we can break down the expected prediction error (expected test MSE) \\(E(y_0) - \\hat f(x_0))^2\\) and see where the bias-variance tradeoff comes from.\n\\[\ny = f(x) + \\epsilon \\\\\n\\text{where } E(\\epsilon) = 0, \\epsilon \\perp x, x \\text{ is fixed }\n\\]\nWe can rewrite the expected prediction error as below \\[\nE(y_0) - \\hat f(x_0))^2 \\\\\n= E(y_0 \\color{red}{- f(x_0) + f(x_0)} \\color{blue}{- E\\hat f(x_0) + E\\hat f(X_0)} - \\hat f(x_0))^2\n\\] What’s within \\(E()^2\\) can be thought of as a summation of three parts a, b, and c where \\[\na = y_0 - f(x_0) \\\\\nb = f(x_0) - E\\hat f(x_0) \\\\\nc = E\\hat f(x_0) - \\hat f(x_0)\n\\] Recall that \\[\n(a+b+c)^2 = a^2 + b^2 + c^2 + 2ab + 2bc + 2ac\n\\] then we have \\[\nE(y_0) - \\hat f(x_0))^2 \\\\\n= E(a^2) + E(b^2) + E(c^2) + 2E(ab) + 2E(bc) + 2E(ac)\n\\] Let’s walk through the last three terms first.\nIt’s not hard to see \\(E(ab) = 0\\), \\(E(bc)=0\\), and \\(E(ac)=0\\) as highlighted below.\n\\[\n\\begin{aligned}\nE(ab) &= E((y_0 - f(x_0))(f(x_0) - E \\hat f(x_0))) \\\\\n&= \\color{red}{E(y_0 - f(x_0))}(f(x_0) - E \\hat f(x_0))\\\\\n&= 0\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(bc) &= (f(x_0) - E \\hat f(x_0)) E((E\\hat f(x_0) - \\hat f(x_0)))\\\\\n&= (f(x_0) - E \\hat f(x_0)) \\color{red}{(E \\hat f(x_0) - E \\hat f(x_0))} \\\\\n&= 0\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\nE(ac) &= E((y_0) - f(x_0)) \\color{red}{(E \\hat f(x_0) - \\hat f(x_0))}\\\\\n&= 0\n\\end{aligned}\n\\] Now, let’s turn to the first three terms.\nRecall that \\[\nE(E(\\hat \\theta) - \\hat \\theta)^2 = var(\\hat \\theta)\\\\\n\\text{Bias}(\\hat \\theta)^2 = (E(\\hat \\theta) - \\theta))^2\n\\]\nWe can rewrite these terms as follows: \\[\n\\begin{aligned}\nE(a^2) &= E(y_0 - f(x_0))^2 \\\\\n&= E(\\epsilon_0)^2 \\\\\n&= E(\\epsilon^2) - \\color{red}{(E(\\epsilon))^2} \\\\\n&= var(\\epsilon)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE(b^2) &= E(f(x_0) - E \\hat f(x_0))^2 \\\\\n&= (f(x_0) - E \\hat f(x_0))^2 \\\\\n&= \\text{Bias}^2(\\hat f(x_0))\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\nE(c^2) &= E(E\\hat f(x_0) - \\hat f(x_0))^2 \\\\\n&= var(\\hat f(x_0))\n\\end{aligned}\n\\] Taken together, it becomes clear that the expected prediction error can be decomposed into three parts. The first term \\(var(\\epsilon)\\) is the irreducible error and the others constitute the reducible error that we try to minimize.\n\\[\nE(y_0) - \\hat f(x_0))^2 = \\color{red}{var(\\epsilon)} +  \\color{blue}{\\text{Bias}^2(\\hat f(x_0)) + var(\\hat f(x_0))}\n\\]"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n10 Graphs about Sentiments in AI News Coverage\n\n\n\ncode\n\n\nvisualization\n\n\ntext data\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nApr 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday Data Visualizations\n\n\n\ncode\n\n\nvisualization\n\n\ntidytuesday\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nOct 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Racial Justice: Moving Beyond Bar Charts\n\n\n\ncode\n\n\nvisualization\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nFeb 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Green Spaces with OpenStreetMap in R\n\n\n\ncode\n\n\nvisualization\n\n\nspatial data\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Calendar of Gun Violence\n\n\n\ncode\n\n\nvisualization\n\n\nspatial data\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevisiting Tree Equity Gap: Hexbin or Geofacet?\n\n\n\ncode\n\n\nvisualization\n\n\nspatial data\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreen Space for Everyone? Visualizing Tree Equity Gap\n\n\n\ncode\n\n\nvisualization\n\n\nspatial data\n\n\n\n\n\n\n\nZhaowen Guo\n\n\nFeb 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dataviz/tree-equity/index.html",
    "href": "dataviz/tree-equity/index.html",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "",
    "text": "Urban green spaces, such as parks, gardens, and forests, provide numerous benefits in creating livable cities. They help improve air and water quality, reduce heat islands, and increase physical activity and mental health. However, not all communities have equal access to these benefits, leading to the concept of “tree equity.”\nTree equity refers to the fair distribution of urban green spaces and trees, regardless of a community’s socio-economic status, race, or ethnicity. American Forests, a non-profit organization dedicated to protecting and restoring forests, has been working to measure and improve tree equity across the United States.\nIn this blog post, I will show you how to visualize tree equity score data to tell a compelling story. All code can be found here."
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-collection",
    "href": "dataviz/tree-equity/index.html#data-collection",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Collection",
    "text": "Data Collection\nTree equity score data are currently hosted on this webpage, where users are required to manually download zip files containing geospatial data for each state. Furthermore, each shapefile includes not only tree equity scores, but also several other variables. Thus, we need to find an efficient method to automate the process of downloading files with minimal manual intervention and extract the desired variable of interest - the tree equity score - from the shapefiles.\nUpon checking the web addresses of several files, a pattern has become apparent: each URL shares a similar structure in its file name, while also possessing a unique postal code. This discovery suggests that it is possible to create a script that can automatically scrape the files. Fortunately, unzip() in base R and sf package offer a convenient solution for unzipping files and reading shapefiles, and we can easily write a function to automate this process.\n```{r}\nlibrary(sf)\nlibrary(tidyverse)\n\nread_shape_URL &lt;- function(URL){\n  cur_tempfile &lt;- tempfile()\n  download.file(url = URL, destfile = cur_tempfile)\n  out_directory &lt;- tempfile()\n  unzip(cur_tempfile, exdir = out_directory)\n  \n  read_sf(dsn = out_directory)\n}\n```\nThe question that I am interested in is: which state has the greatest disparity in tree equity score across counties, as measured by the difference between the maximum and minimum tree equity scores within each state. Also, it is important to note that data availability may vary as not all states have available tree equity score data. To verify the existence of a URL, we can use Rcurl package and write a for-loop to produce the key variable of interest, the tree equity gap.\n```{r}\nstates &lt;- read.csv(\"state-names.csv\") # downloaded https://worldpopulationreview.com/states/state-abbreviations\n\nstate_names &lt;- rep(NA, 51)\ntes_gaps &lt;- rep(NA, 51)\nfor (i in 1:nrow(states)){\n  state &lt;- states$lower_code[i]\n  state_names[i] &lt;- state\n  print(state)\n  URL &lt;- paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\")\n  if (RCurl::url.exists(URL) == T) {\n    map &lt;- read_shape_URL(paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\"))\n    tes_gap &lt;- max(map$tes) - min(map$tes)\n    tes_gaps[i] &lt;- tes_gap\n  } else {\n    tes_gaps[i] &lt;- NA\n  }\n}\n\ndata &lt;- data.frame(lower_code = state_names,\n                   gap = tes_gaps) %&gt;%\n  cbind(states[1])\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-cleaning",
    "href": "dataviz/tree-equity/index.html#data-cleaning",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nGreat, we now have our data! The next step is to clean and prepare it for visualization. With the consideration of working with spatial data across multiple regions and the fact that the tree equity gap is a continuous variable, hexbin maps become a desirable choice. Hexbin maps divide the map area into small hexagonal bins and consolidate the data points within them, presenting a clear and compact depiction of the vast amount of data.\n```{r}\nlibrary(geojsonio)\nlibrary(rgeos)\n\n# create a base hexbin map of US\nhex_states &lt;- geojson_read(\"us_states_hexgrid.geojson\", what = \"sp\") \n\n# extract state names\nhex_states@data &lt;- hex_states@data %&gt;%\n  mutate(google_name = str_replace(google_name, \" \\\\(United States\\\\)\", \"\"))\n\n# create a data frame for hexbin map\nhex_states_fortify &lt;- broom::tidy(hex_states, region = \"google_name\")\n\n# match state names\ndata_map &lt;- hex_states_fortify %&gt;%\n  right_join(data, by = c(\"id\" = \"state\")) %&gt;%\n  mutate(id = state.abb[match(id, state.name)])\ndata_map$id[data_map$group == \"District of Columbia.1\"] &lt;- \"DC\"\n\nlabels &lt;- cbind.data.frame(data.frame(gCentroid(hex_states, byid = T),\n                                      id = hex_states@data$iso3166_2))\ndata_map &lt;- data_map %&gt;%\n  right_join(labels, by = \"id\") %&gt;%\n  filter(is.na(gap) == F)\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#data-visualization",
    "href": "dataviz/tree-equity/index.html#data-visualization",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Data Visualization",
    "text": "Data Visualization\nWe now have our spatial polygon data frame ready and can visualize it! To make it visually appealing, I pick a custom Google font “Pragati Narrow” for the graph and a stunning color palette from the scico package. One trick is to adjust text colors that contrast with the background color. For instance, in the case of Washington D.C., which has the narrowest gap in tree equity scores, the bin’s background can be made lighter. However, if white text is still used as in other areas, the label may not be easily visible. To mitigate this, we can establish a threshold for switching the text color as necessary.\n```{r}\nlibrary(scico)\nlibrary(showtext)\ntheme_set(theme_minimal(base_family = \"Pragati Narrow\"))\n\ntheme_update(\n  # legend\n  legend.title = element_blank(),\n  legend.position = 'top',\n  legend.direction = 'horizontal',\n  legend.key.width = unit(1.5, \"cm\"),\n  legend.text = element_text(color = \"black\",  size=30),\n  \n  # axis\n  axis.text.x = element_blank(),\n  axis.text.y = element_blank(),\n  \n  # titles\n  panel.grid = element_blank(),\n  plot.margin = margin(15, 30, 15, 30),\n  plot.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  panel.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  legend.title.align=1,\n  plot.title = element_text(\n    color = \"black\", \n    size = 70, \n    face = \"bold\",\n    margin = margin(t = 15),\n    hjust = 0.5\n  ),\n  plot.subtitle = element_text(\n    color = \"grey10\", \n    size = 45,\n    lineheight = 3,\n    margin = margin(t = 5),\n    hjust = 0.5\n  ),\n  plot.title.position = \"plot\",\n  plot.caption.position = \"plot\",\n  plot.caption = element_text(\n    color = \"grey20\", \n    size = 20,\n    lineheight = 0.5, \n    hjust = 0.5,\n    margin = margin(t = 40))\n)\n\ndata_map %&gt;% \n  ggplot () +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = gap), linewidth = 0.5) +\n  scale_fill_scico(palette = \"lajolla\", direction = 1) + \n  geom_text(aes(x=x, y=y, label=id, color = gap &lt; 60), size = 8, alpha = 0.5, \n             show.legend = F) +\n  scale_color_manual(values = c(\"white\", \"black\")) +\n  coord_map(clip = \"off\") +\n  labs(title = \"TREE EQUITY GAP\",\n       subtitle = \"Block-level disparities in tree equity scores within each state\",\n       x = \"\", y = \"\",\n       caption= \n         str_wrap(\n       \"Data comes from the Green Space Data Challenge, \n       collected and shared by the American Forests. \n       Tree Equity Score (TES) computes how much tree canopy and surface temperature align with income, \n       employment, race, age and health factors in the U.S | Visualization by Zhaowen Guo\", width=150))\n```"
  },
  {
    "objectID": "dataviz/tree-equity/index.html#implications",
    "href": "dataviz/tree-equity/index.html#implications",
    "title": "Green Space for Everyone? Visualizing Tree Equity Gap",
    "section": "Implications",
    "text": "Implications\nWhat do we learn from this visualization? The graph clearly illustrates the unequal distribution of green spaces, particularly in states like Ohio and Minnesota, where the gap is much more pronounced. This calls for prompt and effective action to rectify this imbalance."
  },
  {
    "objectID": "dataviz/racial-disparity/index.html",
    "href": "dataviz/racial-disparity/index.html",
    "title": "Visualizing Racial Justice: Moving Beyond Bar Charts",
    "section": "",
    "text": "Gun violence disproportionately and overwhelmingly affects communities of color, resulting in significant racial disparities in exposure to gun violence. To tell stories of racial justice in gun violence, it’s crucial to use compelling visuals. While many reports have relied on bar charts and stacked bar charts to illustrate unequal gun exposure, using them exclusively may not always be visually engaging. In this blog post, we will explore some alternative ways to visualize racial justice in gun violence. My full code can be accessed here.\nOne effective way to display racial disparities in gun violence is using a parliament chart, which is suitable for categorical data in a two-dimensional grid format. Let’s use recorded gunshot incidents in Washington DC in 2021 as an example. After cleaning the data, it is evident that out of the 555 communities in the city, 293 experienced gun violence. Shockingly, over 70% of these communities were African-American, highlighting a clear instance of racial disparity.\nTo convey this message using a parliament chart, we can use the ggparliament package which just takes two steps to get our work done. Firstly, we need to create a data frame with the categorical data we want to visualize, which in this case is the number of communities per racial group that have been exposed to gun violence. Next, we can convert this data frame to an appropriate structure for creating a parliament chart using the parliament_data() function. In the second step, we can specify the shape of the parliament chart (circle, semicircle, square, or rectangle), the number of rows to display, and the counts for each category.\n```{r}\ncommunity_gunshot &lt;- data.frame(groups = c(\"White Community\", \"African-American Community\"),\n                                count = c(84, 209),\n                                colors = c(\"#d39a2d\",\"#591c19\"))\n\ncommunity_gunshot_data &lt;- parliament_data(election_data = community_gunshot,\n                                          type = \"semicircle\",\n                                          parl_rows = 6,\n                                          party_seats = community_gunshot$count)\n```\nOnce we have completed these two steps, we can then use the data frame with ggplot and add a layer called geom_parliament_seats(). We can also add whatever aesthetics we desire. And voila, our parliament chart is ready to be displayed!\n\nWhen attempting to replicate this graph by comparing Hispanic/Latino communities to others, an issue arose: there are only five Hispanic/Latino communities out of the 555 communities. If we display the raw counts of communities experiencing gun violence, this could give the false impression that non-Hispanic or non-Latino communities had greater exposure to gun violence. However, in reality, four out of the five Hispanic/Latino communities experienced gun violence. As a result, it is crucial to display proportions rather than raw counts when comparing ethnic communities.\nWhat alternative visualization options do we have besides using bar charts? One idea that came to mind was using filled icons, such as handguns, with different heights of filled colors to represent the proportions of communities that experienced gun violence. However, implementing this idea was more time-consuming than anticipated. I would appreciate any suggestions on how to streamline this process.\nMy approach was to combine the echarts4r and the ggplot workflows. echarts4r is a powerful tool for creating interactive visualizations and includes a handy function called e_pictorial(), which allows us to incorporate any images we want to plot, such as a handgun icon in our case. To use an image, we simply need to provide the path to the svg file.\n```{r}\nicon_path = \"path://M544 64h-16V56C528 42.74 517.3 32 504 32S480 42.74 480 56V64H43.17C19.33 64 0 83.33 0 107.2v89.66C0 220.7 19.33 240 43.17 240c21.26 0 36.61 20.35 30.77 40.79l-40.69 158.4C27.41 459.6 42.76 480 64.02 480h103.8c14.29 0 26.84-9.469 30.77-23.21L226.4 352h94.58c24.16 0 45.5-15.41 53.13-38.28L398.6 240h36.1c8.486 0 16.62-3.369 22.63-9.373L480 208h64c17.67 0 32-14.33 32-32V96C576 78.33 561.7 64 544 64zM328.5 298.6C327.4 301.8 324.4 304 320.9 304H239.1L256 240h92.02L328.5 298.6zM480 160H64V128h416V160z\"\n\nhispanic = data.frame(ethnic = c(\"Hispanic\", \"Others\"),\n                      ratio = c(40, 25), # scale down the numbers 80 and 53\n                      path = c(icon_path,\n                               icon_path))\n# create a filled image graph\nhispanic %&gt;% \n  e_charts(ethnic) %&gt;% \n  e_x_axis(splitLine=list(show = FALSE), \n           axisTick=list(show=FALSE),\n           axisLine=list(show=FALSE),\n           axisLabel = list(show=FALSE)) %&gt;%\n  e_y_axis(max=100, \n           splitLine=list(show = FALSE),\n           axisTick=list(show=FALSE),\n           axisLine=list(show=FALSE),\n           axisLabel=list(show=FALSE)) %&gt;%\n  e_color(color = c('#811e18','grey'), background = \"#f5f5f2\") %&gt;%\n  e_pictorial(ratio, symbol = path, z=10, name = \"\",\n              symbolBoundingData= 50, symbolClip= TRUE) %&gt;% \n  e_pictorial(ratio, symbol = path, name= '', \n              symbolBoundingData= 50) %&gt;%\n  e_legend(show = FALSE) %&gt;%\n  e_grid(bottom = \"35%\") \n```\nHowever, one drawback is that echarts4r does not work directly with ggplot themes, so I couldn’t adjust the aesthetics to match my other ggplot charts. To work around this, I created a graph with two filled handgun icons and used it as a background image in ggplot. I then added supporting annotations to enhance the visualization.\nTo annotate the previously downloaded graph, I used the magick and ggpubr packages, which allowed me to easily add text and other annotations in the style of ggplot. The process involved importing the graph as an image using magick, and then using ggpubr to overlay ggplot-style text annotations on top of the image. To ensure that the annotations were positioned correctly, I specified the appropriate x- and y-coordinates for each annotation.\n```{r}\nlibrary(magick)\nlibrary(ggpubr)\nbackground &lt;- image_read(\"ethnic-gun.png\")\nxaxis &lt;- data.frame(xaxis = c(1, 2, 3),\n                    labels = c(\"\", \"\", \"\"))\nyaxis &lt;- data.frame(yaxis = c(1, 2, 3),\n                    labels = c(\"\", \"\", \"\"))\n\nggplot() +\n  background_image(background) +\n  geom_text(data = xaxis, aes(x = xaxis, y = 0, label = labels)) +\n  geom_text(data = yaxis, aes(x = 0, y = yaxis, label = labels)) +\n  labs(x=\"\",y=\"\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.background = element_rect(fill = \"#f5f5f2\", color = NA)) +\n  annotate(geom = \"text\", x = 0.8, y = 0.8, label = \"80% of Hispanic or Latino Communities\",\n           size = 15,  family = \"Pragati Narrow\") +\n  annotate(geom = \"text\", x = 2.1, y = 0.8, label = \"53% of Other Communities\",\n           size = 15,  family = \"Pragati Narrow\") +\n  annotate(geom = \"text\", x = 1.45, y = 2.95, label = \"COMMUNITY EXPOSURE TO GUN VIOLENCE\",\n           size = 26,  family = \"Pragati Narrow\", fontface = \"bold\") +\n  annotate(geom = \"text\", x = 1.45, y = 2.82, label = \"Communities that experienced inidents of gunshots in 2021\",\n           size = 19,  family = \"Pragati Narrow\") +\n  annotate(geom = \"text\", x = 1.5, y = 0, label = \"Data comes from ShotSpotter gunshot detection system. Incidents of probable gunfires and firecrackers are excluded | Visualization by Zhaowen Guo\",\n           size = 12,  family = \"Pragati Narrow\",color = \"grey20\")\n\nggsave(\"dc_ethnic.png\", width = 14, height = 14/1.618, units = \"in\")\n```\nHere’s the final output! With this filled icon chart (sometimes referred to as a pictogram chart), it is easy to see that Hispanic/Latino communities are significantly impacted by gun violence compared to other ethnic groups."
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html",
    "href": "dataviz/green-space-gunshot-map/index.html",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "",
    "text": "OpenStreetMap (OSM) is an open-source mapping platform that provides detailed maps of the world. It is built and maintained by a global community of volunteers who contribute data, such as roads, buildings, and points of interest, to create a free and open map of the world.\nOSM offers a comprehensive API that enables users to easily access and use the map data seamlessly in their own applications. Previously, I relied on the OpenLayers plugin in QGIS to access OSM data. While QGIS provides a nice graphical user interface for loading and visualizing OSM data, I had to switch between R and QGIS to ensure consistency in the resulting graphs. Here comes the good news! Now we can directly access OSM from R using the osmdata package. By specifying the coordinates or the name of the geographic area of interest, we can easily obtain OSM data and perform analyses in R.\nContinuing on my previous articles about green spaces, in this blog post I will introduce how to map green spaces using OSM. Check out my full code here."
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html#data-collection",
    "href": "dataviz/green-space-gunshot-map/index.html#data-collection",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "Data Collection",
    "text": "Data Collection\nOnce we have installed the osmdata package, we can begin gathering our data. The first step is to define a specific area of interest using coordinates or place names. For example, to download all OSM data for Washington DC, we can try either of the following approaches to specify the bounding box:\n```{r}\n# first approach\nopq(getbb(\"Washington, District of Columbia\"))\n\n# second approach\nopq(bbox = c(-77.10, 38.80, 76.90, 39.00))\n```\nDepending on how we define green spaces, we can use add_osm_feature() to set specific key and value attributes and call osmdata_sf() to convert the output to a simple features (sf) object which will simplify the plotting process later. The same approach can be used to display other map elements, such as streets and rivers. The following code will extract the previously obtained OSM data that matches the defined attribute tags.\n```{r}\n# green spaces defined as parks, nature reserve, and golf course\ngreens &lt;- opq(getbb(\"Washington, District of Columbia\")) %&gt;%                \n  add_osm_feature(key = \"leisure\", \n                  value = c(\"park\", \"nature_reserve\", \"golf_course\")) %&gt;%\n  osmdata_sf()\n  \n# green spaces defined as grass  \ngreens &lt;- opq(getbb(\"Washington, District of Columbia\")) %&gt;%\n  add_osm_feature(key = \"landuse\", \n                  value = \"grass\") %&gt;%\n  osmdata_sf()\n```"
  },
  {
    "objectID": "dataviz/green-space-gunshot-map/index.html#data-visualization",
    "href": "dataviz/green-space-gunshot-map/index.html#data-visualization",
    "title": "Mapping Green Spaces with OpenStreetMap in R",
    "section": "Data Visualization",
    "text": "Data Visualization\nWith our data now downloaded, we can begin visualizing it using ggplot2. To visualize spatial data, we simply add a layer by geom_sf(). We can include an argument inherit.aes = FALSE to customize each layer, making its aesthetics (i.e. color, size) not inherit from previous layers.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1)\n```\nWe can also set the coordinate reference system (CRS) for the spatial data being plotted using coord_sf(). By default, the CRS is set to WGS 84 (EPSG code 4326), which I used in this example, and You can adjust it as needed. We can also specify the longitude and latitude in the same layer to “zoom in” the area of interest.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1) +\n  coord_sf(crs = st_crs(4326), xlim = c(-77.12, -76.90), ylim = c(38.79, 39.01)) \n```\nWe can also add additional layers to enrich our spatial visualization by displaying other types of geographic data, such as point locations. For example, in this illustration, I incorporated a geom_point() layer highlighting the locations of 2021 gunshot incidents.\n```{r}\nggplot() +\n  geom_sf(data = greens$osm_polygons, inherit.aes = FALSE, colour = \"#47632a\", fill = \"#47632a\", alpha = .5, size = 1) +\n  geom_point(data = gunshot, aes(x = LONGITUDE, y = LATITUDE), color = \"#c62320\", size = 0.1, inherit.aes = FALSE) +\n  coord_sf(crs = st_crs(4326), xlim = c(-77.12, -76.90), ylim = c(38.79, 39.01))\n```\nJust with a few additional aesthetic touches, I was able to create a plot that effectively visualizes both green spaces in the DC area and the recorded gunshot incidents that occurred in 2021."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html",
    "href": "dataviz/geofacet-hexbin/index.html",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "",
    "text": "Last week, I introduced how to visualize disparities in tree equity scores using hexbin maps. Hexbin maps are a useful tool for visualizing dense data points by summarizing them in a compact and understandable format. By grouping data points into hexagonal bins, they provide a clear picture of the spatial distribution of the data, enabling the detection of patterns that may be hidden by overplotting.\nAlthough hexagonal bins provide a clean representation of data, they result in information loss regarding the underlying shape of the data. For example, my previous visual using hexbin maps suggests that Ohio has the largest tree equity gap, which is defined as the maximum difference across block-level tree equity scores. However, this does not provide a complete picture. When examining the distribution of tree equity scores across census blocks within each state, it is evident that Ohio has a considerable number of blocks with relatively high levels of tree equity, indicating that Ohio performs well in this regard.\nGeofacet offers a useful alternative to hexbin maps, allowing for the restoration of the original data distributions while preserving the spatial information. By maintaining individual data points, geofacet provides a more detailed representation of data to enable a more nuanced view of spatial relationships.\nIn this blog post, I will walk you through how to visualize tree equity score data using geofacet. All code can be found here."
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html#data-collection",
    "href": "dataviz/geofacet-hexbin/index.html#data-collection",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "Data Collection",
    "text": "Data Collection\nWe follow similar steps of data collection and the only difference is we keep the original variable - tree equity score.\n```{r}\n# prepare a function to read zip urls with shapefiles \nread_shape_URL &lt;- function(URL){\n  cur_tempfile &lt;- tempfile()\n  download.file(url = URL, destfile = cur_tempfile)\n  out_directory &lt;- tempfile()\n  unzip(cur_tempfile, exdir = out_directory)\n  \n  read_sf(dsn = out_directory)\n}\n\n# pull state, tes, priority from each dataframe\ndata_lists &lt;- list()\nfor (i in 1:nrow(states)){\n  state &lt;- states$lower_code[i]\n  print(state)\n  URL &lt;- paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\")\n  if (RCurl::url.exists(URL) == T) {\n    map &lt;- read_shape_URL(paste0(\"https://national-tes-data-share.s3.amazonaws.com/national_tes_share/\", state, \".zip.zip\"))\n    data_lists[[i]] &lt;- map %&gt;% select(tes, state, priority)\n  }\n}\ndata &lt;- do.call(rbind, data_lists) %&gt;% na.omit()\n```"
  },
  {
    "objectID": "dataviz/geofacet-hexbin/index.html#data-visualization",
    "href": "dataviz/geofacet-hexbin/index.html#data-visualization",
    "title": "Revisiting Tree Equity Gap: Hexbin or Geofacet?",
    "section": "Data Visualization",
    "text": "Data Visualization\nThe function facet_geo() plays the magic, which creates the base map of the US that can be integrated with various data representations. By using the argument grid = us_state_grid1[c(-2, -11), ], we can exclude Hawaii and Alaska from the map as there is no data available for these states. If you prefer to have the full state name, you can add the argument label = \"name\".\nAnother thing to keep in mind is that the facet_geo() layer should integrate with a pre-existing data representation. In this example where I am interested in displaying the distribution of tree equity scores, I add geom_density() layer beforehand.\n```{r}\nggplot(data) +\n  geom_density(aes(x = tes), color = \"#466c4b\", fill = \"#7fa074\", alpha = 0.5) +\n  coord_cartesian(clip = \"off\") +\n  facet_geo(vars(state), scales = \"free_y\", grid = us_state_grid1[c(-2, -11), ], label = \"name\") +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  scale_y_continuous(\n    labels = scales::number_format(accuracy = 0.01)) +\n  labs(x = \"\", y = \"\",\n       title = \"ACCESS TO GREEN SPACE\",\n       subtitle = \"Distribution of tree equity scores across census blocks in the US\",\n       caption = str_wrap(\"\n       Tree Equity Score (TES) computes how much tree canopy and surface temperature align with income, \n       employment, race, age and health factors in the US, collected by American Forest | Visualization by Zhaowen Guo\", width = 300)) +\n  theme_void(base_family = \"Pragati Narrow\") +\n  theme(strip.text = element_text(face = \"bold\", color = \"grey20\", size = 30),\n        legend.position = \"none\",\n        axis.text = element_text(color = \"grey40\", size = 30),\n        strip.background = element_blank(),\n        plot.background = element_rect(fill = \"#f5f5f2\", color = NA),\n        plot.margin = margin(40, 15, 20, 15),\n        plot.title = element_text(face = \"bold\", size = 70, margin = margin(l=0, t=5)),\n        plot.subtitle = element_text(lineheight = 1, size = 50, margin(l=0, t=7)),\n        plot.caption = element_text(margin = margin(t=35), color = \"grey20\", size = 30),\n        plot.caption.position = \"plot\")\n\nggsave(\"tree-equity-geofacet.png\", dpi = 320, width = 14, height = 10)\n```"
  },
  {
    "objectID": "dataviz/media-coverage-ai-sentiment/index.html",
    "href": "dataviz/media-coverage-ai-sentiment/index.html",
    "title": "10 Graphs about Sentiments in AI News Coverage",
    "section": "",
    "text": "Artificial intelligence is not a substitute for human intelligence; it is a tool to amplify human creativity and ingenuity. — Fei-Fei Li\n\nHow do media outlets approach discussions about AI? What sentiments and emotions do these stories tend to evoke? Here are my takeaways from analyzing 10,000 AI-related news headlines from May to November 2023.\n\n1. Balanced Coverage on the Benefits and Risks of AI\n\nAI media coverage has generally struck a balance between positive and negative perspectives. However, a notable surge in negative sentiment occurred during the first week of July, driven by growing concerns about misinformation and ethical issues.\n\n\n2. Growing Concerns about Generative AI\n\nThe term “generative AI” originally appeared in headlines with predominantly positive connotations. However, it has started to emerge in negatively charged news stories over time, reflecting a growing focus on the potential drawbacks and risks associated with generative AI.\n\n\n3. Overall Favorable and Neutral Outlook on AI\n\nA noticeable rise in AI-related coverage has occurred since September 2023. Overall, the sentiment indicates a favorable attitude, with a predominance of neutral sentiment.\n\n\n4. Ethics, Job Loss, and AI Misuse are Top Concerns in AI Safety News\n\nThe relative sizes of the word clouds indicate the corresponding percentages of each topic. Ethical issues, job displacement, and AI misuse are the top three focuses among media outlets reporting on AI safety and risks. Other topics include concerns about advanced AI, AI privacy, copyright infringement, cybersecurity risks, AI-generated fakes, and AI accuracy.\n\n\n5. Advanced AI Tops Fear-Inducing News Stories Among AI Safety News\n\nAlthough all subtopics within AI safety and risks contain instances of fear, concerns about advanced AI account for the largest share of fear-inducing news stories.\n\n\n6. Left-Leaning Media Adopt a More Cautious Stance on AI\n\nThe graph on the left illustrates the proportion of AI-related news headlines across eight dominant themes by media leanings. Left-leaning outlets tend to focus more on safety and risk topics, adopting a cautious perspective. In contrast, right-leaning media emphasize AI stocks, suggesting a more pragmatic or neutral attitude toward AI.\nThe graph on the right analyzes the trend of fear-mongering headlines over time among media with varying ideological leanings. The majority of these fear-inducing headlines come from left-leaning outlets.\n\n\n7. AI Leaders Express Safety Concerns but Maintain Positive Outlook\n The graph examines key topics related to prominent AI figures, including Sam Altman, Bill Gates, and Elon Musk. While all three have voiced concerns about AI safety and risks, the majority still maintain a positive outlook toward AI.\n\n\n8. OpenAI Leads AI Coverage, with Google and Microsoft Among Early Adopters\n\nTech companies also play a key role in shaping AI coverage. Open AI continues to capture media spotlight, but numerous competitors have emerged. Among tech giants, Google and Microsoft have been early adopters in AI development, while Amazon entered the AI field at a later stage.\n\n\n9. Big Tech Dominates AI Competition, While OpenAI Leads Among Startups\n The majority of news stories related to AI competition involve big tech companies. In the smaller segment focusing on startups, OpenAI is at the center of competition narratives.\n\n\n10. Open-Source AI Hype Persists Despite Early Concerns Over Security and Misuse\n\nThere’s been consistent hype surrounding open-source AI. Initially, news about open-source AI could trigger fear, but this cautionary coverage has decreased over time. The smaller portion of news stories that raises concerns about open-source AI focuses on potential risks like security vulnerabilities, large-scale misuse, and worries about intellectual property and control."
  },
  {
    "objectID": "dataviz/tidytuesday/index.html",
    "href": "dataviz/tidytuesday/index.html",
    "title": "TidyTuesday Data Visualizations",
    "section": "",
    "text": "TidyTuesday is a weekly data project that encourages data enthusiasts to practice their skills in data manipulation, visualization, and analysis using R. Each week, a unique dataset is provided, allowing participants to explore a variety of topics and hone their R programming skills. Below, you’ll find a collection of my TidyTuesday visualizations, each accompanied by a brief description and a link to the corresponding code."
  },
  {
    "objectID": "dataviz/tidytuesday/index.html#tidytuesday-visualizations",
    "href": "dataviz/tidytuesday/index.html#tidytuesday-visualizations",
    "title": "TidyTuesday Data Visualizations",
    "section": "TidyTuesday Visualizations",
    "text": "TidyTuesday Visualizations\n\nTech Company Stock Prices (02/07/2023)\n\n\n\nTech Company Stock Prices\n\n\nHighlights:\n\nExplored stock price trends and volatility for major tech companies.\nVisualized trading volumes alongside price movements to show market activity.\n\nThe dataset focused on daily stock prices and trading volume for 14 major tech companies. Code here.\n\n\nHollywood Age Gap (02/14/2023)\n\n\n\nHollywood Age Gap\n\n\nHighlights:\n\nAnalyzed the age difference between male and female love interests in movies.\nVisualized trends over time to show how the age gap in Hollywood has evolved.\n\nThe dataset for this week focused on the age gap between movie love interests. Code here.\n\n\nBob Ross Paintings Analysis (02/21/2023)\n\n\n\nBob Ross Paintings\n\n\nHighlights:\n\nExplored the most frequently used colors in Bob Ross’s paintings from the 1990s.\nExperimented with waffle charts as a new visualization technique.\n\nThe dataset for this week focuses on the paintings featured in Bob Ross’s television show, The Joy of Painting. Code here.\n\n\nAfrican Language Sentiments (02/28/2023)\n\n\n\nAfrican Language Sentiments\n\n\nHighlights:\n\nVisualized sentiment proportions across African languages using a unique measuring cup design.\nAdjusted the positioning and size of the cup to ensure accurate alignment with sentiment proportion markings.\n\nThe dataset focused on sentiment analysis in African languages. Code here.\n\n\nEuropean Drug Development Timeline (03/14/2023)\n\n\n\nEuropean Drug Development Timeline\n\n\nHighlights:\n\nAnalyzed the development timelines of European drugs, with a specific focus on COVID-19 treatments.\nFound that COVID-19 drugs were expedited to market significantly faster than non-COVID drugs.\nEnhanced the visualization with a magnifying glass icon to draw attention to this key finding.\n\nThe dataset was scraped by Miquel Anglada Girotto from the European Medicines Agency and focuses on the development of European drugs. Code here.\n\n\nHaunted Places Sentiment Analysis (10/10/2023)\n Highlights:\n\nCreated a beeswarm plot to represent the distribution of fear sentiment across various haunted locations.\nDiscovered that the DoubleTree Hotel in Spokane and the Civil War Cemetery in Seattle have the highest fear sentiment scores.\n\nThe dataset focused on haunted locations and their associated fear sentiment scores. Using a basic sentiment analysis, I scored each location from 0 to 1, with 1 indicating intense fear sentiment. The beeswarm plot provides a fresh twist on traditional scatter plots, effectively showing the distribution of fear levels across locations. Check out the code for this project here."
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html",
    "href": "dataviz/tree-gunshot/index.html",
    "title": "The Calendar of Gun Violence",
    "section": "",
    "text": "Despite being a small city, Washington, D.C. has the highest homicide rate among all U.S. states, with 226 deaths and 1330 emergency department visits due to gunshot wounds in 2021. As part of its efforts to combat gun violence, the city has adopted innovative strategies and technologies, including ShotSpotter which is a gunshot detection system that uses acoustic sensors to identify and locate gunfire in real-time.\nIn this blog post, we will examine 2021 gunshot data in D.C. and explore how data visualization can provide deeper insights into gun violence in the city. All code can be found here."
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html#data-cleaning",
    "href": "dataviz/tree-gunshot/index.html#data-cleaning",
    "title": "The Calendar of Gun Violence",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nSuppose that we are interested in the temporal patterns of gun violence in D.C., one common way to visualize time series data is through line charts. These charts can break down time points into intervals, such as months, to observe how the values of gun violence incidents change over time. While this approach offers valuable insight into high-level temporal trends, it has one limitation: it only allows us to focus on one time interval at a time. For example, we can only see the temporal changes over months OR weeks, but not both intervals together.\nInspired by GitHub’s contribution graph, we can use geom_tile() to create a similar calendar graph that effectively visualizes gun violence incidents. To situate a calendar within a data frame, we can observe clear parallels: the week of the month corresponds to the row number, the day of the week represents the column number, and each month is treated as a separate facet.\nOne challenge here is to figure out the week for each month. Unfortunately, week() in the lubridate package only returns the week for the year, requiring us to create a week incrementer to manually calculate the week for each month. In other words, we increment the week counter by 1 when we encounter a “Sunday” or when the day is the first of the month.\nMoving forward, we can consider whether to treat gunshot incidents as a continuous variable. Upon plotting the data distribution, it becomes apparent that the distribution is highly right-skewed, which means treating it as continuous would not allow for clear differentiation of color in the legend. Therefore, I categorize gunshot incidents to represent low, medium, and high levels of gun violence.\n```{r}\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(tidyverse)\ngunshot &lt;- read_csv(\"Shot_Spotter_Gun_Shots.csv\")\n\n# data cleaning\ngunshot_daily &lt;- gunshot %&gt;%\n  mutate(date = as_date(DATETIME),\n         year = year(date)) %&gt;%\n  filter((year == 2021) & (TYPE %in% c(\"Single_Gunshot\", \"Multiple_Gunshots\", \"Multiple Gunshots\", \"Single Gunshot\"))) %&gt;%\n  group_by(date) %&gt;%\n  summarise(shots = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(week_day = str_sub(weekdays(date), 1, 3),\n         month_day = day(date),\n         month = month(date),\n         week_start = ifelse(month_day == 1 | week_day == \"Sun\", 1, 0)) %&gt;% # set up when to increment the week\n  group_by(month) %&gt;%\n  mutate(week = cumsum(week_start),\n         month_name = months(date)) %&gt;%\n  ungroup() %&gt;%\n  mutate(shots_range = case_when(shots &lt;= 10 ~ \"1\",\n                                 shots &gt; 10 & shots &lt;= 20 ~ \"2\",\n                                 shots &gt; 20 ~ \"3\"))\n\nweek_day_code &lt;- c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\ngunshot_daily$week_day &lt;- factor(gunshot_daily$week_day, levels = week_day_code)\nmonth_code &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\") \ngunshot_daily$month_name &lt;- factor(gunshot_daily$month_name, levels = month_code)\n```"
  },
  {
    "objectID": "dataviz/tree-gunshot/index.html#data-visualization",
    "href": "dataviz/tree-gunshot/index.html#data-visualization",
    "title": "The Calendar of Gun Violence",
    "section": "Data Visualization",
    "text": "Data Visualization\nNow, let’s visualize the data! Interestingly, the part I spent most time upon was making the month names appear above the weekday names, as they were initially positioned the opposite way. To resolve this issue, we have the option to adjust either the strips (weekday names) or the axes (month names). I decided to go for the former route by increasing the bottom parameter with margin(b=25), which enabled the weekday names to move upwards until they were placed above the month names.\n```{r}\n# theme\nfont_add_google(\"Pragati Narrow\")\nshowtext_auto()\n\n\n# customize theme\ntheme_set(theme_minimal(base_family = \"Pragati Narrow\"))\n\ntheme_update(\n  # legend\n  legend.title = element_blank(),\n  legend.position = 'bottom',\n  legend.direction = 'horizontal',\n  legend.key.width = unit(1.5, \"cm\"),\n  legend.text = element_text(color = \"black\",  size=35),\n  legend.box.margin = margin(t = 35),\n  legend.spacing.x = unit(1, \"cm\"),\n  legend.spacing.y = unit(0.5, \"cm\"),\n  \n  # axis\n  axis.text.y = element_blank(),\n  axis.text.x = element_text(vjust = 50),\n  text = element_text(size = 40),\n  strip.text.x = element_text(size = 43, margin = margin(b = 25)),\n  \n  # titles\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  plot.margin = margin(20, 50, 20, 50),\n  plot.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  panel.background = element_rect(fill = \"#f5f5f2\", color = NA), \n  \n  #legend.title.align=1,\n  plot.title = element_text(\n    color = \"black\", \n    size = 70, \n    face = \"bold\",\n    margin = margin(t = 10),\n    hjust = 0.5\n  ),\n  plot.subtitle = element_text(\n    color = \"grey10\", \n    size = 45,\n    lineheight = 3,\n    margin = margin(t = 5, b = 30),\n    hjust = 0.5\n  ),\n  plot.title.position = \"plot\",\n  plot.caption.position = \"plot\",\n  plot.caption = element_text(\n    color = \"grey20\", \n    size = 35,\n    lineheight = 0.5, \n    hjust = 0.5,\n    margin = margin(t = 30))\n)\n\n\ngunshot_daily %&gt;%\n  ggplot(aes(x = week_day, y = week)) +\n  geom_tile(aes(fill = shots_range), color = \"white\") + \n  scale_fill_manual(values = MetBrewer::met.brewer(\"Tam\", n=3),\n                    labels = c(\"below 10\", \"10 to 20\", \"over 20\"),\n                    guide = guide_legend(label.position = \"bottom\", nrow = 1)) +\n  facet_wrap(~month_name, scales = \"free\") +\n  scale_y_reverse() +\n  scale_x_discrete(position = \"bottom\") +\n  labs(x = \"\", y = \"\", title = \"GUNSHOT DETECTION CALENDAR\",\n       subtitle = \"Recorded shooting incidents in Washington D.C. during 2021\",\n       caption = str_wrap(\"Data comes from ShotSpotter gunshot detection system. Incidents of probable gunfires and firecrackers are excluded | Visualization by Zhaowen Guo\", width = 300))\n\nggsave(\"dc-gunshot-time.png\", width = 14, height = 14/1.618, units = \"in\")\n```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhaowen Guo",
    "section": "",
    "text": "I am a social and data scientist interested in the intersection of experimental design, survey methodology, and machine learning. My research focuses on developing metrics for social constructs using innovative data collection and evaluating the impact of digital interventions on civic behavior and public opinion. I work with diverse data sources—including administrative, survey, text, and spatial data—and collaborate widely with government agencies and tech companies.\nCurrently, I am a postdoctoral researcher at the Better Government Lab, Georgetown University.\n\n\n\n\nExperimental Design Survey Methodology\nMachine Learning\nCausal Inference Natural Language Processing Data Visualization"
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html",
    "href": "statistics/dssg-tracking-poverty/index.html",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "",
    "text": "Poverty is a complex issue that is typically measured at the household level, taking into account the resources that are shared among multiple individuals. However, many existing studies rely solely on individual-level survey data to estimate poverty rates. This approach can lead to inaccurate estimates, particularly in cases where individuals within a household have differing levels of access to resources.\nThe use of administrative data is a promising approach to measuring poverty, as it provides a wealth of information on individual addresses. By aggregating information across individuals who are more likely to belong to the same household, we can develop more accurate estimates of poverty and better understand the ways in which poverty affects families and communities.\nIn our 2022 Data Science for Social Good (DSSG) project, we used the Washington Merged Longitudinal Administrative Dataset (WMLAD) to predict household groupings in the Washington State. This database merges administrative data from various state agencies including the Employment Security Department (ESD), Department of Social and Health Services (DSHS), Department of Health (DOH), Secretary of State, Department of Licensing (DOL), and WA State Patrol, covering the period from 2010 to 2017. Each time an individual interacts with any of these agencies, they are assigned an anonymized address code, which is further augmented by an imputation algorithm developed by previous WMLAD users (more details here).\nIn this blog post, I will introduce a longitudinal approach that I proposed for our team to improve the imputed addresses and construct more reliable household groupings. Don’t forget to also check out our presentation and media coverage :)"
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-1-lack-of-interactions",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-1-lack-of-interactions",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 1: Lack of interactions",
    "text": "Scenario 1: Lack of interactions\nThis occurs when certain household members, especially children, have less interactions with government agencies than adults, resulting in fewer recorded addresses for imputation. For example, addresses for children may only be recorded when they register to vote or obtain a driver’s license, which implies that the observation of some one-person residences may simply be due to the absence of their children in previous records.\nTo address this circumstance, I recommended adjusting certain one-person residences to reflect their actual co-residence with children. For example, in the scenario illustrated below, a child (purple face) began to co-reside with others (blue face) in month 4, while the address records showing only one person living at that address previously. In such cases, we need to reflect their co-residence accurately."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-2-imputation-error",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-2-imputation-error",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 2: Imputation error",
    "text": "Scenario 2: Imputation error\nAnother pattern that drew my attention was frequent movements into and out of the same place, which may indicate an imputation error.\nA plausible fix was in cases where we observe a person living alone and co-residing with others alternately at the same address, we could adjust the one-person residences to reflect the nearest co-residence that the individual belongs to."
  },
  {
    "objectID": "statistics/dssg-tracking-poverty/index.html#scenario-3-move-in-and-out",
    "href": "statistics/dssg-tracking-poverty/index.html#scenario-3-move-in-and-out",
    "title": "Predicting Household Groupings: A Longitudinal Approach",
    "section": "Scenario 3: Move in and out",
    "text": "Scenario 3: Move in and out\nThe third scenario is more complex. One-person residences may occur during a transition period when a multi-person residence moves out simultaneously to a new location, but their addresses are not updated accordingly.\nIn cases where we observe distinct co-residence members before and after a one-person residence at a particular address, and the prior co-residence appears again in the future, we adjust the one-person residence to reflect the subsequent co-residence. As depicted in the illustration below, we should only modify one-person residences in month 3 when the co-residence composition in month 2 and month 4 matches.\n\n\nModifying one-person residences that fall under these scenarios did not conclude the analysis, as it could result in duplicates where a person appears at multiple addresses during the same time period. Therefore, we should eliminate duplicates based on recorded addresses, rather than imputed ones, which provide a more reliable indication of the address that the person belongs to.\nHow effective is this algorithm? After implementing the modifications, the percentage of one-person residences decreased from 38% to 31%, bringing it much closer to the census record of 27%. We are currently developing a methodology paper that details this approach along with more real-world data applications. We anticipate that our methodology and resulting household groupings will be beneficial in addressing significant policy questions."
  },
  {
    "objectID": "statistics/rasch-hlm/index.html",
    "href": "statistics/rasch-hlm/index.html",
    "title": "Unveilling Model Equivalence: Linking the Rasch Model with the Hierarchical Linear Model",
    "section": "",
    "text": "When learning more about statistical modeling, I find it helpful to consider the relationships and connections between different models. This allows me to gain a better understanding of the various approaches to model fitting and their respective advantages and disadvantages. In this blog post, we will explore the connection between two commonly used models in psychometrics: the Rasch model and the hierarchical linear model. For those interested, I have also included links to my slides and code.\nThe Rasch model, also known as the one-parameter logistic model (1PL), is a statistical model used to analyze responses to rating scale items. It is based on the idea that the probability of a person getting an item correct depends on the person’s ability and the difficulty of the item. Mathematically, the Rasch model can be expressed as follows.\n\\[\nP_i(X_{ij} = 1|\\theta_j, b_i) = \\frac{e^{(\\theta_j - b_i)}}{1 + e^{(\\theta_j - b_i)}}\n= \\frac{1}{1 + e^{-(\\theta_j - b_i)}}\n\\] where i means respondents, j represents items, \\(X_ij\\) refers to response of person j to item i and takes a value of 0 or 1, \\(\\theta_j\\) corresponds to ability for person j, and \\(b_i\\) is the difficulty parameter for item i.\nIntuitively, we can recover the hierarchical structure in item responses by treating persons as the higher level (level-2) and items as the lower level (level-1). This data structure has two characteristics that our model needs to capture:\n\nrepeated measures nested within each person\ninterdependent item responses from the same person\n\nNow let’s rewrite the Rasch model into two levels where the level 1 is the item level and level 2 is the person level.\nApplying logit link function, we have \\[\n\\begin{aligned}\nlog(\\frac{p_{ij}}{1 - p_{ij}}) &= \\beta_{0j} + \\beta_1X_{1ij} + ...+\\beta_{(k-1)j}X_{(k-1)ij} \\\\\n&= \\beta_0j + \\sum_{q=1}^{k-1}\\beta_{qj}X_{qij}\n\\end{aligned}\n\\] where q=1,…,k-1 since the dummy variable for the reference item is dropped, \\(X_ij\\) is the \\(i^{th}\\) term dummy indicator for person j, \\(\\beta_{0j}\\) is an intercept term of the expected effect of the reference item for person j, and \\(\\beta_{qj}\\) is the difference of effect for item q from \\(\\beta_{0j}\\).\nNow let’s turn to level 2, the person level. The basic idea is to introduce randomness to the reference term \\(\\beta_{0j}\\) in level 1, which means we want it to vary across persons. We don’t want to introduce randomness to the other \\(\\beta\\) terms because items answered by the same person j are interdependent. Therefore, level 2 model can be written as below:\n\\[\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\color{red}{u_{0j}} \\\\\n\\beta_{1j} &= \\gamma_{10} \\\\\n...\\\\\n\\beta_{(k-1)j} &= \\gamma_{(k-1)0}\n\\end{aligned}\n\\] The magic appears when we combine level-1 and level-2 models:\n\\[\nP_{ij} = \\frac{1}{1 + exp{-[\\color{red}{u_{0j}} - (\\color{blue}{-\\gamma_{i0} - \\gamma_{00}}})]}\n\\] which looks equivalent to the Rasch model we saw before\n\\[\nP_i(X_{ij} = 1 |\\theta_j, b_i) = \\frac{1}{1 + e^{(-\\theta_j - b_i)}} = \\frac{1}{1 + exp[-(\\color{red}{\\theta_j} - \\color{blue}{b_i})]}\n\\] where \\[\n\\theta_j = u_{0j} \\\\\nb_i = -\\gamma_{i0} - \\gamma_{00}\n\\] What can we learn from this model equivalence? One potential application is that we can now apply both the mirt package and the lme4 package, commonly used for fitting hierarchical models, to fit the Rasch model. The following example, which uses simulated LSAT data, demonstrates how this approach can be implemented.\nWhen using the mirt package, our item response data is typically organized such that each row represents a respondent, while each column represents a question. We can derive both person and item parameters by employing the following code:\n```{r}\n# fit a Rasch model\nrasch &lt;- mirt(data  = lsat,\n              model = 1,\n              itemtype = \"Rasch\",\n              SE = TRUE)\n\n# retrieve item parameter estimates\nrasch_coef &lt;- coef(rasch, IRTpars=TRUE, simplify=TRUE)\n\n# retrieve person parameter estimates\nrasch_fs &lt;- fscores(rasch, full.scores.SE = TRUE)\n```\nWe can also fit the same model using the lme4 package, although we must first restore its hierarchical data structure. Specifically, we need to convert the wide-format data into long-format data, where the first column denotes the individuals, the second column denotes the items, and the third column records the item responses.\n```{r}\n# reshape the data into a long format \nlsat_long &lt;- lsat %&gt;%\n  mutate(ID = row_number()) %&gt;%\n  pivot_longer(!ID, names_to = \"items\", values_to = \"responses\")\n\n# fit the model \nmlm &lt;- glmer(responses ~ -1 + items + (1|ID), \n             family = \"binomial\", \n             data = lsat_long, \n             control = control)\n```\nWe can now compare the estimates of item difficulty between the two approaches. The item difficulty estimates derived via the mirt package, represented by the b parameter, are compared with the fixed effects estimated via the lme4 package. One thing to note here is that fixed effects denote item easiness (because \\(b_i = \\gamma_{i0} - \\gamma_{00}\\)) and thus we need to invert the sign to accurately reflect item difficulty.\n```{r}\n# compare item estimates\ncoef(summary(mlm))[,1]\nrasch_items$b\ncor(coef(summary(mlm))[,1], rasch_items$b) # -1\n\n# compare person estimates\nunlist(ranef(mlm))\nfscores(rasch)\ncor(unlist(ranef(mlm)), fscores(rasch)) # 1\n\n```\nFor more details, please refer to Kamata (2001) which provides further insight into how this model equivalence can be extended to a third level."
  },
  {
    "objectID": "statistics/svd/index.html",
    "href": "statistics/svd/index.html",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "",
    "text": "SVD is not nearly as famous as it should be. — Gilbert Strang\nSVD is a great 1-stop shop for data analysis. — Daniela Witten"
  },
  {
    "objectID": "statistics/svd/index.html#introduction",
    "href": "statistics/svd/index.html#introduction",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Introduction",
    "text": "Introduction\nSingular Vector Decomposition (SVD) is a matrix factorization technique that has become a cornerstone in the field of machine learning (ML). It not only allows for efficiently calculating the inverse of a matrix (if it exists) by multiplying the inverse of each decomposed simpler matrices, but also opens the door to a wide array of applications in ML and beyond.\nIn what follows, I will start by the definition and properties of SVD, and establish its connection with Principal Component Analysis (PCA). Then I will demonstrate different applications of SVD in ML, including but not limited to missing value imputation and latent feature extraction.\nDefinition and properties of SVD\nSVD decomposes a data matrix \\(X_{m \\times n}\\) into three matrices \\(U_{m\\times r}\\), \\(D_{r\\times r}\\), and \\(V_{n\\times r}\\), regardless of the characteristics of the original matrix.\n\\[\nX = UDV^T\n\\] where\n\nU and V are orthogonal matrices (\\(U^T U = I\\) and \\(V^T V = I\\)), which are called left singular vector, and right singular vector, respectively\nD is a diagonal matrix with non-negative and decreasing elements, which are called singular values\n\n\nLet’s first check dimensions of the resulting matrices after applying SVD to a toy matrix X.\n\n# Define a matrix\nX &lt;- matrix(c(1:12),\n            nrow = 4,\n            ncol = 3,\n            byrow = T)\n\n# Apply SVD\nsvd_result &lt;- svd(X)\n\n# Extract U, D, and V matrices\nU &lt;- svd_result$u\nD &lt;- diag(svd_result$d)\nV &lt;- svd_result$v\nprint(paste0(\"The dimension for U matrix: \", dim(U)[1], \" X \", dim(U)[2]))\n\n[1] \"The dimension for U matrix: 4 X 3\"\n\nprint(paste0(\"The dimension for D matrix: \", dim(D)[1], \" X \", dim(D)[2]))\n\n[1] \"The dimension for D matrix: 3 X 3\"\n\nprint(paste0(\"The dimension for V matrix: \", dim(V)[1], \" X \", dim(V)[2]))\n\n[1] \"The dimension for V matrix: 3 X 3\"\n\n\nWe can then check matrix properties of SVD. As we can observe, matrices U and V are orthogonal, and matrix D is diagonal.\n\n# Check properties of U and V (orthogonal matrix)\nis_orthogonal &lt;- function(A){\n  A_T &lt;- t(A)\n  dot_product_1 &lt;- A %*% A_T\n  dot_product_2 &lt;- A_T %*% A\n  identity_matrix_1 &lt;- diag(nrow(A))\n  identity_matrix_2 &lt;- diag(ncol(A))\n  \n  result &lt;- isTRUE(all.equal(dot_product_1, identity_matrix_1)) +\n            isTRUE(all.equal(dot_product_2, identity_matrix_2)) # all.equal checks \"nearly equal\"\n  \n  return(result&gt;=1)\n}\n\nis_orthogonal(U) # TRUE\n\n[1] TRUE\n\nis_orthogonal(V) # TRUE\n\n[1] TRUE\n\n# Check properties of D\ndiag(D) # diagonal values (or singular values in this case)\n\n[1] 2.546241e+01 1.290662e+00 2.311734e-15\n\nD[!row(D) == col(D)] # off-diagonal values are 0\n\n[1] 0 0 0 0 0 0\n\n\nConnection between SVD and PCA\nNow, knowing that SVD can be used to approximate any matrix, it’s an opportune moment to revisit Principal Component Analysis (PCA), an unsupervised ML method that we might be more familiar with. As we will see, SVD on a de-meaned (centered) data matrix is the same as PCA.\nRecall that PCA seeks to find principal components, or the direction in the feature space with maximum variance in the data.\n\n# Center the data matrix (column means are 0)\nX_centered &lt;- scale(X, center = T, scale = T)\ncolMeans(X_centered) # check if centered\n\n[1] 0 0 0\n\n# Apply SVD to the centered data matrix\nsvd_result &lt;- svd(X_centered)\n\n# Apply PCA to the data\npca_result &lt;- prcomp(X, scale. = T)\n\nAs we can see, columns of the right singular vector V correspond to principal components extracted from PCA, and SVD also yields less elapsed time than PCA. A key advantage of SVD is that it does not require a preliminary step of constructing a covariance as PCA does, providing greater computational efficiency in extracting principal components.\nThis efficiency becomes particularly prominent when handling\n\nHigh-dimensional datasets: when a data matrix possess too many features, the computational cost for constructing its covariance matrix can be huge\nFull-rank data matrix: when the data matrix is full-rank, it often implies that many singular values will be non-negligible, and many principal components will be needed to reconstruct the original matrix\n\n\nprint(svd_result$v) # right singular vectors\n\n          [,1]       [,2]       [,3]\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n\nprint(pca_result$rotation) # principal components\n\n           PC1        PC2        PC3\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n\n\n\n# Construct a high-dimensional and sparse data matrix\nn_rows &lt;- 1000\nn_cols &lt;- 500\n\nsparse_matrix &lt;- matrix(0, nrow = n_rows, ncol = n_cols)\n\n# Manually add some non-zero elements to mimic sparsity\nset.seed(123)\nnon_zero_elements &lt;- 200\nfor (i in 1:non_zero_elements) {\n  row_index &lt;- sample(n_rows, 1)\n  col_index &lt;- sample(n_cols, 1)\n  sparse_matrix[row_index, col_index] &lt;- runif(1)\n}\n\n\n# Compute every possible rank approximations\nsystem.time({\n  svd_result &lt;- svd(sparse_matrix)\n})\n\n   user  system elapsed \n   0.19    0.00    0.58 \n\nsystem.time({\n  pca_res &lt;- prcomp(sparse_matrix)\n})\n\n   user  system elapsed \n   0.34    0.00    0.72 \n\n\n\n# Compute top 10 rank approximations\nsystem.time({\n  svd_result &lt;- irlba(sparse_matrix, nv = 10)\n})\n\n   user  system elapsed \n   0.02    0.00    0.03 \n\nsystem.time({\n  pca_res &lt;- prcomp(sparse_matrix, rank. = 10)\n})\n\n   user  system elapsed \n   0.39    0.00    0.61"
  },
  {
    "objectID": "statistics/svd/index.html#application-impute-missing-values",
    "href": "statistics/svd/index.html#application-impute-missing-values",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Application: Impute Missing Values",
    "text": "Application: Impute Missing Values\nOne popular application of SVD is to impute missing values. Without keeping all singular values and vectors, we can just retain the first d largest singular values to approximate the matrix A. The intuition is that the approximated matrix \\(A_d\\) being a dense matrix that captures the primary structure and patterns in the original data.\nThis procedure is also called lower-rank approximation, which can be implemented in the following steps:\n\nMatrix approximation: fill in NAs with an initial guess (e.g. column means, zeros) and apply SVD with rank d, meaning that we only keep top d singular values and vectors\nMissingness imputation: use the approximated matrix \\(A_d\\) to fill in NAs in the original matrix\n\nLet’s use the following example for illustration:\nWe start by creating a toy data matrix A and call it our ground truth matrix. Then we manually add sparsity by replacing certain elements with NAs.\n\n# Create a toy dataset (sparse matrix)\nset.seed(123)\nA &lt;- matrix(sample(c(NA, 1:5), 25, replace = T), 5, 5)\nground_truth_matrix &lt;- A\nA[c(2, 8, 10, 14, 20)] &lt;- NA\n\n\nA\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]   NA    2   NA    2    4\n[3,]    2   NA    1   NA    2\n[4,]    1    3   NA    3    1\n[5,]    1   NA    4   NA    1\n\nground_truth_matrix\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]    5    2   NA    2    4\n[3,]    2    4    1   NA    2\n[4,]    1    3    2    3    1\n[5,]    1    5    4   NA    1\n\n\nNext, we apply SVD with varying d, which indicates the number of singular values/vectors.\n\n# Define svd\nimpute_svd &lt;- function(matrix, d){\n  \n  # fill in missingness with column means\n  column_means &lt;- colMeans(matrix, na.rm = T)\n  matrix_filled &lt;- matrix\n  na_indices &lt;- is.na(matrix)\n  matrix_filled[na_indices] &lt;- column_means[col(matrix)[na_indices]]\n  \n  # perform svd\n  svd_res &lt;- svd(matrix_filled)\n  svd_res$d &lt;- c(svd_res$d[1:d], rep(0, length(svd_res$d) - d))\n  \n  # reconstruct the matrix\n  approx_matrix &lt;- svd_res$u %*% diag(svd_res$d) %*% t(svd_res$v)\n  imputed_vals &lt;- approx_matrix\n  imputed_vals[!is.na(matrix)] &lt;- NA\n  return(imputed_vals)\n}\n\nWe can use the approximated matrix \\(A_d\\) to reconstruct the original matrix and impute missing values. We can evaluate the performance of missingness imputation by mean squared error (MSE).\n\n# Construct the metric MSE\nmse &lt;- function(predicted, truth) {\n  mean((predicted - truth)^2, na.rm = TRUE)\n}\n\n# Display MSE for different d values (rank, or number of dimensions to define the reduced matrix)\nsvd_errors &lt;- numeric(5)\nfor (d in 1:5) {\n  imputed_values &lt;- impute_svd(A, d)\n  svd_errors[d] &lt;- mse(imputed_values, ground_truth_matrix)\n}\n\nHow does SVD perform? As a baseline, consider a simple approach by replacing missing values with column means. It seems that rank-2 approximation is an optimal choice, which yields the lowest MSE. However, it’s important to note that it is not always the case that SVD approximation would outperform simple column mean imputation. We might need to consider other matrix decomposition techniques for missingness imputation, such as Non-negative Matrix Factorization (NMF), Alternating Least Squares (ALS), etc..\n\n# Create baseline imputation from column means\nna_indices &lt;- is.na(A)\ncolmean_matrix &lt;- A\ncolmean_matrix[na_indices] &lt;- colMeans(A, na.rm = T)[col(A)[na_indices]]\ncolmean_errors &lt;- mse(colmean_matrix[na_indices], ground_truth_matrix[na_indices])\n\n# Report comparison of performance\ncomparison &lt;- tibble(\"Method\" = c(\"Column Means\", \n                                  \"Rank-1 Approximation\",\n                                  \"Rank-2 Approximation\",\n                                  \"Rank-3 Approximation\",\n                                  \"Rank-4 Approximation\",\n                                  \"Rank-5 Approximation\"),\n                     \"MSE\" = c(colmean_errors, svd_errors))\n\ncomparison %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\nMethod\nMSE\n\n\n\nColumn Means\n4.312500\n\n\nRank-1 Approximation\n4.754839\n\n\nRank-2 Approximation\n4.094591\n\n\nRank-3 Approximation\n4.146353\n\n\nRank-4 Approximation\n4.319335\n\n\nRank-5 Approximation\n4.312500\n\n\n\n\n\nIn line with the idea of missingness imputation, SVD can also be leveraged to enhance recommendation systems! The goal is to predict unknown preferences or ratings of users for items (e.g. movies, products, or services) based on existing ratings. A notable example is Netflix Prize competition, where Netflix offered $1 million award to anyone who could improve the accuracy of its movie recommendation system by 10%. The winning team just used SVD, along with techniques that incorporate other metadata, achieving a 10.06% improvement!"
  },
  {
    "objectID": "statistics/svd/index.html#application-topic-modeling",
    "href": "statistics/svd/index.html#application-topic-modeling",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Application: Topic Modeling",
    "text": "Application: Topic Modeling\nSVD is a powerful and generalizable technique that provides us another perspective on topic modeling. We begin by first transforming documents into a document-term matrix, where each row represents a document, each column reflects a term, and each cell denotes frequency. To refine this step further, we can also apply Term Frequency-Inverse Document Frequency (TF-IDF) to reweigh the cell values, adjusting for the uniqueness of each term for a given document.\nSVD can then be perceived as decomposing a document-term matrix \\(X_{m \\times n}\\) into\n\n\\(U_{m \\times r}\\): document-topic matrix\n\\(D_{r \\times r}\\): diagonal elements represent topic importance\n\\(V_{n \\times r}\\): term-topic matrix\n\nFor topic modeling, a crucial hyperparameter that requires tuning is the number of topics (often denoted by k). In the context of SVD, the idea is equivalent to selecting the top k singular values and their corresponding singular vectors in order to approximate the original data matrix.\n\n# Construct a document-term matrix\nlibrary(tidytext)\nlibrary(tm)\ndocuments &lt;- tibble(\n  doc_id = 1:8,\n  text = c(\"The sky is blue and beautiful.\",\n           \"Love this blue and beautiful sky!\",\n           \"The quick brown fox jumps over the lazy dog.\",\n           \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n           \"I love green eggs, ham, sausages, and bacon!\",\n           \"The brown fox is quick and the blue dog is lazy!\",\n           \"The sky is very blue and the sky is very beautiful today\",\n           \"The dog is lazy but the brown fox is quick!\")\n)\n\ntidy_documents &lt;- documents %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\ndtm &lt;- tidy_documents %&gt;%\n  count(doc_id, word) %&gt;%\n  cast_dtm(doc_id, word, n)\n\n\n# Apply SVD and examine each decomposed matrix\nsvd_result &lt;- svd(as.matrix(dtm))\n\nk &lt;- 2 # choose k=2 for simplicity\nUk &lt;- svd_result$u[, 1:k]\nDk &lt;- svd_result$d[1:k]\nVk &lt;- svd_result$v[, 1:k]\n\nAs we can see, the decomposed \\(U_k\\) matrix captures documents by topics.\n\nUs &lt;- tibble(`Document ID` = 1:8,\n             `Topic 1` = Uk[,1],\n             `Topic 2` = Uk[,2])\nUs %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\nDocument ID\nTopic 1\nTopic 2\n\n\n\n1\n-0.1294362\n0.4303175\n\n\n2\n-0.1392703\n0.4926330\n\n\n3\n-0.5597761\n-0.1933906\n\n\n4\n-0.0088357\n0.2551944\n\n\n5\n-0.0175544\n0.2534139\n\n\n6\n-0.5889438\n-0.0537521\n\n\n7\n-0.1672636\n0.6091753\n\n\n8\n-0.5246738\n-0.1772371\n\n\n\n\n\nThe singular values are stored in the following matrix \\(D_k\\), which correspond to how important each topic is.\n\nD_matrix &lt;- diag(Dk)\nrownames(D_matrix) &lt;- c(\"Topic 1\", \"Topic 2\")\ncolnames(D_matrix) &lt;- c(\"Topic 1\", \"Topic 2\")\n\nD_matrix %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nTopic 1\nTopic 2\n\n\n\nTopic 1\n3.993368\n0.000000\n\n\nTopic 2\n0.000000\n3.460071\n\n\n\n\n\nThe \\(V_k\\) matrix represents terms by topics.\n\nterms &lt;- colnames(dtm)\nV_matrix &lt;- tibble(`Term` = terms,\n                   `Topic 1` = Vk[,1],\n                   `Topic 2` = Vk[,2])\n\nV_matrix %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\nTerm\nTopic 1\nTopic 2\n\n\n\nbeautiful\n-0.1091735\n0.4428019\n\n\nblue\n-0.2566540\n0.4272669\n\n\nsky\n-0.1510589\n0.6188605\n\n\nlove\n-0.0392713\n0.2156161\n\n\nbrown\n-0.4190432\n-0.1226506\n\n\ndog\n-0.4190432\n-0.1226506\n\n\nfox\n-0.4190432\n-0.1226506\n\n\njumps\n-0.1401764\n-0.0558921\n\n\nlazy\n-0.4190432\n-0.1226506\n\n\nquick\n-0.4190432\n-0.1226506\n\n\nbacon\n-0.0066085\n0.1469936\n\n\nbeans\n-0.0022126\n0.0737541\n\n\nbreakfast\n-0.0022126\n0.0737541\n\n\neggs\n-0.0066085\n0.1469936\n\n\nham\n-0.0066085\n0.1469936\n\n\nking's\n-0.0022126\n0.0737541\n\n\nsausages\n-0.0066085\n0.1469936\n\n\ntoast\n-0.0022126\n0.0737541\n\n\ngreen\n-0.0043959\n0.0732395\n\n\n\n\n\nNow, we can examine top 5 terms associated with each topic.\n\ntop_terms &lt;- apply(Vk, 2, function(x) terms[order(abs(x), decreasing = TRUE)[1:5]])\nprint(top_terms)\n\n     [,1]    [,2]       \n[1,] \"brown\" \"sky\"      \n[2,] \"dog\"   \"beautiful\"\n[3,] \"fox\"   \"blue\"     \n[4,] \"lazy\"  \"love\"     \n[5,] \"quick\" \"bacon\"    \n\n\nBeyond what has been discussed, some other cool applications of SVD in NLP include: information retrieval via Latent Semantic Analysis and word co-occurrence detection in word embeddings and other downstream tasks (e.g. text classification). Feel free to explore!\nReferences and additional resources:\n\nA wonderful twitter thread on SVD by Daniela Witten (a nice summary can be found here)\nA cool geometric interpretation of SVD\nA nice tutorial illustrating the connection between SVD and topic modeling using Python"
  },
  {
    "objectID": "statistics/weights-definition/index.html",
    "href": "statistics/weights-definition/index.html",
    "title": "Weights in Statistics: What Do People Often Get Wrong?",
    "section": "",
    "text": "Survey weighting is a mess. — Andrew Gelman\nRecently, many clients have come to us with questions about weights – how to create weighted statistics? how to conduct weighted regressions? When I ask about the specific weights they’re referring to, many seem unsure. This confusion underscores the complexity of discussions surrounding weights. Motivated by these encounters, I started looking into different types of weights, and what weighting can and cannot do. In this blog post, I will address common misconceptions surrounding these questions and hopefully bring clarity to how weights should be understood and used in statistical practices."
  },
  {
    "objectID": "statistics/weights-definition/index.html#there-exists-more-than-one-type-of-weights",
    "href": "statistics/weights-definition/index.html#there-exists-more-than-one-type-of-weights",
    "title": "Weights in Statistics: What Do People Often Get Wrong?",
    "section": "There exists more than one type of weights",
    "text": "There exists more than one type of weights\nThe first common misconception is that when people talk about weights in statistics, they are all pointing to the same concept. This is not the case, as highlighted in the insightful discussions by Thomas Lumley and Andrew Gelman. Building upon their work, I will provide visual and code demonstrations below to illustrate the differences between various types of weights.\n\n\nTypes of Weights in Statistics\n\nLumley and Gelman identify four primary types of weights in statistics: sampling weights, precision weights, frequency weights, and importance weights, which are highlighted in yellow boxes in this graph. I included alternative names that are commonly used to describe each type of weights.\nWhat does a weight of 100 mean for each type of weights?\n\nA of 100 indicates that the observation represents 100 individuals in the population. Alternatively, the probability that this observation was selected from the population to the sample is 1/100. This helps to ensure the sample is representative of the population.\nA of 100 means that the observation’s variance is 100 times less than that of an observation with a precision weight of 1. This weighting is used to enhance the estimation efficiency like OLS, where typically each observation is equally weighted. By using precision weights, observations with lower variance (and thus higher precision) have a greater influence on determining the line of best fit.\nA of 100 suggests that there are 100 identical observations in the sample. To save space and processing power, only one value is used to represent these observations.\n\nImportance weights are different from the other types. Lumley provides a good example related to dual-frame sampling. When your sample comes from two overlapping sampling frames, it’s crucial to properly account for individuals who appear in both frames to prevent double-counting. In such cases, importance weights are applied to adjust for the overlap, typically by dividing the original sampling weights by 2 of those duplicated individuals.\nAdditionally, I introduced a fifth category – adjustment weights – marked in an orange box. Adjustment weights are often used to rectify discrepancies between the sample and the population, addressing issues such as respondent attrition and non-response through techniques like post-stratification and raking. You might also have heard of terms like “longitudinal weights” or “cross-sectional weights” which often appeared in public health research. These terms fall under the umbrella of adjustment weights. While adjustment weights typically complement sampling weights, they can extend to non-probability samples as well.\nBelow I turn to each type of weight using the California Academic Performance Index data from the survey package. We’ll work with the apisrs dataset, a simple random sample of 200 schools where sampling weights are included (pw). Our goal is to analyze the relationship between academic performance in the year 2000 (api00) and percentage of economically disadvantaged students (meals).\nSampling weights\nWe incorporate sampling weights pw into our survey design, which serves as the foundation for further analysis.\n\nlibrary(survey)\nlibrary(tidyverse)\ndata(api)\n\n# Set options to adjust for potential issues with single primary sampling units (PSUs)\noptions(survey.lonely.psu = \"adjust\")\noptions(survey.adjust.domain.lonely = TRUE)\n\n# Define the survey design\nsrs.design &lt;- svydesign(id =~ 1, # no clustering (all data has equal PSU)\n                        weights = ~pw, # sampling weights\n                        data = apisrs)\n\n# Run a linear regression with sampling weights\nols_sampling_weights &lt;- svyglm(api00 ~ meals, design = srs.design)\n\nPrecision weights\nWe can manually create precision weights and incorporate them in the weights= argument from the glm() function. The basic idea is that if the variance is heteroscedastic (unequal across observations) and depends linearly on the fitted values, we could use the inverse of these fitted values as weights. Alternatively, if we already have an estimate of the variance for each observation from prior knowledge, we could also directly use those values to compute the inverse variance weights.\n\n# Fit a traditional OLS model\nols &lt;- glm(api00 ~ meals, data = apisrs)\n\n# Estimate variance (assuming variance of residuals might depend on the magnitude of fitted values)\nols_var &lt;- glm(I(residuals(ols)^2) ~ fitted(ols), data = apisrs)\n\n# Compute precision weights as the inverse of the estimated variance\nprecision_weights &lt;- 1 / fitted(ols_var)\n\n# Fit the weighted least squares model with precision weights\nols_precision_weights &lt;- glm(api00 ~ meals, data = apisrs, weights = precision_weights)\n\nFrequency weights\nAssuming we want to store data at the student level rather than the school level, we could generate frequency weights that represent the number of students in each school, and run our analysis on the uncompressed data.\n\nset.seed(123)\n\n# For simplicity, assume the number of students per school ranges from 1 to 3\napisrs$frequency_weights &lt;- sample(1:3, size = nrow(apisrs), replace = TRUE)\n\n# Construct individual-level dataset\napisrs_individual &lt;- apisrs %&gt;%\n  uncount(weights = frequency_weights, .id = \"cds\")\n\n# Run a linear model on the uncompressed data\nols_frequency_weights &lt;- glm(api00 ~ meals, data = apisrs_individual)\n\nAdjustment weights\nAssume that we under-sampled middle schools and high schools, and we want to correct this imbalance and ensure our sample mirrors the known population breakdown by school types, this is where adjustment weights come into play.\n\n# Assume we know population totals for each school type\npop_totals &lt;- data.frame(\n  stype = c(\"E\", \"M\", \"H\"),\n  Freq = c(500, 300, 200)  # hypothetical population sizes for Elementary, Middle, High\n)\n\n# Post-stratification\npost_design &lt;- postStratify(srs.design, strata = ~stype, population = pop_totals)\n\n# Run a linear model with the updated design\nols_adjustment_weights &lt;- svyglm(api00 ~ meals, design = post_design)\n\nIt is worth noting that adjustment weights are still useful even when we don’t have sampling weights. For example, in respondent-driven sampling, where the selection probability of each respondent is unknown and individuals with larger social networks are likely over-represented, we could improve sample representativeness using adjustment weights that are inversely proportional to the number of social connections for each respondent."
  },
  {
    "objectID": "statistics/weights-definition/index.html#non-probability-samples-hold-promise-when-paired-with-careful-weighting",
    "href": "statistics/weights-definition/index.html#non-probability-samples-hold-promise-when-paired-with-careful-weighting",
    "title": "Weights in Statistics: What Do People Often Get Wrong?",
    "section": "Non-probability samples hold promise when paired with careful weighting",
    "text": "Non-probability samples hold promise when paired with careful weighting\nAnother misconception is that many tend to hold a dismissive stance towards non-probability sampling, including American Association of Public Opinion Research (AAPOR) in 2014. A frequently cited example of non-probability sampling’s pitfalls is the 1936 Literary Digest poll, which incorrectly predicted the outcome of the 1936 U.S. presidential election due to its unrepresentative sample of the magazine’s subscribers.\n\n\n1936 Literary Digest\n\nHowever, the benefit of probability samples might not come true for every project. In scenarios where response rates are extremely low, a probability sample might not offer more accuracy than a non-probability convenience sample. Moreover, the application of weighting techniques can significantly enhance the accuracy of non-probability samples, making them viable for reliable estimates.\nSelecting appropriate variables for weighting then becomes crucial. An ideal auxiliary vector should (1) strongly predict the outcome of interest, (2) strongly predict the probability of responding, and (3) correspond to domains of interest (Caughey et al., 2020). For instance, Lohr and Brick’s 2017 analysis demonstrated that using voter data from the 1932 election to weight the Literary Digest’s sample could have accurately predicted Roosevelt’s victory in the 1936 election.\nIt’s also worth mentioning that a well-chosen auxiliary vector can potentially reduce both bias and variance (Little and Vartivarian, 2005), especially when the first two conditions above are met. If achieving both is not possible, it’s generally better to prioritize the former (outcome) over the latter (probability of response).\nNonetheless, identifying a suitable auxiliary vector for weighting is not always straightforward. Consider Gelman’s project which used an opt-in poll from the Xbox gaming platform before the 2012 presidential election. This sample was unrepresentative, skewing towards younger and more male respondents. Gelman and his team chose to use party ID to reweight the data. Partisanship is not often used for weighting because it varies over time and is not readily available from census data where demographic variables are often included for weighting. Despite these challenges, the results are impressive – the adjusted estimates aligned well with forecasts from leading poll analysts!"
  },
  {
    "objectID": "statistics/weights-definition/index.html#weighting-in-regressions-still-needs-careful-considerations",
    "href": "statistics/weights-definition/index.html#weighting-in-regressions-still-needs-careful-considerations",
    "title": "Weights in Statistics: What Do People Often Get Wrong?",
    "section": "Weighting in regressions still needs careful considerations",
    "text": "Weighting in regressions still needs careful considerations\nWhen analyzing survey data, sampling weights and adjustment weights are often discussed. A key question that arises is: when do we need to incorporate weights? One general consensus is that the need for weights depends on the research purpose: weights become more crucial when estimating population descriptive statistics but less so when examining the relationship between predictors and the outcome.\nHowever, even in regression analyses, there are circumstances where unweighted data may be preferred to weighted, and vice versa. Below are some important considerations for deciding whether to incorporate weights:\n\n\nRelationship between weights and the outcome (Y)\n\nIf the sampling weights are dependent on the outcome (Y), such as in case-control sampling, we should incorporate weights. For instance, if high values of Y are more likely to be sampled, omitting weights could introduce bias.\n\n\n\nRelationship between weights and the predictors (X)\n\nWhen sampling weights are a function of the predictors (X) included in the model and does not depend on outcome (Y), unweighted model estimates are often preferable. These estimates tend to be unbiased, consistent, and exhibit smaller standard errors compared to their weighted counterparts.\nEven when weights are associated with the outcome (Y), some researchers still recommend first re-specifying the model to ensure that weights depend solely on the predictors (X). If this re-specification is feasible and effective, using the unweighted model is again preferred.\n\n\n\nReferences:\n\nAndrew Gelman and David Rothschild, 2014. “Modern polling needs innovation, not traditionalism”. The Washington Post.\nAndrew Gelman, 2014. “Tracking public opinion with biased polls”. The Washington Post.\nAndrew Gelman, 2007. “Struggles with Survey Weighting and Regression Modeling”. Statistical Science, 22(2): 153-164.\nDevin Caughey, Adam J. Berinsky, Sara Chatfield, Erin Hartman, Eric Schikler, and Jasjeet S. Sekhon, 2020. “Target Estimation and Adjustment Weighting for Survey Nonresponse and Sampling Bias”. Cambridge University Press.\nGary Solon, Steven J. Haider, and Jeffrey M. Wooldridge, 2015. “What Are We Weighting For?”. The Journal of Human Resources, 50(2): 301-316.\nSharon L. Lohr and J. Michael Brick, 2017. “Roosevelt Predicted to Win: Revisiting the 1936 Literary Digest Poll”. Statistical, Politics and Policy, 8(1): 65-84.\nThomas Lumley, 2024. “Importance weights”.\nThomas Lumley, 2020. “Weights in statistics”.\nChristopher Winship and Larry Radbill, 1994. “Sampling Weights and Regression Analysis”. Sociological Methods & Research."
  }
]